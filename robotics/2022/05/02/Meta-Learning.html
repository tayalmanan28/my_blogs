<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Meta Learning | Manan Tayal</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Meta Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Meta Learning Learning to Learn Fast." />
<meta property="og:description" content="Meta Learning Learning to Learn Fast." />
<link rel="canonical" href="https://tayalmanan28.github.io/my_blogs/robotics/2022/05/02/Meta-Learning.html" />
<meta property="og:url" content="https://tayalmanan28.github.io/my_blogs/robotics/2022/05/02/Meta-Learning.html" />
<meta property="og:site_name" content="Manan Tayal" />
<meta property="og:image" content="https://tayalmanan28.github.io/my_blogs/images/few-shot-classification.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-02T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://tayalmanan28.github.io/my_blogs/images/few-shot-classification.png" />
<meta property="twitter:title" content="Meta Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-05-02T00:00:00-05:00","datePublished":"2022-05-02T00:00:00-05:00","description":"Meta Learning Learning to Learn Fast.","headline":"Meta Learning","image":"https://tayalmanan28.github.io/my_blogs/images/few-shot-classification.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://tayalmanan28.github.io/my_blogs/robotics/2022/05/02/Meta-Learning.html"},"url":"https://tayalmanan28.github.io/my_blogs/robotics/2022/05/02/Meta-Learning.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/my_blogs/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://tayalmanan28.github.io/my_blogs/feed.xml" title="Manan Tayal" /><link rel="shortcut icon" type="image/x-icon" href="/my_blogs/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/my_blogs/">Manan Tayal</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/my_blogs/search/">Search</a><a class="page-link" href="/my_blogs/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Meta Learning</h1><p class="page-description">Meta Learning Learning to Learn Fast.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-05-02T00:00:00-05:00" itemprop="datePublished">
        May 2, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      33 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/my_blogs/categories/#Robotics">Robotics</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#define-the-meta-learning-problem">Define the Meta-Learning Problem</a></li>
<li class="toc-entry toc-h2"><a href="#a-simple-view">A Simple View</a></li>
<li class="toc-entry toc-h2"><a href="#training-in-the-same-way-as-testing">Training in the Same Way as Testing</a></li>
<li class="toc-entry toc-h2"><a href="#learner-and-meta-learner">Learner and Meta-Learner</a></li>
<li class="toc-entry toc-h2"><a href="#common-approaches">Common Approaches</a></li>
<li class="toc-entry toc-h2"><a href="#metric-based">Metric-Based</a></li>
<li class="toc-entry toc-h2"><a href="#convolutional-siamese-neural-network">Convolutional Siamese Neural Network</a></li>
<li class="toc-entry toc-h2"><a href="#matching-networks">Matching Networks</a>
<ul>
<li class="toc-entry toc-h3"><a href="#simple-embedding">Simple Embedding</a></li>
<li class="toc-entry toc-h3"><a href="#full-context-embeddings">Full Context Embeddings</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#relation-network">Relation Network</a></li>
<li class="toc-entry toc-h2"><a href="#prototypical-networks">Prototypical Networks</a></li>
<li class="toc-entry toc-h2"><a href="#model-based">Model-Based</a></li>
<li class="toc-entry toc-h2"><a href="#memory-augmented-neural-networks">Memory-Augmented Neural Networks</a>
<ul>
<li class="toc-entry toc-h3"><a href="#mann-for-meta-learning">MANN for Meta-Learning</a></li>
<li class="toc-entry toc-h3"><a href="#addressing-mechanism-for-meta-learning">Addressing Mechanism for Meta-Learning</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#meta-networks">Meta Networks</a>
<ul>
<li class="toc-entry toc-h3"><a href="#fast-weights">Fast Weights#</a></li>
<li class="toc-entry toc-h3"><a href="#model-components">Model Components</a></li>
<li class="toc-entry toc-h3"><a href="#training-process">Training Process#</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#optimization-based">Optimization-Based</a></li>
<li class="toc-entry toc-h2"><a href="#lstm-meta-learner">LSTM Meta-Learner</a>
<ul>
<li class="toc-entry toc-h3"><a href="#why-lstm">Why LSTM?</a></li>
<li class="toc-entry toc-h3"><a href="#model-setup">Model Setup</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#maml">MAML</a>
<ul>
<li class="toc-entry toc-h3"><a href="#first-order-maml">First-Order MAML#</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#reptile">Reptile</a>
<ul>
<li class="toc-entry toc-h3"><a href="#the-optimization-assumption">The Optimization Assumption</a></li>
<li class="toc-entry toc-h3"><a href="#reptile-vs-fomaml">Reptile vs FOMAML</a></li>
</ul>
</li>
</ul><p>A good machine learning model often requires training with a large number of samples. Humans, in contrast, learn new concepts and skills much faster and more efficiently. Kids who have seen cats and birds only a few times can quickly tell them apart. People who know how to ride a bike are likely to discover the way to ride a motorcycle fast with little or even no demonstration. Is it possible to design a machine learning model with similar properties - learning new concepts and skills fast with a few training examples? That’s essentially what <strong>meta-learning</strong> aims to solve.</p>

<p>We expect a good meta-learning model capable of well adapting or generalizing to new tasks and new environments that have never been encountered during training time. The adaptation process, essentially a mini learning session, happens during test but with a limited exposure to the new task configurations. Eventually, the adapted model can complete new tasks. This is why meta-learning is also known as <a href="https://www.cs.cmu.edu/~rsalakhu/papers/LakeEtAl2015Science.pdf">learning to learn</a>.</p>

<p>The tasks can be any well-defined family of machine learning problems: supervised learning, reinforcement learning, etc. For example, here are a couple concrete meta-learning tasks:</p>

<ul>
  <li>A classifier trained on non-cat images can tell whether a given image contains a cat after seeing a handful of cat pictures.</li>
  <li>A game bot is able to quickly master a new game.</li>
  <li>A mini robot completes the desired task on an uphill surface during test even through it was only trained in a flat surface environment.</li>
</ul>

<h2 id="define-the-meta-learning-problem">
<a class="anchor" href="#define-the-meta-learning-problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Define the Meta-Learning Problem</h2>

<p>In this post, we focus on the case when each desired task is a supervised learning problem like image classification. There is a lot of interesting literature on meta-learning with reinforcement learning problems (aka “Meta Reinforcement Learning”), but we would not cover them here.</p>

<h2 id="a-simple-view">
<a class="anchor" href="#a-simple-view" aria-hidden="true"><span class="octicon octicon-link"></span></a>A Simple View</h2>

<p>A good meta-learning model should be trained over a variety of learning tasks and optimized for the best performance on a distribution of tasks, including potentially unseen tasks. Each task is associated with a dataset $\mathcal{D}$, containing both feature vectors and true labels. The optimal model parameters are:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Can't use function '\]' in math mode at position 110: …ta(\mathcal{D})\̲]̲" style="color:#cc0000">\theta^\* = \arg \min\_\theta \mathbb{E}\_{\mathcal{D}\sim p(\mathcal{D})} \[\mathcal{L}\_\theta(\mathcal{D})\]</span>

<p>It looks very similar to a normal learning task, but <em>one dataset</em> is considered as <em>one data sample</em>.</p>

<p><em>Few-shot classification</em> is an instantiation of meta-learning in the field of supervised learning. The dataset $\mathcal{D}$ is often split into two parts, a support set $S$ for learning and a prediction set $B$ for training or testing, $\mathcal{D}=\langle S, B\rangle$. Often we consider a <em>K-shot N-class classification</em> task: the support set contains K labelled examples for each of N classes.</p>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/few-shot-classification.png" alt=""></p>

<p>Fig. 1. An example of 4-shot 2-class image classification. (Image thumbnails are from <a href="https://www.pinterest.com/">Pinterest</a>)</p>

<h2 id="training-in-the-same-way-as-testing">
<a class="anchor" href="#training-in-the-same-way-as-testing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training in the Same Way as Testing</h2>

<p>A dataset $\mathcal{D}$ contains pairs of feature vectors and labels, $\mathcal{D} = {(\mathbf{x}_i, y_i)}$ and each label belongs to a known label set $\mathcal{L}^\text{label}$. Let’s say, our classifier $f_\theta$ with parameter $\theta$ outputs a probability of a data point belonging to the class $y$ given the feature vector $\mathbf{x}$, $P_\theta(y \vert \mathbf{x})$.</p>

<p>The optimal parameters should maximize the probability of true labels across multiple training batches $B \subset \mathcal{D}$:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 28: …ned} \theta^\* &amp;̲= {\arg\max}\_{…" style="color:#cc0000">\\begin{aligned} \theta^\* &amp;= {\arg\max}\_{\theta} \mathbb{E}\_{(\mathbf{x}, y)\in \mathcal{D}}\[P\_\theta(y \vert \mathbf{x})\] &amp; \theta^\* &amp;= {\arg\max}\_{\theta} \mathbb{E}\_{B\subset \mathcal{D}}\[\sum\_{(\mathbf{x}, y)\in B}P\_\theta(y \vert \mathbf{x})\] &amp; \scriptstyle{\text{; trained with mini-batches.}} \end{aligned}</span>

<p>In few-shot classification, the goal is to reduce the prediction error on data samples with unknown labels given a small support set for “fast learning” (think of how “fine-tuning” works). To make the training process mimics what happens during inference, we would like to “fake” datasets with a subset of labels to avoid exposing all the labels to the model and modify the optimization procedure accordingly to encourage fast learning:</p>

<ol>
  <li>Sample a subset of labels, $L \subset \mathcal{L}^\text{label}$.</li>
  <li>Sample a support set $S^L \subset \mathcal{D}$ and a training batch $B^L \subset \mathcal{D}$. Both of them only contain data points with labels belonging to the sampled label set $L$, $y \in L, \forall (x, y) \in S^L, B^L$.</li>
  <li>The support set is part of the model input.</li>
  <li>The final optimization uses the mini-batch $B^L$ to compute the loss and update the model parameters through backpropagation, in the same way as how we use it in the supervised learning.</li>
</ol>

<p>You may consider each pair of sampled dataset $(S^L, B^L)$ as one data point. The model is trained such that it can generalize to other datasets. Symbols in red are added for meta-learning in addition to the supervised learning objective.</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Can't use function '\]' in math mode at position 197: …or{red}{, S^L})\̲]̲ \color{red}{\]…" style="color:#cc0000">\theta = \arg \max\_\theta \color{red}{E\_{L \subset \mathcal{L}}\[} E\_{ \color{red}{S^L \subset \mathcal{D}, }B^L \subset \mathcal{D}} \[\sum\_{(x, y) \in B^L} P\_\theta(x, y \color{red}{, S^L})\] \color{red}{\]}</span>

<p>The idea is to some extent similar to using a pre-trained model in image classification (ImageNet) or language modeling (big text corpora) when only a limited set of task-specific data samples are available. Meta-learning takes this idea one step further, rather than fine-tuning according to one down-steam task, it optimizes the model to be good at many, if not all.</p>

<h2 id="learner-and-meta-learner">
<a class="anchor" href="#learner-and-meta-learner" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learner and Meta-Learner</h2>

<p>Another popular view of meta-learning decomposes the model update into two stages:</p>

<ul>
  <li>A classifier $f_\theta$ is the “learner” model, trained for operating a given task;</li>
  <li>In the meantime, a optimizer $g_\phi$ learns how to update the learner model’s parameters via the support set $S$, $\theta’ = g_\phi(\theta, S)$.</li>
</ul>

<p>Then in final optimization step, we need to update both $\theta$ and $\phi$ to maximize:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Can't use function '\]' in math mode at position 182: …ert \mathbf{x})\̲]̲\]" style="color:#cc0000">\mathbb{E}\_{L \subset \mathcal{L}}\[ \mathbb{E}\_{S^L \subset \mathcal{D}, B^L \subset \mathcal{D}} \[ \sum\_{(\mathbf{x}, y) \in B^L} P\_{g\_\phi(\theta, S^L)}(y \vert \mathbf{x})\]\]</span>

<h2 id="common-approaches">
<a class="anchor" href="#common-approaches" aria-hidden="true"><span class="octicon octicon-link"></span></a>Common Approaches</h2>

<p>There are three common approaches to meta-learning: metric-based, model-based, and optimization-based. Oriol Vinyals has a nice summary in his <a href="http://metalearning-symposium.ml/files/vinyals.pdf">talk</a> at meta-learning symposium @ NIPS 2018:</p>

<p>Model-based</p>

<p>Metric-based</p>

<p>Optimization-based</p>

<p><strong>Key idea</strong></p>

<p>RNN; memory</p>

<p>Metric learning</p>

<p>Gradient descent</p>

<p><strong>How $P_\theta(y \vert \mathbf{x})$ is modeled?</strong></p>

<p>$f_\theta(\mathbf{x}, S)$</p>

<p>$\sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i$ (*)</p>

<p>$P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})$</p>

<p>(*) $k_\theta$ is a kernel function measuring the similarity between $\mathbf{x}_i$ and $\mathbf{x}$.</p>

<p>Next we are gonna review classic models in each approach.</p>

<h2 id="metric-based">
<a class="anchor" href="#metric-based" aria-hidden="true"><span class="octicon octicon-link"></span></a>Metric-Based</h2>

<p>The core idea in metric-based meta-learning is similar to nearest neighbors algorithms (i.e., <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-NN</a> classificer and <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means</a> clustering) and <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">kernel density estimation</a>. The predicted probability over a set of known labels $y$ is a weighted sum of labels of support set samples. The weight is generated by a kernel function $k_\theta$, measuring the similarity between two data samples.</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi mathvariant="normal">_</mi><mi>θ</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mo>∑</mo><mi mathvariant="normal">_</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>S</mi></mrow><mi>k</mi><mi mathvariant="normal">_</mi><mi>θ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">P\_\theta(y \vert \mathbf{x}, S) = \sum\_{(\mathbf{x}\_i, y\_i) \in S} k\_\theta(\mathbf{x}, \mathbf{x}\_i)y\_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathbf">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.6em;vertical-align:-0.55em;"></span><span class="mop op-symbol large-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span></span></span></span></span>

<p>To learn a good kernel is crucial to the success of a metric-based meta-learning model. <a href="https://en.wikipedia.org/wiki/Similarity_learning#Metric_learning">Metric learning</a> is well aligned with this intention, as it aims to learn a metric or distance function over objects. The notion of a good metric is problem-dependent. It should represent the relationship between inputs in the task space and facilitate problem solving.</p>

<p>All the models introduced below learn embedding vectors of input data explicitly and use them to design proper kernel functions.</p>

<h2 id="convolutional-siamese-neural-network">
<a class="anchor" href="#convolutional-siamese-neural-network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Convolutional Siamese Neural Network</h2>

<p>The <a href="https://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf">Siamese Neural Network</a> is composed of two twin networks and their outputs are jointly trained on top with a function to learn the relationship between pairs of input data samples. The twin networks are identical, sharing the same weights and network parameters. In other words, both refer to the same embedding network that learns an efficient embedding to reveal relationship between pairs of data points.</p>

<p><a href="http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf">Koch, Zemel &amp; Salakhutdinov (2015)</a> proposed a method to use the siamese neural network to do one-shot image classification. First, the siamese network is trained for a verification task for telling whether two input images are in the same class. It outputs the probability of two images belonging to the same class. Then, during test time, the siamese network processes all the image pairs between a test image and every image in the support set. The final prediction is the class of the support image with the highest probability.</p>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/siamese-conv-net.png" alt=""></p>

<p>Fig. 2. The architecture of convolutional siamese neural network for few-show image classification.</p>

<ol>
  <li>First, convolutional siamese network learns to encode two images into feature vectors via a embedding function $f_\theta$ which contains a couple of convolutional layers.</li>
  <li>The L1-distance between two embeddings is $\vert f_\theta(\mathbf{x}_i) - f_\theta(\mathbf{x}_j) \vert$.</li>
  <li>The distance is converted to a probability $p$ by a linear feedforward layer and sigmoid. It is the probability of whether two images are drawn from the same class.</li>
  <li>Intuitively the loss is cross entropy because the label is binary.</li>
</ol>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left right" columnspacing="0em 1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>j</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">W</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mi mathvariant="normal">_</mi><mi>θ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mi mathvariant="normal">_</mi><mi>θ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>j</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo><mi mathvariant="script">L</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo>=</mo><mo>∑</mo><mi mathvariant="normal">_</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>j</mi><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mi>j</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>B</mi></mrow><mn mathvariant="bold">1</mn><mi mathvariant="normal">_</mi><mrow><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo>=</mo><mi>y</mi><mi mathvariant="normal">_</mi><mi>j</mi></mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>j</mi><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn mathvariant="bold">1</mn><mi mathvariant="normal">_</mi><mrow><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo>=</mo><mi>y</mi><mi mathvariant="normal">_</mi><mi>j</mi></mrow><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>j</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} p(\mathbf{x}\_i, \mathbf{x}\_j) &amp;= \sigma(\mathbf{W}\vert f\_\theta(\mathbf{x}\_i) - f\_\theta(\mathbf{x}\_j) \vert) \mathcal{L}(B) &amp;= \sum\_{(\mathbf{x}\_i, \mathbf{x}\_j, y\_i, y\_j) \in B} \mathbf{1}\_{y\_i=y\_j}\log p(\mathbf{x}\_i, \mathbf{x}\_j) + (1-\mathbf{1}\_{y\_i=y\_j})\log (1-p(\mathbf{x}\_i, \mathbf{x}\_j)) \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.9em;vertical-align:-0.7em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2em;"><span style="top:-3.2em;"><span class="pstrut" style="height:3.05em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2em;"><span style="top:-3.2em;"><span class="pstrut" style="height:3.05em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mord">∣</span><span class="mclose">)</span><span class="mord mathcal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2em;"><span style="top:-3.2em;"><span class="pstrut" style="height:3.05em;"></span><span class="mord"><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mop op-symbol large-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span><span class="mord mathbf">1</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathbf">1</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">))</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7em;"><span></span></span></span></span></span></span></span></span></span></span></span>

<p>Images in the training batch $B$ can be augmented with distortion. Of course, you can replace the L1 distance with other distance metric, L2, cosine, etc. Just make sure they are differential and then everything else works the same.</p>

<p>Given a support set $S$ and a test image $\mathbf{x}$, the final predicted class is:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><mi mathvariant="normal">_</mi><mi>S</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>c</mi><mo stretchy="false">(</mo><mi>arg</mi><mo>⁡</mo><mi>max</mi><mo>⁡</mo><mi mathvariant="normal">_</mi><mrow><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo>∈</mo><mi>S</mi></mrow><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{c}\_S(\mathbf{x}) = c(\arg\max\_{\mathbf{x}\_i \in S} P(\mathbf{x}, \mathbf{x}\_i))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">c</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">c</span><span class="mopen">(</span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">max</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mclose">))</span></span></span></span></span>

<p>where $c(\mathbf{x})$ is the class label of an image $\mathbf{x}$ and $\hat{c}(.)$ is the predicted label.</p>

<p>The assumption is that the learned embedding can be generalized to be useful for measuring the distance between images of unknown categories. This is the same assumption behind transfer learning via the adoption of a pre-trained model; for example, the convolutional features learned in the model pre-trained with ImageNet are expected to help other image tasks. However, the benefit of a pre-trained model decreases when the new task diverges from the original task that the model was trained on.</p>

<h2 id="matching-networks">
<a class="anchor" href="#matching-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Matching Networks</h2>

<p>The task of <strong>Matching Networks</strong> (<a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf">Vinyals et al., 2016</a>) is to learn a classifier $c_S$ for any given (small) support set $S={x_i, y_i}_{i=1}^k$ (<em>k-shot</em> classification). This classifier defines a probability distribution over output labels $y$ given a test example $\mathbf{x}$. Similar to other metric-based models, the classifier output is defined as a sum of labels of support samples weighted by attention kernel $a(\mathbf{x}, \mathbf{x}_i)$ - which should be proportional to the similarity between $\mathbf{x}$ and $\mathbf{x}_i$.</p>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/matching-networks.png" alt=""></p>

<p>Fig. 3. The architecture of Matching Networks. (Image source: <a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf">original paper</a>)</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>c</mi><mi mathvariant="normal">_</mi><mi>S</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msup><mi>a</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi><mtext>, where </mtext><mi>S</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo></mrow><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">c\_S(\mathbf{x}) = P(y \vert \mathbf{x}, S) = \sum\_{i=1}^k a(\mathbf{x}, \mathbf{x}\_i) y\_i \text{, where }S={(\mathbf{x}\_i, y\_i)}\_{i=1}^k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">c</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathbf">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.6em;vertical-align:-0.55em;"></span><span class="mop op-symbol large-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">1</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mord mathnormal">a</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mord text"><span class="mord">, where </span></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2091em;vertical-align:-0.31em;"></span><span class="mord"><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">1</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span>

<p>The attention kernel depends on two embedding functions, $f$ and $g$, for encoding the test sample and the support set samples respectively. The attention weight between two data points is the cosine similarity, $\text{cosine}(.)$, between their embedding vectors, normalized by softmax:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext>cosine</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>g</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msup><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext>cosine</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>g</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>j</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">a(\mathbf{x}, \mathbf{x}\_i) = \frac{\exp(\text{cosine}(f(\mathbf{x}), g(\mathbf{x}\_i))}{\sum\_{j=1}^k\exp(\text{cosine}(f(\mathbf{x}), g(\mathbf{x}\_j))}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">a</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.5485em;vertical-align:-1.0985em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-2.2115em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">1</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8985em;"><span style="top:-3.1124em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord text"><span class="mord">cosine</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">))</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.7em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord text"><span class="mord">cosine</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mclose">))</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0985em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>

<h3 id="simple-embedding">
<a class="anchor" href="#simple-embedding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simple Embedding</h3>

<p>In the simple version, an embedding function is a neural network with a single data sample as input. Potentially we can set $f=g$.</p>

<h3 id="full-context-embeddings">
<a class="anchor" href="#full-context-embeddings" aria-hidden="true"><span class="octicon octicon-link"></span></a>Full Context Embeddings</h3>

<p>The embedding vectors are critical inputs for building a good classifier. Taking a single data point as input might not be enough to efficiently gauge the entire feature space. Therefore, the Matching Network model further proposed to enhance the embedding functions by taking as input the whole support set $S$ in addition to the original input, so that the learned embedding can be adjusted based on the relationship with other support samples.</p>

<ul>
  <li>
    <p>$g_\theta(\mathbf{x}_i, S)$ uses a bidirectional LSTM to encode $\mathbf{x}_i$ in the context of the entire support set $S$.</p>
  </li>
  <li>
    <p>$f_\theta(\mathbf{x}, S)$ encodes the test sample $\mathbf{x}$ visa an LSTM with read attention over the support set $S$.</p>

    <ol>
      <li>First the test sample goes through a simple neural network, such as a CNN, to extract basic features, $f’(\mathbf{x})$.</li>
      <li>Then an LSTM is trained with a read attention vector over the support set as part of the hidden state:</li>
    </ol>

    <span class="katex-error" title="ParseError: KaTeX parse error: Can't use function '\]' in math mode at position 121: …athbf{r}\_{t-1}\̲]̲, \mathbf{c}\_{…" style="color:#cc0000">\begin{aligned} \hat{\mathbf{h}}\_t, \mathbf{c}\_t &amp;= \text{LSTM}(f'(\mathbf{x}), \[\mathbf{h}\_{t-1}, \mathbf{r}\_{t-1}\], \mathbf{c}\_{t-1}) \mathbf{h}\_t &amp;= \hat{\mathbf{h}}\_t + f'(\mathbf{x}) \mathbf{r}\_{t-1} &amp;= \sum\_{i=1}^k a(\mathbf{h}\_{t-1}, g(\mathbf{x}\_i)) g(\mathbf{x}\_i)  a(\mathbf{h}\_{t-1}, g(\mathbf{x}\_i)) &amp;= \text{softmax}(\mathbf{h}\_{t-1}^\top g(\mathbf{x}\_i)) = \frac{\exp(\mathbf{h}\_{t-1}^\top g(\mathbf{x}\_i))}{\sum\_{j=1}^k \exp(\mathbf{h}\_{t-1}^\top g(\mathbf{x}\_j))} \end{aligned}</span>

    <ol>
      <li>Eventually $f(\mathbf{x}, S)=\mathbf{h}_K$ if we do K steps of “read”.</li>
    </ol>
  </li>
</ul>

<p>This embedding method is called “Full Contextual Embeddings (FCE)”. Interestingly it does help improve the performance on a hard task (few-shot classification on mini ImageNet), but makes no difference on a simple task (Omniglot).</p>

<p>The training process in Matching Networks is designed to match inference at test time, see the details in the earlier <a href="#training-in-the-same-way-as-testing">section</a>. It is worthy of mentioning that the Matching Networks paper refined the idea that training and testing conditions should match.</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Can't use function '\]' in math mode at position 197: …mathbf{x}, S^L)\̲]̲\]" style="color:#cc0000">\theta^\* = \arg\max\_\theta \mathbb{E}\_{L\subset \mathcal{L}}\[ \mathbb{E}\_{S^L \subset\mathcal{D}, B^L \subset \mathcal{D}} \[\sum\_{(\mathbf{x}, y) \in B^L} P\_\theta(y \vert \mathbf{x}, S^L)\]\]</span>

<h2 id="relation-network">
<a class="anchor" href="#relation-network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Relation Network</h2>
<hr>

<p><strong>Relation Network (RN)</strong> (<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf">Sung et al., 2018</a>) is similar to <a href="#convolutional-siamese-neural-network">siamese network</a> but with a few differences:</p>

<ol>
  <li>The relationship is not captured by a simple L1 distance in the feature space, but predicted by a CNN classifier $g_\phi$. The relation score between a pair of inputs, $\mathbf{x}_i$ and $\mathbf{x}_j$, is $r_{ij} = g_\phi([\mathbf{x}_i, \mathbf{x}_j])$ where $[.,.]$ is concatenation.</li>
  <li>The objective function is MSE loss instead of cross-entropy, because conceptually RN focuses more on predicting relation scores which is more like regression, rather than binary classification, $\mathcal{L}(B) = \sum_{(\mathbf{x}_i, \mathbf{x}_j, y_i, y_j)\in B} (r_{ij} - \mathbf{1}_{y_i=y_j})^2$.</li>
</ol>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/relation-network.png" alt=""></p>

<p>Fig. 4. Relation Network architecture for a 5-way 1-shot problem with one query example. (Image source: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf">original paper</a>)</p>

<p>(Note: There is another <a href="https://deepmind.com/blog/neural-approach-relational-reasoning/">Relation Network</a> for relational reasoning, proposed by DeepMind. Don’t get confused.)</p>

<h2 id="prototypical-networks">
<a class="anchor" href="#prototypical-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prototypical Networks</h2>

<p><strong>Prototypical Networks</strong> (<a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf">Snell, Swersky &amp; Zemel, 2017</a>) use an embedding function $f_\theta$ to encode each input into a $M$-dimensional feature vector. A <em>prototype</em> feature vector is defined for every class $c \in \mathcal{C}$, as the mean vector of the embedded support data samples in this class.</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">v</mi><mi mathvariant="normal">_</mi><mi>c</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>S</mi><mi mathvariant="normal">_</mi><mi>c</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><mo>∑</mo><mi mathvariant="normal">_</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo separator="true">,</mo><mi>y</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>S</mi><mi mathvariant="normal">_</mi><mi>c</mi></mrow><mi>f</mi><mi mathvariant="normal">_</mi><mi>θ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{v}\_c = \frac{1}{|S\_c|} \sum\_{(\mathbf{x}\_i, y\_i) \in S\_c} f\_\theta(\mathbf{x}\_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7544em;vertical-align:-0.31em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">c</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3174em;vertical-align:-0.996em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">c</span><span class="mord">∣</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.996em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-symbol large-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">c</span></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span></span></span>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/prototypical-networks.png" alt=""></p>

<p>Fig. 5. Prototypical networks in the few-shot and zero-shot scenarios. (Image source: <a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf">original paper</a>)</p>

<p>The distribution over classes for a given test input $\mathbf{x}$ is a softmax over the inverse of distances between the test data embedding and prototype vectors.</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mi>c</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mo>−</mo><mi>d</mi><mi mathvariant="normal">_</mi><mi>φ</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">_</mi><mi>θ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="bold">v</mi><mi mathvariant="normal">_</mi><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>d</mi><mi mathvariant="normal">_</mi><mi>φ</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">_</mi><mi>θ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="bold">v</mi><mi mathvariant="normal">_</mi><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mo>∑</mo><mi mathvariant="normal">_</mi><mrow><msup><mi>c</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∈</mo><mi mathvariant="script">C</mi></mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>d</mi><mi mathvariant="normal">_</mi><mi>φ</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">_</mi><mi>θ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="bold">v</mi><mi mathvariant="normal">_</mi><msup><mi>c</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(y=c\vert\mathbf{x})=\text{softmax}(-d\_\varphi(f\_\theta(\mathbf{x}), \mathbf{v}\_c)) = \frac{\exp(-d\_\varphi(f\_\theta(\mathbf{x}), \mathbf{v}\_c))}{\sum\_{c' \in \mathcal{C}}\exp(-d\_\varphi(f\_\theta(\mathbf{x}), \mathbf{v}\_{c'}))}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">c</span><span class="mord">∣</span><span class="mord mathbf">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord">−</span><span class="mord mathnormal">d</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">φ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">c</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.446em;vertical-align:-0.996em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6779em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathcal" style="margin-right:0.05834em;">C</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord">−</span><span class="mord mathnormal">d</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">φ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6779em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span><span class="mclose">))</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.7em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord">−</span><span class="mord mathnormal">d</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">φ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">c</span><span class="mclose">))</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.996em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>

<p>where $d_\varphi$ can be any distance function as long as $\varphi$ is differentiable. In the paper, they used the squared euclidean distance.</p>

<p>The loss function is the negative log-likelihood: $\mathcal{L}(\theta) = -\log P_\theta(y=c\vert\mathbf{x})$.</p>

<h2 id="model-based">
<a class="anchor" href="#model-based" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model-Based</h2>

<p>Model-based meta-learning models make no assumption on the form of $P_\theta(y\vert\mathbf{x})$. Rather it depends on a model designed specifically for fast learning — a model that updates its parameters rapidly with a few training steps. This rapid parameter update can be achieved by its internal architecture or controlled by another meta-learner model.</p>

<h2 id="memory-augmented-neural-networks">
<a class="anchor" href="#memory-augmented-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Memory-Augmented Neural Networks</h2>

<p>A family of model architectures use external memory storage to facilitate the learning process of neural networks, including <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#neural-turing-machines">Neural Turing Machines</a> and <a href="https://arxiv.org/abs/1410.3916">Memory Networks</a>. With an explicit storage buffer, it is easier for the network to rapidly incorporate new information and not to forget in the future. Such a model is known as <strong>MANN</strong>, short for “<strong>Memory-Augmented Neural Network</strong>”. Note that recurrent neural networks with only <em>internal memory</em> such as vanilla RNN or LSTM are not MANNs.</p>

<p>Because MANN is expected to encode new information fast and thus to adapt to new tasks after only a few samples, it fits well for meta-learning. Taking the Neural Turing Machine (NTM) as the base model, <a href="http://proceedings.mlr.press/v48/santoro16.pdf">Santoro et al. (2016)</a> proposed a set of modifications on the training setup and the memory retrieval mechanisms (or “addressing mechanisms”, deciding how to assign attention weights to memory vectors). Please go through <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#neural-turing-machines">the NTM section</a> in my other post first if you are not familiar with this matter before reading forward.</p>

<p>As a quick recap, NTM couples a controller neural network with external memory storage. The controller learns to read and write memory rows by soft attention, while the memory serves as a knowledge repository. The attention weights are generated by its addressing mechanism: content-based + location based.</p>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/NTM.png" alt=""></p>

<p>Fig. 6. The architecture of Neural Turing Machine (NTM). The memory at time t, $\mathbf{M}_t$ is a matrix of size $N \times M$, containing N vector rows and each has M dimensions.</p>

<h3 id="mann-for-meta-learning">
<a class="anchor" href="#mann-for-meta-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>MANN for Meta-Learning</h3>

<p>To use MANN for meta-learning tasks, we need to train it in a way that the memory can encode and capture information of new tasks fast and, in the meantime, any stored representation is easily and stably accessible.</p>

<p>The training described in <a href="http://proceedings.mlr.press/v48/santoro16.pdf">Santoro et al., 2016</a> happens in an interesting way so that the memory is forced to hold information for longer until the appropriate labels are presented later. In each training episode, the truth label $y_t$ is presented with <strong>one step offset</strong>, $(\mathbf{x}_{t+1}, y_t)$: it is the true label for the input at the previous time step t, but presented as part of the input at time step t+1.</p>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/mann-meta-learning.png" alt=""></p>

<p>Fig. 7. Task setup in MANN for meta-learning (Image source: <a href="http://proceedings.mlr.press/v48/santoro16.pdf">original paper</a>).</p>

<p>In this way, MANN is motivated to memorize the information of a new dataset, because the memory has to hold the current input until the label is present later and then retrieve the old information to make a prediction accordingly.</p>

<p>Next let us see how the memory is updated for efficient information retrieval and storage.</p>

<h3 id="addressing-mechanism-for-meta-learning">
<a class="anchor" href="#addressing-mechanism-for-meta-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Addressing Mechanism for Meta-Learning</h3>

<p>Aside from the training process, a new pure content-based addressing mechanism is utilized to make the model better suitable for meta-learning.</p>

<p><strong>How to read from memory?</strong><br>
The read attention is constructed purely based on the content similarity.</p>

<p>First, a key feature vector $\mathbf{k}_t$ is produced at the time step t by the controller as a function of the input $\mathbf{x}$. Similar to NTM, a read weighting vector $\mathbf{w}_t^r$ of N elements is computed as the cosine similarity between the key vector and every memory vector row, normalized by softmax. The read vector $\mathbf{r}_t$ is a sum of memory records weighted by such weightings:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">r</mi><mi mathvariant="normal">_</mi><mi>i</mi><mo>=</mo><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msup><mi>w</mi><mi mathvariant="normal">_</mi><msup><mi>t</mi><mi>r</mi></msup><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>b</mi><mi>f</mi><mi>M</mi><mi mathvariant="normal">_</mi><mi>t</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mtext>, where </mtext><mi>w</mi><mi mathvariant="normal">_</mi><msup><mi>t</mi><mi>r</mi></msup><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mfrac><mrow><mi mathvariant="bold">k</mi><mi mathvariant="normal">_</mi><mi>t</mi><mo>⋅</mo><mi mathvariant="bold">M</mi><mi mathvariant="normal">_</mi><mi>t</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∥</mi><mi mathvariant="bold">k</mi><mi mathvariant="normal">_</mi><mi>t</mi><mi mathvariant="normal">∥</mi><mo>⋅</mo><mi mathvariant="normal">∥</mi><mi mathvariant="bold">M</mi><mi mathvariant="normal">_</mi><mi>t</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∥</mi></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{r}\_i = \sum\_{i=1}^N w\_t^r(i)\\mathbf{M}\_t(i) \text{, where } w\_t^r(i) = \text{softmax}(\frac{\mathbf{k}\_t \cdot \mathbf{M}\_t(i)}{\|\mathbf{k}\_t\| \cdot \|\mathbf{M}\_t(i)\|})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9695em;vertical-align:-0.31em;"></span><span class="mord mathbf">r</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.6em;vertical-align:-0.55em;"></span><span class="mop op-symbol large-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">1</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">hb</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mord text"><span class="mord">, where </span></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.446em;vertical-align:-0.996em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∥</span><span class="mord mathbf">k</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">t</span><span class="mord">∥</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">∥</span><span class="mord mathbf">M</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mord">∥</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.7em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathbf">k</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathbf">M</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.996em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span>

<p>where $M_t$ is the memory matrix at time t and $M_t(i)$ is the i-th row in this matrix.</p>

<p><strong>» How to write into memory?</strong><br>
The addressing mechanism for writing newly received information into memory operates a lot like the <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies">cache replacement</a> policy. The <strong>Least Recently Used Access (LRUA)</strong> writer is designed for MANN to better work in the scenario of meta-learning. A LRUA write head prefers to write new content to either the <em>least used</em> memory location or the <em>most recently used</em> memory location.</p>

<ul>
  <li>Rarely used locations: so that we can preserve frequently used information (see <a href="https://en.wikipedia.org/wiki/Least_frequently_used">LFU</a>);</li>
  <li>The last used location: the motivation is that once a piece of information is retrieved once, it probably won’t be called again for a while (see <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies#Most_recently_used_(MRU)">MRU</a>).</li>
</ul>

<p>There are many cache replacement algorithms and each of them could potentially replace the design here with better performance in different use cases. Furthermore, it would be a good idea to learn the memory usage pattern and addressing strategies rather than arbitrarily set it.</p>

<p>The preference of LRUA is carried out in a way that everything is differentiable:</p>

<ol>
  <li>The usage weight $\mathbf{w}^u_t$ at time t is a sum of current read and write vectors, in addition to the decayed last usage weight, $\gamma \mathbf{w}^u_{t-1}$, where $\gamma$ is a decay factor.</li>
  <li>The write vector is an interpolation between the previous read weight (prefer “the last used location”) and the previous least-used weight (prefer “rarely used location”). The interpolation parameter is the sigmoid of a hyperparameter $\alpha$.</li>
  <li>The least-used weight $\mathbf{w}^{lu}$ is scaled according to usage weights $\mathbf{w}_t^u$, in which any dimension remains at 1 if smaller than the n-th smallest element in the vector and 0 otherwise.</li>
</ol>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 35: …mathbf{w}\_t^u &amp;̲= \\gamma \\mat…" style="color:#cc0000">\\begin{aligned} \\mathbf{w}\_t^u &amp;= \\gamma \\mathbf{w}\_{t-1}^u + \\mathbf{w}\_t^r + \\mathbf{w}\_t^w \\\\ \\mathbf{w}\_t^r &amp;= \\text{softmax}(\\text{cosine}(\\mathbf{k}\_t, \\mathbf{M}\_t(i))) \\\\ \\mathbf{w}\_t^w &amp;= \\sigma(\\alpha)\\mathbf{w}\_{t-1}^r + (1-\\sigma(\\alpha))\\mathbf{w}^{lu}\_{t-1}\\\\ \\mathbf{w}\_t^{lu} &amp;= \\mathbf{1}\_{w\_t^u(i) \\leq m(\\mathbf{w}\_t^u, n)} \\text{, where }m(\\mathbf{w}\_t^u, n)\\text{ is the }n\\text{-th smallest element in vector }\\mathbf{w}\_t^u\\text{.} \\end{aligned}</span>

<p>Finally, after the least used memory location, indicated by $\mathbf{w}_t^{lu}$, is set to zero, every memory row is updated:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>b</mi><mi>f</mi><mi>M</mi><mi mathvariant="normal">_</mi><mi>t</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>b</mi><mi>f</mi><mi>M</mi><mi mathvariant="normal">_</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>+</mo><mi>w</mi><mi mathvariant="normal">_</mi><msup><mi>t</mi><mi>w</mi></msup><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>b</mi><mi>f</mi><mi>k</mi><mi mathvariant="normal">_</mi><mi>t</mi><mo separator="true">,</mo><mspace linebreak="newline"></mspace><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">\\mathbf{M}\_t(i) = \\mathbf{M}\_{t-1}(i) + w\_t^w(i)\\mathbf{k}\_t, \\forall i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">hb</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">hb</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">hb</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">t</span><span class="mpunct">,</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">ll</span><span class="mord mathnormal">i</span></span></span></span></span>

<h2 id="meta-networks">
<a class="anchor" href="#meta-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Meta Networks</h2>

<p><strong>Meta Networks</strong> (<a href="https://arxiv.org/abs/1703.00837">Munkhdalai &amp; Yu, 2017</a>), short for <strong>MetaNet</strong>, is a meta-learning model with architecture and training process designed for <em>rapid</em> generalization across tasks.</p>

<h3 id="fast-weights">
<a class="anchor" href="#fast-weights" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fast Weights<a href="#fast-weights">#</a>
</h3>

<p>The rapid generalization of MetaNet relies on “fast weights”. There are a handful of papers on this topic, but I haven’t read all of them in detail and I failed to find a very concrete definition, only a vague agreement on the concept. Normally weights in the neural networks are updated by stochastic gradient descent in an objective function and this process is known to be slow. One faster way to learn is to utilize one neural network to predict the parameters of another neural network and the generated weights are called <em>fast weights</em>. In comparison, the ordinary SGD-based weights are named <em>slow weights</em>.</p>

<p>In MetaNet, loss gradients are used as <em>meta information</em> to populate models that learn fast weights. Slow and fast weights are combined to make predictions in neural networks.</p>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/combine-slow-fast-weights.png" alt=""></p>

<p>Fig. 8. Combining slow and fast weights in a MLP. $\bigoplus$ is element-wise sum. (Image source: <a href="https://arxiv.org/abs/1703.00837">original paper</a>).</p>

<h3 id="model-components">
<a class="anchor" href="#model-components" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model Components</h3>

<blockquote>
  <p>Disclaimer: Below you will find my annotations are different from those in the paper. imo, the paper is poorly written, but the idea is still interesting. So I’m presenting the idea in my own language.</p>
</blockquote>

<p>Key components of MetaNet are:</p>

<ul>
  <li>An embedding function $f_\theta$, parameterized by $\theta$, encodes raw inputs into feature vectors. Similar to <a href="#convolutional-siamese-neural-network">Siamese Neural Network</a>, these embeddings are trained to be useful for telling whether two inputs are of the same class (verification task).</li>
  <li>A base learner model $g_\phi$, parameterized by weights $\phi$, completes the actual learning task.</li>
</ul>

<p>If we stop here, it looks just like <a href="#relation-network">Relation Network</a>. MetaNet, in addition, explicitly models the fast weights of both functions and then aggregates them back into the model (See Fig. 8).</p>

<p>Therefore we need additional two functions to output fast weights for $f$ and $g$ respectively.</p>

<ul>
  <li>$F_w$: a LSTM parameterized by $w$ for learning fast weights $\theta^+$ of the embedding function $f$. It takes as input gradients of $f$’s embedding loss for verification task.</li>
  <li>$G_v$: a neural network parameterized by $v$ learning fast weights $\phi^+$ for the base learner $g$ from its loss gradients. In MetaNet, the learner’s loss gradients are viewed as the <em>meta information</em> of the task.</li>
</ul>

<p>Ok, now let’s see how meta networks are trained. The training data contains multiple pairs of datasets: a support set $S=\{\mathbf{x}’_i, y’_i\}_{i=1}^K$ and a test set $U=\{\mathbf{x}_i, y_i\}_{i=1}^L$. Recall that we have four networks and four sets of model parameters to learn, $(\theta, \phi, w, v)$.</p>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/meta-network.png" alt=""></p>

<p>Fig.9. The MetaNet architecture.</p>

<h3 id="training-process">
<a class="anchor" href="#training-process" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training Process<a href="#training-process">#</a>
</h3>

<ol>
  <li>
    <p>Sample a random pair of inputs at each time step t from the support set $S$, $(\mathbf{x}’_i, y’_i)$ and $(\mathbf{x}’_j, y_j)$. Let $\mathbf{x}_{(t,1)}=\mathbf{x}’_i$ and $\mathbf{x}_{(t,2)}=\mathbf{x}’_j$.<br>
for $t = 1, \dots, K$:</p>

    <p>a. Compute a loss for representation learning; i.e., cross entropy for the verification task:<br>
    $\mathcal{L}^\text{emb}_t = \mathbf{1}_{y’_i=y’_j} \log P_t + (1 - \mathbf{1}_{y’_i=y’_j})\log(1 - P_t)\text{, where }P_t = \sigma(\mathbf{W}\vert f_\theta(\mathbf{x}_{(t,1)}) - f_\theta(\mathbf{x}_{(t,2)})\vert)$</p>
  </li>
  <li>
    <p>Compute the task-level fast weights: $\theta^+ = F_w(\nabla_\theta \mathcal{L}^\text{emb}_1, \dots, \mathcal{L}^\text{emb}_T)$</p>
  </li>
  <li>
    <p>Next go through examples in the support set $S$ and compute the example-level fast weights. Meanwhile, update the memory with learned representations.<br>
for $i=1, \dots, K$:</p>

    <p>a. The base learner outputs a probability distribution: $P(\hat{y}_i \vert \mathbf{x}_i) = g_\phi(\mathbf{x}_i)$ and the loss can be cross-entropy or MSE: $\mathcal{L}^\text{task}_i = y’_i \log g_\phi(\mathbf{x}’_i) + (1- y’_i) \log (1 - g_\phi(\mathbf{x}’_i))$
b. Extract meta information (loss gradients) of the task and compute the example-level fast weights: $\phi_i^+ = G_v(\nabla_\phi\mathcal{L}^\text{task}_i)$
c. Then store $\phi^+_i$ into $i$-th location of the “value” memory $\mathbf{M}$.</p>

    <p>d. Encode the support sample into a task-specific input representation using both slow and fast weights: $r’_i = f_{\theta, \theta^+}(\mathbf{x}’_i)$
    Then store $r’_i$ into $i$-th location of the “key” memory $\mathbf{R}$.</p>
  </li>
  <li>
    <p>Finally it is the time to construct the training loss using the test set $U=\{\mathbf{x}_i, y_i\}_{i=1}^L$.<br>
Starts with $\mathcal{L}_\text{train}=0$:<br>
for $j=1, \dots, L$:</p>

    <p>a. Encode the test sample into a task-specific input representation: $r_j = f_{\theta, \theta^+}(\mathbf{x}_j)$
b. The fast weights are computed by attending to representations of support set samples in memory $\mathbf{R}$. The attention function is of your choice. Here MetaNet uses cosine similarity:</p>

    <span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 23: …{aligned} a\_j &amp;̲= \\text{cosine…" style="color:#cc0000">\\begin{aligned} a\_j &amp;= \\text{cosine}(\\mathbf{R}, r\_j) = \[\\frac{r'\_1\\cdot r\_j}{\\|r'\_1\\|\\cdot\\|r\_j\\|}, \\dots, \\frac{r'\_N\\cdot r\_j}{\\|r'\_N\\|\\cdot\\|r\_j\\|}\]\\\\ \\phi^+\_j &amp;= \\text{softmax}(a\_j)^\\top \\mathbf{M} \\end{aligned}</span>

    <p>c. Update the training loss: $\mathcal{L}_\text{train} \leftarrow \mathcal{L}_\text{train} + \mathcal{L}^\text{task}(g_{\phi, \phi^+}(\mathbf{x}_i), y_i) $</p>
  </li>
  <li>
    <p>Update all the parameters $(\theta, \phi, w, v)$ using $\mathcal{L}_\text{train}$.</p>
  </li>
</ol>

<h2 id="optimization-based">
<a class="anchor" href="#optimization-based" aria-hidden="true"><span class="octicon octicon-link"></span></a>Optimization-Based</h2>

<p>Deep learning models learn through backpropagation of gradients. However, the gradient-based optimization is neither designed to cope with a small number of training samples, nor to converge within a small number of optimization steps. Is there a way to adjust the optimization algorithm so that the model can be good at learning with a few examples? This is what optimization-based approach meta-learning algorithms intend for.</p>

<h2 id="lstm-meta-learner">
<a class="anchor" href="#lstm-meta-learner" aria-hidden="true"><span class="octicon octicon-link"></span></a>LSTM Meta-Learner</h2>

<p>The optimization algorithm can be explicitly modeled. <a href="https://openreview.net/pdf?id=rJY0-Kcll">Ravi &amp; Larochelle (2017)</a> did so and named it “meta-learner”, while the original model for handling the task is called “learner”. The goal of the meta-learner is to efficiently update the learner’s parameters using a small support set so that the learner can adapt to the new task quickly.</p>

<p>Let’s denote the learner model as $M_\theta$ parameterized by $\theta$, the meta-learner as $R_\Theta$ with parameters $\Theta$, and the loss function $\mathcal{L}$.</p>

<h3 id="why-lstm">
<a class="anchor" href="#why-lstm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why LSTM?</h3>

<p>The meta-learner is modeled as a LSTM, because:</p>

<ol>
  <li>There is similarity between the gradient-based update in backpropagation and the cell-state update in LSTM.</li>
  <li>Knowing a history of gradients benefits the gradient update; think about how <a href="http://ruder.io/optimizing-gradient-descent/index.html#momentum">momentum</a> works.</li>
</ol>

<p>The update for the learner’s parameters at time step t with a learning rate $\alpha_t$ is:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mi>t</mi><mo>=</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>−</mo><mspace linebreak="newline"></mspace><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mi mathvariant="normal">_</mi><mi>t</mi><mspace linebreak="newline"></mspace><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><mi>a</mi><mi mathvariant="normal">_</mi><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mrow><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>L</mi><mi mathvariant="normal">_</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\\theta\_t = \\theta\_{t-1} - \\alpha\_t \\nabla\_{\\theta\_{t-1}}\\mathcal{L}\_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">lp</span><span class="mord mathnormal">ha</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">t</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">nab</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord mathnormal">L</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">t</span></span></span></span></span>

<p>It has the same form as the cell state update in LSTM, if we set forget gate $f_t=1$, input gate $i_t = \alpha_t$, cell state $c_t = \theta_t$, and new cell state $\tilde{c}_t = -\nabla_{\theta_{t-1}}\mathcal{L}_t$:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 23: …{aligned} c\_t &amp;̲= f\_t \\odot c…" style="color:#cc0000">\\begin{aligned} c\_t &amp;= f\_t \\odot c\_{t-1} + i\_t \\odot \\tilde{c}\_t\\\\ &amp;= \\theta\_{t-1} - \\alpha\_t\\nabla\_{\\theta\_{t-1}}\\mathcal{L}\_t \\end{aligned}</span>

<p>While fixing $f_t=1$ and $i_t=\alpha_t$ might not be the optimal, both of them can be learnable and adaptable to different datasets.</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 23: …{aligned} f\_t &amp;̲= \\sigma(\\mat…" style="color:#cc0000">\\begin{aligned} f\_t &amp;= \\sigma(\\mathbf{W}\_f \\cdot \[\\nabla\_{\\theta\_{t-1}}\\mathcal{L}\_t, \\mathcal{L}\_t, \\theta\_{t-1}, f\_{t-1}\] + \\mathbf{b}\_f) &amp; \\scriptstyle{\\text{; how much to forget the old value of parameters.}}\\\\ i\_t &amp;= \\sigma(\\mathbf{W}\_i \\cdot \[\\nabla\_{\\theta\_{t-1}}\\mathcal{L}\_t, \\mathcal{L}\_t, \\theta\_{t-1}, i\_{t-1}\] + \\mathbf{b}\_i) &amp; \\scriptstyle{\\text{; corresponding to the learning rate at time step t.}}\\\\ \\tilde{\\theta}\_t &amp;= -\\nabla\_{\\theta\_{t-1}}\\mathcal{L}\_t &amp;\\\\ \\theta\_t &amp;= f\_t \\odot \\theta\_{t-1} + i\_t \\odot \\tilde{\\theta}\_t &amp;\\\\ \\end{aligned}</span>

<h3 id="model-setup">
<a class="anchor" href="#model-setup" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model Setup</h3>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/lstm-meta-learner.png" alt=""></p>

<p>Fig. 10. How the learner $M\_\theta$ and the meta-learner $R\_\Theta$ are trained. (Image source: <a href="https://openreview.net/pdf?id=rJY0-Kcll">original paper</a>) with more annotations)</p>

<p>The training process mimics what happens during test, since it has been proved to be beneficial in <a href="#matching-networks">Matching Networks</a>. During each training epoch, we first sample a dataset $\mathcal{D} = (\mathcal{D}_\text{train}, \mathcal{D}_\text{test}) \in \hat{\mathcal{D}}_\text{meta-train}$ and then sample mini-batches out of $\mathcal{D}_\text{train}$ to update $\theta$ for $T$ rounds. The final state of the learner parameter $\theta_T$ is used to train the meta-learner on the test data $\mathcal{D}_\text{test}$.</p>

<p>Two implementation details to pay extra attention to:</p>

<ol>
  <li>How to compress the parameter space in LSTM meta-learner? As the meta-learner is modeling parameters of another neural network, it would have hundreds of thousands of variables to learn. Following the <a href="https://arxiv.org/abs/1606.04474">idea</a> of sharing parameters across coordinates,</li>
  <li>To simplify the training process, the meta-learner assumes that the loss $\mathcal{L}_t$ and the gradient $\nabla_{\theta_{t-1}} \mathcal{L}_t$ are independent.</li>
</ol>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/train-meta-learner.png" alt=""></p>

<h2 id="maml">
<a class="anchor" href="#maml" aria-hidden="true"><span class="octicon octicon-link"></span></a>MAML</h2>

<p><strong>MAML</strong>, short for <strong>Model-Agnostic Meta-Learning</strong> (<a href="https://arxiv.org/abs/1703.03400">Finn, et al. 2017</a>) is a fairly general optimization algorithm, compatible with any model that learns through gradient descent.</p>

<p>Let’s say our model is $f_\theta$ with parameters $\theta$. Given a task $\tau_i$ and its associated dataset $(\mathcal{D}^{(i)}_\text{train}, \mathcal{D}^{(i)}_\text{test})$, we can update the model parameters by one or more gradient descent steps (the following example only contains one step):</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mi mathvariant="normal">_</mi><mi>i</mi><mo>=</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>−</mo><mspace linebreak="newline"></mspace><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mspace linebreak="newline"></mspace><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><mi>a</mi><mi mathvariant="normal">_</mi><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>c</mi><mi>a</mi><mi>l</mi><msup><mi>L</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mi mathvariant="normal">_</mi><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>a</mi><mi>u</mi><mi mathvariant="normal">_</mi><mi>i</mi></mrow><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">_</mi><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\theta'\_i = \\theta - \\alpha \\nabla\_\\theta\\mathcal{L}^{(0)}\_{\\tau\_i}(f\_\\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.1119em;vertical-align:-0.31em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">lp</span><span class="mord mathnormal">ha</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">nab</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.248em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord"><span class="mord mathnormal">L</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">u</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">i</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord" style="margin-right:0.02778em;">_</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span></span>

<p>where $\mathcal{L}^{(0)}$ is the loss computed using the mini data batch with id (0).</p>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/maml.png" alt=""></p>

<p>Fig. 11. Diagram of MAML. (Image source: <a href="https://arxiv.org/abs/1703.03400">original paper</a>)</p>

<p>Well, the above formula only optimizes for one task. To achieve a good generalization across a variety of tasks, we would like to find the optimal $\theta^*$ so that the task-specific fine-tuning is more efficient. Now, we sample a new data batch with id (1) for updating the meta-objective. The loss, denoted as $\mathcal{L}^{(1)}$, depends on the mini batch (1). The superscripts in $\mathcal{L}^{(0)}$ and $\mathcal{L}^{(1)}$ only indicate different data batches, and they refer to the same loss objective for the same task.</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 29: …ed} \\theta^\* &amp;̲= \\arg\\min\_\…" style="color:#cc0000">\\begin{aligned} \\theta^\* &amp;= \\arg\\min\_\\theta \\sum\_{\\tau\_i \\sim p(\\tau)} \\mathcal{L}\_{\\tau\_i}^{(1)} (f\_{\\theta'\_i}) = \\arg\\min\_\\theta \\sum\_{\\tau\_i \\sim p(\\tau)} \\mathcal{L}\_{\\tau\_i}^{(1)} (f\_{\\theta - \\alpha\\nabla\_\\theta \\mathcal{L}\_{\\tau\_i}^{(0)}(f\_\\theta)}) &amp; \\\\ \\theta &amp;\\leftarrow \\theta - \\beta \\nabla\_{\\theta} \\sum\_{\\tau\_i \\sim p(\\tau)} \\mathcal{L}\_{\\tau\_i}^{(1)} (f\_{\\theta - \\alpha\\nabla\_\\theta \\mathcal{L}\_{\\tau\_i}^{(0)}(f\_\\theta)}) &amp; \\scriptstyle{\\text{; updating rule}} \\end{aligned}</span>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/maml-algo.png" alt=""></p>

<p>Fig. 12. The general form of MAML algorithm. (Image source: <a href="https://arxiv.org/abs/1703.03400">original paper</a>)</p>

<h3 id="first-order-maml">
<a class="anchor" href="#first-order-maml" aria-hidden="true"><span class="octicon octicon-link"></span></a>First-Order MAML<a href="#first-order-maml">#</a>
</h3>

<p>The meta-optimization step above relies on second derivatives. To make the computation less expensive, a modified version of MAML omits second derivatives, resulting in a simplified and cheaper implementation, known as <strong>First-Order MAML (FOMAML)</strong>.</p>

<p>Let’s consider the case of performing $k$ inner gradient steps, $k\geq1$. Starting with the initial model parameter $\theta_\text{meta}$:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 29: …ed} \\theta\_0 &amp;̲= \\theta\_\\te…" style="color:#cc0000">\\begin{aligned} \\theta\_0 &amp;= \\theta\_\\text{meta}\\\\ \\theta\_1 &amp;= \\theta\_0 - \\alpha\\nabla\_\\theta\\mathcal{L}^{(0)}(\\theta\_0)\\\\ \\theta\_2 &amp;= \\theta\_1 - \\alpha\\nabla\_\\theta\\mathcal{L}^{(0)}(\\theta\_1)\\\\ &amp;\\dots\\\\ \\theta\_k &amp;= \\theta\_{k-1} - \\alpha\\nabla\_\\theta\\mathcal{L}^{(0)}(\\theta\_{k-1}) \\end{aligned}</span>

<p>Then in the outer loop, we sample a new data batch for updating the meta-objective.</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 40: …\_\\text{meta} &amp;̲\\leftarrow \\t…" style="color:#cc0000">\\begin{aligned} \\theta\_\\text{meta} &amp;\\leftarrow \\theta\_\\text{meta} - \\beta g\_\\text{MAML} &amp; \\scriptstyle{\\text{; update for meta-objective}} \\\\\[2mm\] \\text{where } g\_\\text{MAML} &amp;= \\nabla\_{\\theta} \\mathcal{L}^{(1)}(\\theta\_k) &amp;\\\\\[2mm\] &amp;= \\nabla\_{\\theta\_k} \\mathcal{L}^{(1)}(\\theta\_k) \\cdot (\\nabla\_{\\theta\_{k-1}} \\theta\_k) \\dots (\\nabla\_{\\theta\_0} \\theta\_1) \\cdot (\\nabla\_{\\theta} \\theta\_0) &amp; \\scriptstyle{\\text{; following the chain rule}} \\\\ &amp;= \\nabla\_{\\theta\_k} \\mathcal{L}^{(1)}(\\theta\_k) \\cdot \\Big( \\prod\_{i=1}^k \\nabla\_{\\theta\_{i-1}} \\theta\_i \\Big) \\cdot I &amp; \\\\ &amp;= \\nabla\_{\\theta\_k} \\mathcal{L}^{(1)}(\\theta\_k) \\cdot \\prod\_{i=1}^k \\nabla\_{\\theta\_{i-1}} (\\theta\_{i-1} - \\alpha\\nabla\_\\theta\\mathcal{L}^{(0)}(\\theta\_{i-1})) &amp; \\\\ &amp;= \\nabla\_{\\theta\_k} \\mathcal{L}^{(1)}(\\theta\_k) \\cdot \\prod\_{i=1}^k (I - \\alpha\\nabla\_{\\theta\_{i-1}}(\\nabla\_\\theta\\mathcal{L}^{(0)}(\\theta\_{i-1}))) &amp; \\end{aligned}</span>

<p>The MAML gradient is:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>g</mi><mi mathvariant="normal">_</mi><mspace linebreak="newline"></mspace><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mrow><mi>M</mi><mi>A</mi><mi>M</mi><mi>L</mi></mrow><mo>=</mo><mspace linebreak="newline"></mspace><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><mi>a</mi><mi mathvariant="normal">_</mi><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mi>k</mi></mrow><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>c</mi><mi>a</mi><mi>l</mi><msup><mi>L</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">(</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mi>k</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mspace linebreak="newline"></mspace><mi>p</mi><mi>r</mi><mi>o</mi><mi>d</mi><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msup><mo stretchy="false">(</mo><mi>I</mi><mo>−</mo><mspace linebreak="newline"></mspace><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mspace linebreak="newline"></mspace><mi>c</mi><mi>o</mi><mi>l</mi><mi>o</mi><mi>r</mi><mrow><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mspace linebreak="newline"></mspace><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><mi>a</mi><mi mathvariant="normal">_</mi><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></mrow><mo stretchy="false">(</mo><mspace linebreak="newline"></mspace><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><mi>a</mi><mi mathvariant="normal">_</mi><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>c</mi><mi>a</mi><mi>l</mi><msup><mi>L</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">(</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g\_\\text{MAML} = \\nabla\_{\\theta\_k} \\mathcal{L}^{(1)}(\\theta\_k) \\cdot \\prod\_{i=1}^k (I - \\alpha \\color{red}{\\nabla\_{\\theta\_{i-1}}(\\nabla\_\\theta\\mathcal{L}^{(0)}(\\theta\_{i-1}))})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7406em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord" style="margin-right:0.02778em;">_</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">t</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">L</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">nab</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord"><span class="mord mathnormal">L</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mopen">(</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">d</span><span class="mord mathnormal">o</span><span class="mord mathnormal">t</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.2091em;vertical-align:-0.31em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">ro</span><span class="mord mathnormal">d</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">1</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">lp</span><span class="mord mathnormal">ha</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.248em;vertical-align:-0.31em;"></span><span class="mord mathnormal">co</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mord"><span class="mord mathnormal">re</span><span class="mord mathnormal">d</span></span><span class="mord"><span class="mspace newline"></span><span class="mord mathnormal">nab</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span></span></span><span class="mopen">(</span><span class="mspace newline"></span><span class="mord mathnormal">nab</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mspace newline"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord"><span class="mord mathnormal">L</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span></span><span class="mclose">))</span></span><span class="mclose">)</span></span></span></span></span>

<p>The First-Order MAML ignores the second derivative part in red. It is simplified as follows, equivalent to the derivative of the last inner gradient update result.</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>g</mi><mi mathvariant="normal">_</mi><mspace linebreak="newline"></mspace><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mrow><mi>F</mi><mi>O</mi><mi>M</mi><mi>A</mi><mi>M</mi><mi>L</mi></mrow><mo>=</mo><mspace linebreak="newline"></mspace><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><mi>a</mi><mi mathvariant="normal">_</mi><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mi>k</mi></mrow><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>c</mi><mi>a</mi><mi>l</mi><msup><mi>L</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">(</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g\_\\text{FOMAML} = \\nabla\_{\\theta\_k} \\mathcal{L}^{(1)}(\\theta\_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7406em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord" style="margin-right:0.02778em;">_</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">t</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">FOM</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">L</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">nab</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord"><span class="mord mathnormal">L</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mopen">(</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span></span>

<h2 id="reptile">
<a class="anchor" href="#reptile" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reptile</h2>

<p><strong>Reptile</strong> (<a href="https://arxiv.org/abs/1803.02999">Nichol, Achiam &amp; Schulman, 2018</a>) is a remarkably simple meta-learning optimization algorithm. It is similar to MAML in many ways, given that both rely on meta-optimization through gradient descent and both are model-agnostic.</p>

<p>The Reptile works by repeatedly:</p>

<ol>
  <li>sampling a task,</li>
  <li>training on it by multiple gradient descent steps,</li>
  <li>and then moving the model weights towards the new parameters.</li>
</ol>

<p>See the algorithm below: $\text{SGD}(\mathcal{L}_{\tau_i}, \theta, k)$ performs stochastic gradient update for k steps on the loss $\mathcal{L}_{\tau_i}$ starting with initial parameter $\theta$ and returns the final parameter vector. The batch version samples multiple tasks instead of one within each iteration. The reptile gradient is defined as $(\theta - W)/\alpha$, where $\alpha$ is the stepsize used by the SGD operation.</p>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/reptile-algo.png" alt=""></p>

<p>Fig. 13. The batched version of Reptile algorithm. (Image source: <a href="https://arxiv.org/abs/1803.02999">original paper</a>)</p>

<p>At a glance, the algorithm looks a lot like an ordinary SGD. However, because the task-specific optimization can take more than one step. it eventually makes <span class="katex-error" title="ParseError: KaTeX parse error: Got function '\\' with no arguments as subscript at position 26: …}(\\mathbb{E} _\̲\̲tau\[\\mathcal{…" style="color:#cc0000">\\text{SGD}(\\mathbb{E} _\\tau\[\\mathcal{L}_{\\tau}\], \\theta, k)$ diverge from $\\mathbb{E}_\\tau \[\\text{SGD}(\\mathcal{L}_{\\tau}, \\theta, k)\]</span> when k &gt; 1.</p>

<h3 id="the-optimization-assumption">
<a class="anchor" href="#the-optimization-assumption" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Optimization Assumption</h3>

<p>Assuming that a task $\tau \sim p(\tau)$ has a manifold of optimal network configuration, $\mathcal{W}_{\tau}^*$. The model $f_\theta$ achieves the best performance for task $\tau$ when $\theta$ lays on the surface of $\mathcal{W}_{\tau}^*$. To find a solution that is good across tasks, we would like to find a parameter close to all the optimal manifolds of all tasks:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Can't use function '\]' in math mode at position 132: …W}\_\\tau^\*)^2\̲]̲" style="color:#cc0000">\\theta^\* = \\arg\\min\_\\theta \\mathbb{E}\_{\\tau \\sim p(\\tau)} \[\\frac{1}{2} \\text{dist}(\\theta, \\mathcal{W}\_\\tau^\*)^2\]</span>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/reptile-optim.png" alt=""></p>

<p>Fig. 14. The Reptile algorithm updates the parameter alternatively to be closer to the optimal manifolds of different tasks. (Image source: <a href="https://arxiv.org/abs/1803.02999">original paper</a>)</p>

<p>Let’s use the L2 distance as $\text{dist}(.)$ and the distance between a point $\theta$ and a set $\mathcal{W}_\tau^*$ equals to the distance between $\theta$ and a point $W_{\tau}^*(\theta)$ on the manifold that is closest to $\theta$:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi></mrow><mo stretchy="false">(</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo separator="true">,</mo><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>W</mi><mi mathvariant="normal">_</mi><msup><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>a</mi><mi>u</mi></mrow><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle></msup><mo stretchy="false">)</mo><mo>=</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi></mrow><mo stretchy="false">(</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo separator="true">,</mo><mi>W</mi><mi mathvariant="normal">_</mi><msup><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>a</mi><mi>u</mi></mrow><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle></msup><mo stretchy="false">(</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mrow><mo separator="true">,</mo><mi>w</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>e</mi></mrow><mi>W</mi><mi mathvariant="normal">_</mi><msup><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>a</mi><mi>u</mi></mrow><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle></msup><mo stretchy="false">(</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mspace linebreak="newline"></mspace><mi>a</mi><mi>r</mi><mi>g</mi><mspace linebreak="newline"></mspace><mi>m</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mrow><mi>W</mi><mspace linebreak="newline"></mspace><mi>i</mi><mi>n</mi><mspace linebreak="newline"></mspace><mi>m</mi><mi>a</mi><mi>t</mi><mi>h</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>W</mi><mi mathvariant="normal">_</mi><msup><mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>a</mi><mi>u</mi></mrow><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle></msup></mrow><mspace linebreak="newline"></mspace><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi></mrow><mo stretchy="false">(</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\text{dist}(\\theta, \\mathcal{W}\_{\\tau}^\*) = \\text{dist}(\\theta, W\_{\\tau}^\*(\\theta)) \\text{, where }W\_{\\tau}^\*(\\theta) = \\arg\\min\_{W\\in\\mathcal{W}\_{\\tau}^\*} \\text{dist}(\\theta, W)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">t</span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span></span><span class="mopen">(</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mpunct">,</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.248em;vertical-align:-0.31em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">u</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight" style="color:#cc0000;"><span class="mord mtight" style="color:#cc0000;">\*</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">t</span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span></span><span class="mopen">(</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.248em;vertical-align:-0.31em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">u</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight" style="color:#cc0000;"><span class="mord mtight" style="color:#cc0000;">\*</span></span></span></span></span></span></span></span></span><span class="mopen">(</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mclose">))</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.248em;vertical-align:-0.31em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">t</span><span class="mord"><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">h</span><span class="mord mathnormal">ere</span></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">u</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight" style="color:#cc0000;"><span class="mord mtight" style="color:#cc0000;">\*</span></span></span></span></span></span></span></span></span><span class="mopen">(</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.248em;vertical-align:-0.31em;"></span><span class="mord mathnormal">min</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace newline"></span><span class="mord mathnormal">in</span><span class="mspace newline"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord"><span class="mspace newline"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">u</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight" style="color:#cc0000;"><span class="mord mtight" style="color:#cc0000;">\*</span></span></span></span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">t</span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span></span><span class="mopen">(</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span></span></span></span></span>

<p>The gradient of the squared euclidean distance is:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Can't use function '\]' in math mode at position 100: …\\tau\_i}^\*)^2\̲]̲ &amp;= \\nabla\_\\…" style="color:#cc0000">\\begin{aligned} \\nabla\_\\theta\[\\frac{1}{2}\\text{dist}(\\theta, \\mathcal{W}\_{\\tau\_i}^\*)^2\] &amp;= \\nabla\_\\theta\[\\frac{1}{2}\\text{dist}(\\theta, W\_{\\tau\_i}^\*(\\theta))^2\] &amp; \\\\ &amp;= \\nabla\_\\theta\[\\frac{1}{2}(\\theta - W\_{\\tau\_i}^\*(\\theta))^2\] &amp; \\\\ &amp;= \\theta - W\_{\\tau\_i}^\*(\\theta) &amp; \\scriptstyle{\\text{; See notes.}} \\end{aligned}</span>

<p>Notes: According to the Reptile paper, “<em>the gradient of the squared euclidean distance between a point Θ and a set S is the vector 2(Θ − p), where p is the closest point in S to Θ</em>”. Technically the closest point in S is also a function of Θ, but I’m not sure why the gradient does not need to worry about the derivative of p. (Please feel free to leave me a comment or send me an email about this if you have ideas.)</p>

<p>Thus the update rule for one stochastic gradient step is:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Can't use function '\]' in math mode at position 112: …\\tau\_i}^\*)^2\̲]̲ = \\theta - \\…" style="color:#cc0000">\\theta = \\theta - \\alpha \\nabla\_\\theta\[\\frac{1}{2} \\text{dist}(\\theta, \\mathcal{W}\_{\\tau\_i}^\*)^2\] = \\theta - \\alpha(\\theta - W\_{\\tau\_i}^\*(\\theta)) = (1-\\alpha)\\theta + \\alpha W\_{\\tau\_i}^\*(\\theta)</span>

<p>The closest point on the optimal task manifold $W_{\tau_i}^*(\theta)$ cannot be computed exactly, but Reptile approximates it using $\text{SGD}(\mathcal{L}_\tau, \theta, k)$.</p>

<h3 id="reptile-vs-fomaml">
<a class="anchor" href="#reptile-vs-fomaml" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reptile vs FOMAML</h3>

<p>To demonstrate the deeper connection between Reptile and MAML, let’s expand the update formula with an example performing two gradient steps, k=2 in $\text{SGD}(.)$. Same as defined <a href="#maml">above</a>, $\mathcal{L}^{(0)}$ and $\mathcal{L}^{(1)}$ are losses using different mini-batches of data. For ease of reading, we adopt two simplified annotations: $g^{(i)}_j = \nabla_{\theta} \mathcal{L}^{(i)}(\theta_j)$ and $H^{(i)}_j = \nabla^2_{\theta} \mathcal{L}^{(i)}(\theta_j)$.</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 29: …ed} \\theta\_0 &amp;̲= \\theta\_\\te…" style="color:#cc0000">\\begin{aligned} \\theta\_0 &amp;= \\theta\_\\text{meta}\\\\ \\theta\_1 &amp;= \\theta\_0 - \\alpha\\nabla\_\\theta\\mathcal{L}^{(0)}(\\theta\_0)= \\theta\_0 - \\alpha g^{(0)}\_0 \\\\ \\theta\_2 &amp;= \\theta\_1 - \\alpha\\nabla\_\\theta\\mathcal{L}^{(1)}(\\theta\_1) = \\theta\_0 - \\alpha g^{(0)}\_0 - \\alpha g^{(1)}\_1 \\end{aligned}</span>

<p>According to the <a href="#first-order-maml">early section</a>, the gradient of FOMAML is the last inner gradient update result. Therefore, when k=1:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 36: …\\text{FOMAML} &amp;̲= \\nabla\_{\\t…" style="color:#cc0000">\\begin{aligned} g\_\\text{FOMAML} &amp;= \\nabla\_{\\theta\_1} \\mathcal{L}^{(1)}(\\theta\_1) = g^{(1)}\_1 \\\\ g\_\\text{MAML} &amp;= \\nabla\_{\\theta\_1} \\mathcal{L}^{(1)}(\\theta\_1) \\cdot (I - \\alpha\\nabla^2\_{\\theta} \\mathcal{L}^{(0)}(\\theta\_0)) = g^{(1)}\_1 - \\alpha H^{(0)}\_0 g^{(1)}\_1 \\end{aligned}</span>

<p>The Reptile gradient is defined as:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>g</mi><mi mathvariant="normal">_</mi><mspace linebreak="newline"></mspace><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mrow><mi>R</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>l</mi><mi>e</mi></mrow><mo>=</mo><mo stretchy="false">(</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mn>0</mn><mo>−</mo><mspace linebreak="newline"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mspace linebreak="newline"></mspace><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>0</mn><mo>+</mo><msup><mi>g</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">g\_\\text{Reptile} = (\\theta\_0 - \\theta\_2) / \\alpha = g^{(0)}\_0 + g^{(1)}\_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7406em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord" style="margin-right:0.02778em;">_</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">t</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">pt</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord">_0</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord">_2</span><span class="mclose">)</span><span class="mord">/</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">lp</span><span class="mord mathnormal">ha</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.248em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord">_0</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.248em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord">_1</span></span></span></span></span>

<p>Up to now we have:</p>

<p><img src="https://tayalmanan28.github.io/my_blogs/images/reptile_vs_FOMAML.png" alt=""></p>

<p>Fig. 15. Reptile versus FOMAML in one loop of meta-optimization. (Image source: <a href="https://www.slideshare.net/YoonhoLee4/on-firstorder-metalearning-algorithms">slides</a> on Reptile by Yoonho Lee.)</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 36: …\\text{FOMAML} &amp;̲= g^{(1)}\_1 \\…" style="color:#cc0000">\\begin{aligned} g\_\\text{FOMAML} &amp;= g^{(1)}\_1 \\\\ g\_\\text{MAML} &amp;= g^{(1)}\_1 - \\alpha H^{(0)}\_0 g^{(1)}\_1 \\\\ g\_\\text{Reptile} &amp;= g^{(0)}\_0 + g^{(1)}\_1 \\end{aligned}</span>

<p>Next let’s try further expand $g^{(1)}_1$ using <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor expansion</a>. Recall that Taylor expansion of a function $f(x)$ that is differentiable at a number $a$ is:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Got function '\\' with no arguments as superscript at position 90: …= \\sum\_{i=0}^\̲\̲infty \\frac{f^…" style="color:#cc0000">f(x) = f(a) + \\frac{f'(a)}{1!}(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\dots = \\sum\_{i=0}^\\infty \\frac{f^{(i)}(a)}{i!}(x-a)^i</span>

<p>We can consider $\nabla_{\theta}\mathcal{L}^{(1)}(.)$ as a function and $\theta_0$ as a value point. The Taylor expansion of $g_1^{(1)}$ at the value point $\theta_0$ is:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 29: …ed} g\_1^{(1)} &amp;̲= \\nabla\_{\\t…" style="color:#cc0000">\\begin{aligned} g\_1^{(1)} &amp;= \\nabla\_{\\theta}\\mathcal{L}^{(1)}(\\theta\_1) \\\\ &amp;= \\nabla\_{\\theta}\\mathcal{L}^{(1)}(\\theta\_0) + \\nabla^2\_\\theta\\mathcal{L}^{(1)}(\\theta\_0)(\\theta\_1 - \\theta\_0) + \\frac{1}{2}\\nabla^3\_\\theta\\mathcal{L}^{(1)}(\\theta\_0)(\\theta\_1 - \\theta\_0)^2 + \\dots &amp; \\\\ &amp;= g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} + \\frac{\\alpha^2}{2}\\nabla^3\_\\theta\\mathcal{L}^{(1)}(\\theta\_0) (g\_0^{(0)})^2 + \\dots &amp; \\scriptstyle{\\text{; because }\\theta\_1-\\theta\_0=-\\alpha g\_0^{(0)}} \\\\ &amp;= g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} + O(\\alpha^2) \\end{aligned}</span>

<p>Plug in the expanded form of $g_1^{(1)}$ into the MAML gradients with one step inner gradient update:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 36: …\\text{FOMAML} &amp;̲= g^{(1)}\_1 = …" style="color:#cc0000">\\begin{aligned} g\_\\text{FOMAML} &amp;= g^{(1)}\_1 = g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} + O(\\alpha^2)\\\\ g\_\\text{MAML} &amp;= g^{(1)}\_1 - \\alpha H^{(0)}\_0 g^{(1)}\_1 \\\\ &amp;= g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} + O(\\alpha^2) - \\alpha H^{(0)}\_0 (g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} + O(\\alpha^2))\\\\ &amp;= g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} - \\alpha H^{(0)}\_0 g\_0^{(1)} + \\alpha^2 \\alpha H^{(0)}\_0 H^{(1)}\_0 g\_0^{(0)} + O(\\alpha^2)\\\\ &amp;= g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} - \\alpha H^{(0)}\_0 g\_0^{(1)} + O(\\alpha^2) \\end{aligned}</span>

<p>The Reptile gradient becomes:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 37: …\text{Reptile} &amp;̲= g^{(0)}\_0 + …" style="color:#cc0000">\\begin{aligned} g\_\\text{Reptile} &amp;= g^{(0)}\_0 + g^{(1)}\_1 \\\\ &amp;= g^{(0)}\_0 + g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} + O(\\alpha^2) \\end{aligned}</span>

<p>So far we have the formula of three types of gradients:</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Expected 'EOF', got '&amp;' at position 36: …\\text{FOMAML} &amp;̲= g\_0^{(1)} - …" style="color:#cc0000">\\begin{aligned} g\_\\text{FOMAML} &amp;= g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} + O(\\alpha^2)\\\\ g\_\\text{MAML} &amp;= g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} - \\alpha H^{(0)}\_0 g\_0^{(1)} + O(\\alpha^2)\\\\ g\_\\text{Reptile} &amp;= g^{(0)}\_0 + g\_0^{(1)} - \\alpha H^{(1)}\_0 g\_0^{(0)} + O(\\alpha^2) \\end{aligned}</span>

<p>During training, we often average over multiple data batches. In our example, the mini batches (0) and (1) are interchangeable since both are drawn at random. The expectation $\mathbb{E}_{\tau,0,1}$ is averaged over two data batches, ids (0) and (1), for task $\tau$.</p>

<p>Let,</p>

<ul>
  <li>$A = \mathbb{E}_{\tau,0,1} [g_0^{(0)}] = \mathbb{E}_{\tau,0,1} [g_0^{(1)}]$; it is the average gradient of task loss. We expect to improve the model parameter to achieve better task performance by following this direction pointed by $A$.</li>
  <li>$B = \mathbb{E}_{\tau,0,1} [H^{(1)}_0 g_0^{(0)}] = \frac{1}{2}\mathbb{E}_{\tau,0,1} [H^{(1)}_0 g_0^{(0)} + H^{(0)}_0 g_0^{(1)}] = \frac{1}{2}\mathbb{E}_{\tau,0,1} [\nabla_\theta(g^{(0)}_0 g_0^{(1)})]$; it is the direction (gradient) that increases the inner product of gradients of two different mini batches for the same task. We expect to improve the model parameter to achieve better generalization over different data by following this direction pointed by $B$.</li>
</ul>

<p>To conclude, both MAML and Reptile aim to optimize for the same goal, better task performance (guided by A) and better generalization (guided by B), when the gradient update is approximated by first three leading terms.</p>

<span class="katex-error" title="ParseError: KaTeX parse error: Can't use function '\]' in math mode at position 61: …_\\text{FOMAML}\̲]̲ &amp;= A - \\alpha…" style="color:#cc0000">\\begin{aligned} \\mathbb{E}\_{\\tau,1,2}\[g\_\\text{FOMAML}\] &amp;= A - \\alpha B + O(\\alpha^2)\\\\ \\mathbb{E}\_{\\tau,1,2}\[g\_\\text{MAML}\] &amp;= A - 2\\alpha B + O(\\alpha^2)\\\\ \\mathbb{E}\_{\\tau,1,2}\[g\_\\text{Reptile}\] &amp;= 2A - \\alpha B + O(\\alpha^2) \\end{aligned}</span>

<p>It is not clear to me whether the ignored term $O(\alpha^2)$ might play a big impact on the parameter learning. But given that FOMAML is able to obtain a similar performance as the full version of MAML, it might be safe to say higher-level derivatives would not be critical during gradient descent update.</p>

  </div><a class="u-url" href="/my_blogs/robotics/2022/05/02/Meta-Learning.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/my_blogs/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/my_blogs/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/my_blogs/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>I Write Blogs Related to Robotics</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/tayalmanan28" target="_blank" title="tayalmanan28"><svg class="svg-icon grey"><use xlink:href="/my_blogs/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/tayalmanan28" target="_blank" title="tayalmanan28"><svg class="svg-icon grey"><use xlink:href="/my_blogs/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
