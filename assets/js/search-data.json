{
  
    
        "post0": {
            "title": "Curriculum for Reinforcement Learning",
            "content": "It sounds like an impossible task if we want to teach integral or derivative to a 3-year-old who does not even know basic arithmetics. That’s why education is important, as it provides a systematic way to break down complex knowledge and a nice curriculum for teaching concepts from simple to hard. A curriculum makes learning difficult things easier and approachable for us humans. But, how about machine learning models? Can we train our models more efficiently with a curriculum? Can we design a curriculum to speed up learning? Back in 1993, Jeffrey Elman has proposed the idea of training neural networks with a curriculum. His early work on learning simple language grammar demonstrated the importance of such a strategy: starting with a restricted set of simple data and gradually increasing the complexity of training samples; otherwise the model was not able to learn at all. Compared to training without a curriculum, we would expect the adoption of the curriculum to expedite the speed of convergence and may or may not improve the final model performance. To design an efficient and effective curriculum is not easy. Keep in mind that, a bad curriculum may even hamper learning. Next, we will look into several categories of curriculum learning, as illustrated in Fig. 1. Most cases are applied to Reinforcement Learning, with a few exceptions on Supervised Learning. . In “The importance of starting small” paper (Elman 1993), I especially like the starting sentences and find them both inspiring and affecting: . “Humans differ from other species along many dimensions, but two are particularly noteworthy. Humans display an exceptional capacity to learn; and humans are remarkable for the unusually long time it takes to reach maturity. The adaptive advantage of learning is clear, and it may be argued that, through culture, learning has created the basis for a non-genetically based transmission of behaviors which may accelerate the evolution of our species” . Indeed, learning is probably the best superpower we humans have. . Task-Specific Curriculum . Bengio, et al. (2009) provided a good overview of curriculum learning in the old days. The paper presented two ideas with toy experiments using a manually designed task-specific curriculum: . Cleaner Examples may yield better generalization faster. | Introducing gradually more difficult examples speeds up online training. | . It is plausible that some curriculum strategies could be useless or even harmful. A good question to answer in the field is: What could be the general principles that make some curriculum strategies work better than others? The Bengio 2009 paper hypothesized it would be beneficial to make learning focus on “interesting” examples that are neither too hard or too easy. If our naive curriculum is to train the model on samples with a gradually increasing level of complexity, we need a way to quantify the difficulty of a task first. One idea is to use its minimal loss with respect to another model while this model is pretrained on other tasks (Weinshall, et al. 2018). In this way, the knowledge of the pretrained model can be transferred to the new model by suggesting a rank of training samples. Fig. 2 shows the effectiveness of the curriculum group (green), compared to control (random order; yellow) and anti (reverse the order; red) groups. . Fig. 2. Image classification accuracy on test image set (5 member classes of “small mammals” in CIFAR100). There are 4 experimental groups, (a) curriculum: sort the labels by the confidence of another trained classifier (e.g. the margin of an SVM); (b) control-curriculum: sort the labels randomly; (c) anti-curriculum: sort the labels reversely; (d) None: no curriculum. (Image source: Weinshall, et al. 2018) Zaremba and Sutskever (2014) did an interesting experiment on training LSTM to predict the output of a short Python program for mathematical ops without actually executing the code. They found curriculum is necessary for learning. The program’s complexity is controlled by two parameters, length ∈ [1, a] and nesting∈ [1, b]. Three strategies are considered: . Naive curriculum: increase length first until reaching a; then increase nesting and reset length to 1; repeat this process until both reach maximum. | Mix curriculum: sample length ~ [1, a] and nesting ~ [1, b] | Combined: naive + mix. | . They noticed that combined strategy always outperformed the naive curriculum and would generally (but not always) outperform the mix strategy — indicating that it is quite important to mix in easy tasks during training to avoid forgetting. Procedural content generation (PCG) is a popular approach for creating video games of various levels of difficulty. PCG involves algorithmic randomness and a heavy dose of human expertise in designing game elements and dependencies among them. Procedurally generated levels have been introduced into several benchmark environments for evaluating whether an RL agent can generalize to a new level that it is not trained on (meta-RL), such as GVGAI, OpenAI CoinRun and Procgen benchmark. Using GVGAI, Justesen, et al. (2018) demonstrated that an RL policy can easily overfit to a specific game but training over a simple curriculum that grows the task difficulty together with the model performance helps its generalization to new human-designed levels. Similar results are also found in CoinRun (Cobbe, et al. 2018). POET (Wang et al, 2019) is another example for leveraging evolutionary algorithm and procedural generated game levels to improve RL generalization, which I’ve described in details in my meta-RL post To follow the curriculum learning approaches described above, generally we need to figure out two problems in the training procedure: . Design a metric to quantify how hard a task is so that we can sort tasks accordingly. | Provide a sequence of tasks with an increasing level of difficulty to the model during training. | . However, the order of tasks does not have to be sequential. In our Rubik’s cube paper (OpenAI et al, 2019), we depended on Automatic domain randomization (ADR) to generate a curriculum by growing a distribution of environments with increasing complexity. The difficulty of each task (i.e. solving a Rubik’s cube in a set of environments) depends on the randomization ranges of various environmental parameters. Even with a simplified assumption that all the environmental parameters are uncorrelated, we were able to create a decent curriculum for our robot hand to learn the task. . Teacher-Guided Curriculum . The idea of Automatic Curriculum Learning was proposed by Graves, et al. 2017 slightly earlier. It considers a N-task curriculum as an N-armed bandit problem and an adaptive policy which learns to optimize the returns from this bandit. Two categories of learning signals have been considered in the paper: . Loss-driven progress: the loss function change before and after one gradient update. This type of reward signals tracks the speed of the learning process, because the greatest task loss decrease is equivalent to the fastest learning. | Complex-driven progress: the KL divergence between posterior and prior distribution over network weights. This type of learning signals are inspired by the MDL principle, increasing the model complexity by a certain amount is only worthwhile if it compresses the data by a greater amount. The model complexity is therefore expected to increase most in response to the model nicely generalizing to training examples. | . This framework of proposing curriculum automatically through another RL agent was formalized as Teacher-Student Curriculum Learning (TSCL; Matiisen, et al. 2017). In TSCL, a student is an RL agent working on actual tasks while a teacher agent is a policy for selecting tasks. The student aims to master a complex task that might be hard to learn directly. To make this task easier to learn, we set up the teacher agent to guide the student’s training process by picking proper sub-tasks. . Fig. 3. The setup of teacher-student curriculum learning. (Image source: Matiisen, et al. 2017 + my annotation in red.) In the process, the student should learn tasks which: . can help the student make fastest learning progress, or | are at risk of being forgotten. | . Note: The setup of framing the teacher model as an RL problem feels quite similar to Neural Architecture Search (NAS), but differently the RL model in TSCL operates on the task space and NAS operates on the main model architecture space. . Training the teacher model is to solve a POMDP problem: . The unobserved $s_t$ is the full state of the student model. | The observed $o = (x_t^{(1)}, dots, x_t^{(N)})$ are a list of scores for $N$ tasks. | The action $a$ is to pick on subtask. | The reward per step is the score delta $r_{t} = sum_{i=1}^{N} x_t^{(i)} - x_{t-1}^{(i)}$ (i.e., equivalent to maximizing the score of all tasks at the end of the episode). | . The method of estimating learning progress from noisy task scores while balancing exploration vs exploitation can be borrowed from the non-stationary multi-armed bandit problem - use ε-greedy, or Thompson sampling. The core idea, in summary, is to use one policy to propose tasks for another policy to learn better. Interestingly, both works above (in the discrete task space) found that uniformly sampling from all tasks is a surprisingly strong benchmark. What if the task space is continuous? Portelas, et al. (2019) studied a continuous teacher-student framework, where the teacher has to sample parameters from continuous task space to generate a learning curriculum. Given a newly sampled parameter $p$, the absolute learning progress (short for ALP) is measured as $ text{ALP}_p = vert r - r_ text{old} vert $, where $r$ is the episodic reward associated with $p$ and $r_ text{old}$ is the reward associated with $p_ text{old}$. Here, $p_ text{old}$ is a previous sampled parameter closest to $p$ in the task space, which can be retrieved by nearest neighbor. Note that how this ALP score is different from learning signals in TSCL or Grave, et al. 2017 above: ALP score measures the reward difference between two tasks rather than performance at two time steps of the same task. On top of the task parameter space, a Gaussian mixture model is trained to fit the distribution of $ text{ALP}_p $ over $p$. ε-greedy is used when sampling the tasks: with some probability, sampling a random task; otherwise sampling proportionally to ALP score from the GMM model. Fig. 4. The algorithm of ALP-GMM (absolute learning progress Gaussian mixture model). (Image source: Portelas, et al., 2019) . Curriculum through Self-Play . Different from the teacher-student framework, two agents are doing very different things. The teacher learns to pick a task for the student without any knowledge of the actual task content. What if we want to make both train on the main task directly? How about even make them compete with each other? Sukhbaatar, et al. (2017) proposed a framework for automatic curriculum learning through asymmetric self-play. Two agents, Alice and Bob, play the same task with different goals: Alice challenges Bob to achieve the same state and Bob attempts to complete it as fast as he can. Fig. 5. Illustration of the self-play setup when training two agents. The example task is MazeBase: An agent is asked to reach a goal flag in a maze with a light switch, a key and a wall with a door. Toggling the key switch can open or close the door and Turning off the light makes only the glowing light switch available to the agent. (Image source: Sukhbaatar, et al. 2017) Let us consider Alice and Bob as two separate copies for one RL agent trained in the same environment but with different brains. Each of them has independent parameters and loss objective. The self-play-driven training consists of two types of episodes: . In the self-play episode, Alice alters the state from $s_0$ to $s_t$ and then Bob is asked to return the environment to its original state $s_0$ to get an internal reward. | In the target task episode, Bob receives an external reward if he visits the target flag. | . Note that since B has to repeat the actions between the same pair of $(s_0, s_t)$ of A, this framework only works in reversible or resettable environments. Alice should learn to push Bob out of his comfort zone, but not give him impossible tasks. Bob’s reward is set as $R_B = - gamma t_B$ and Alice’s reward is $R_A = gamma max(0, t_B - t_A)$, where $t_B$ is the total time for B to complete the task, $t_A$ is the time until Alice performs the STOP action and $ gamma$ is a scalar constant to rescale the reward to be comparable with the external task reward. If B fails a task, $t_B = t_ max - t_A$. Both policies are goal-conditioned. The losses imply: . B wants to finish a task asap. | A prefers tasks that take more time of B. | A does not want to take too many steps when B is failing. | . In this way, the interaction between Alice and Bob automatically builds a curriculum of increasingly challenging tasks. Meanwhile, as A has done the task herself before proposing the task to B, the task is guaranteed to be solvable. The paradigm of A suggesting tasks and then B solving them does sound similar to the Teacher-Student framework. However, in asymmetric self-play, Alice, who plays a teacher role, also works on the same task to find challenging cases for Bob, rather than optimizes B’s learning process explicitly. . Automatic Goal Generation . Often RL policy needs to be able to perform over a set of tasks. The goal should be carefully chosen so that at every training stage, it would not be too hard or too easy for the current policy. A goal $g in mathcal{G}$ can be defined as a set of states $S^g$ and a goal is considered as achieved whenever an agent arrives at any of those states. The approach of Generative Goal Learning (Florensa, et al. 2018) relies on a Goal GAN to generate desired goals automatically. In their experiment, the reward is very sparse, just a binary flag for whether a goal is achieved or not and the policy is conditioned on goal, . π∗(at∣st,g)=arg⁡max⁡πEg∼pg(.)Rg(π)where Rg(π)=Eπ(.∣st,g)1[∃t∈[1,…,T]:st∈Sg] begin{aligned} pi^{*}(a_t vert s_t, g) &amp;= arg max_ pi mathbb{E}_{g sim p_g(.)} R^g( pi) text{where }R^g( pi) &amp;= mathbb{E}_ pi(. mid s_t, g) mathbf{1}[ exists t in [1, dots, T]: s_t in S^g] end{aligned}π∗(at​∣st​,g)where Rg(π)​=argπmax​Eg∼pg​(.)​Rg(π)=Eπ​(.∣st​,g)1[∃t∈[1,…,T]:st​∈Sg]​ . Here $R^g( pi)$ is the expected return, also equivalent to the success probability. Given sampled trajectories from the current policy, as long as any state belongs to the goal set, the return will be positive. Their approach iterates through 3 steps until the policy converges: . Label a set of goals based on whether they are at the appropriate level of difficulty for the current policy. . | The set of goals at the appropriate level of difficulty are named GOID (short for “Goals of Intermediate Difficulty”) $ text{GOID}_i := {g : R_ text{min} leq R^g( pi_i) leq R_ text{max} } subseteq G$ | Here $R_ text{min}$ and $R_ text{max}$ can be interpreted as a minimum and maximum probability of reaching a goal over T time-steps. . | Train a Goal GAN model using labelled goals from step 1 to produce new goals | Use these new goals to train the policy, improving its coverage objective. | . The Goal GAN generates a curriculum automatically: . Generator $G(z)$: produces a new goal. =&gt; expected to be a goal uniformly sampled from $GOID$ set. | Discriminator $D(g)$: evaluates whether a goal can be achieved. =&gt; expected to tell whether a goal is from $GOID$ set. | . The Goal GAN is constructed similar to LSGAN (Least-Squared GAN; Mao et al., (2017)), which has better stability of learning compared to vanilla GAN. According to LSGAN, we should minimize the following losses for $D$ and $G$ respectively: . LLSGAN(D)=12Eg∼pdata(g)[(D(g)−b)2]+12Ez∼pz(z)[(D(G(z))−a)2]LLSGAN(G)=12Ez∼pz(z)[(D(G(z))−c)2] begin{aligned} mathcal{L}_ text{LSGAN}(D) &amp;= frac{1}{2} mathbb{E}_{g sim p_ text{data}(g)} [ (D(g) - b)^2] + frac{1}{2} mathbb{E}_{z sim p_z(z)} [ (D(G(z)) - a)^2] mathcal{L}_ text{LSGAN}(G) &amp;= frac{1}{2} mathbb{E}_{z sim p_z(z)} [ (D(G(z)) - c)^2] end{aligned}LLSGAN​(D)LLSGAN​(G)​=21​Eg∼pdata​(g)​[(D(g)−b)2]+21​Ez∼pz​(z)​[(D(G(z))−a)2]=21​Ez∼pz​(z)​[(D(G(z))−c)2]​ . where $a$ is the label for fake data, $b$ for real data, and $c$ is the value that $G$ wants $D$ to believe for fake data. In LSGAN paper’s experiments, they used $a=-1, b=1, c=0$. The Goal GAN introduces an extra binary flag $y_b$ indicating whether a goal $g$ is real ($y_{g} = 1$) or fake ($y_{g} = 0$) so that the model can use negative samples for training: . LGoalGAN(D)=12Eg∼pdata(g)[(D(g)−b)2+(1−yg)(D(g)−a)2]+12Ez∼pz(z)[(D(G(z))−a)2]LGoalGAN(G)=12Ez∼pz(z)[(D(G(z))−c)2] begin{aligned} mathcal{L}_ text{GoalGAN}(D) &amp;= frac{1}{2} mathbb{E}_{g sim p_ text{data}(g)} [ (D(g) - b)^2 + (1-y_g) (D(g) - a)^2] + frac{1}{2} mathbb{E}_{z sim p_z(z)} [ (D(G(z)) - a)^2] mathcal{L}_ text{GoalGAN}(G) &amp;= frac{1}{2} mathbb{E}_{z sim p_z(z)} [ (D(G(z)) - c)^2] end{aligned}LGoalGAN​(D)LGoalGAN​(G)​=21​Eg∼pdata​(g)​[(D(g)−b)2+(1−yg​)(D(g)−a)2]+21​Ez∼pz​(z)​[(D(G(z))−a)2]=21​Ez∼pz​(z)​[(D(G(z))−c)2]​ . Fig. 6. The algorithm of Generative Goal Learning. (Image source: (Florensa, et al. 2018) Following the same idea, Racaniere and Lampinen, et al. (2019) designs a method to make the objectives of goal generator more sophisticated. Their method contains three components, same as generative goal learning above: . Solver /Policy $ pi$: In each episode, the solver gets a goal $g$ at the beginning and get a single binary reward $R^g$ at the end. | Judge /Discriminator $D(.)$: A classifier to predict the binary reward (whether goal can be achieved or not); precisely it outputs the logit of a probability of achieving the given goal, $ sigma(D(g)) = p(R^g=1 vert g)$, where $ sigma$ is the sigmoid function. | Setter /Generator $G(.)$: The goal setter takes as input a desired feasibility score $f in text{Unif}(0, 1)$ and generates $g = G(z, f)$, where the latent variable $z$ is sampled by $z sim mathcal{N}(0, I)$. The goal generator is designed to reversible, so $G^{-1}$ can map backwards from a goal $g$ to a latent $z = G^{-1}(g, f)$ | . The generator is optimized with three objectives: . Goal validity: The proposed goal should be achievable by an expert policy. The corresponding generative loss is designed to increase the likelihood of generating goals that the solver policy has achieved before (like in HER). . $ mathcal{L}_ text{val}$ is the negative log-likelihood of generated goals that have been solved by the solver in the past. . Lval=Eg∼ achieved by solver,ξ∈Uniform(0,δ),f∈Uniform(0,1)[−log⁡p(G−1(g+ξ,f))] begin{align*} mathcal{L}_ text{val} = mathbb{E}_{ substack{ g sim text{ achieved by solver}, xi in text{Uniform}(0, delta), f in text{Uniform}(0, 1) }} big[ - log p(G^{-1}(g + xi, f)) big] end{align*}Lval​=Eg∼ achieved by solver,ξ∈Uniform(0,δ),f∈Uniform(0,1)​​[−logp(G−1(g+ξ,f))]​ . Goal feasibility: The proposed goal should be achievable by the current policy; that is, the level of difficulty should be appropriate. . $ mathcal{L}_ text{feas}$ is the output probability by the judge model $D$ on the generated goal $G(z, f)$ should match the desired $f$. . Lfeas=Ez∈N(0,1),f∈Uniform(0,1)[D(G(z,f))−σ−1(f)2] begin{align*} mathcal{L}_ text{feas} = mathbb{E}_{ substack{ z in mathcal{N}(0, 1), f in text{Uniform}(0, 1) }} big[ D(G(z, f)) - sigma^{-1}(f)^2 big] end{align*}Lfeas​=Ez∈N(0,1),f∈Uniform(0,1)​​[D(G(z,f))−σ−1(f)2]​ . Goal coverage: We should maximize the entropy of generated goals to encourage diverse goal and to improve the coverage over the goal space. | . Lcov=Ez∈N(0,1),f∈Uniform(0,1)[log⁡p(G(z,f))] begin{align*} mathcal{L}_ text{cov} = mathbb{E}_{ substack{ z in mathcal{N}(0, 1), f in text{Uniform}(0, 1) }} big[ log p(G(z, f)) big] end{align*}Lcov​=Ez∈N(0,1),f∈Uniform(0,1)​​[logp(G(z,f))]​ . Their experiments showed complex environments require all three losses above. When the environment is changing between episodes, both the goal generator and the discriminator need to be conditioned on environmental observation to produce better results. If there is a desired goal distribution, an additional loss can be added to match a desired goal distribution using Wasserstein distance. Using this loss, the generator can push the solver toward mastering the desired tasks more efficiently. Fig. 7. Training schematic for the (a) solver/policy, (b) judge/discriminator, and (c) setter/goal generator models. (Image source: Racaniere &amp; Lampinen, et al., 2019) . Skill-Based Curriculum . Another view is to decompose what an agent is able to complete into a variety of skills and each skill set could be mapped into a task. Let’s imagine when an agent interacts with the environment in an unsupervised manner, is there a way to discover useful skills from such interaction and further build into the solutions for more complicated tasks through a curriculum? Jabri, et al. (2019) developed an automatic curriculum, CARML (short for “Curricula for Unsupervised Meta-Reinforcement Learning”), by modeling unsupervised trajectories into a latent skill space, with a focus on training meta-RL( policies (i.e. can transfer to unseen tasks). The setting of training environments in CARML is similar to DIAYN. Differently, CARML is trained on pixel-level observations but DIAYN operates on the true state space. An RL algorithm $ pi_ theta$, parameterized by $ theta$, is trained via unsupervised interaction formulated as a CMP combined with a learned reward function $r$. This setting naturally works for the meta-learning purpose, since a customized reward function can be given only at the test time. Fig. 8. An illustration of CARML, containing two steps: (1) organizing experiential data into the latent skill space; (2) meta-training the policy with the reward function constructed from the learned skills. (Image source: Jabri, et al 2019) CARML is framed as a variational Expectation-Maximization (EM). (1) E-Step: This is the stage for organizing experiential data. Collected trajectories are modeled with a mixture of latent components forming the basis of skills. Let $z$ be a latent task variable and $q_ phi$ be a variational distribution of $z$, which could be a mixture model with discrete $z$ or a VAE with continuous $z$. A variational posterior $q_ phi(z vert s)$ works like a classifier, predicting a skill given a state, and we would like to maximize $q_ phi(z vert s)$ to discriminate between data produced by different skills as much as possible. In E-step, $q_ phi$ is fitted to a set of trajectories produced by $ pi_ theta$. Precisely, given a trajectory $ tau = (s_1, dots,s_T)$, we would like to find $ phi$ such that . max⁡ϕEz∼qϕ(z)[log⁡qϕ(τ∣z)]=max⁡ϕEz∼qϕ(z)[∑si∈τlog⁡qϕ(si∣z)] max_ phi mathbb{E}_{z sim q_ phi(z)} big[ log q_ phi( tau vert z) big] = max_ phi mathbb{E}_{z sim q_ phi(z)} big[ sum_{s_i in tau} log q_ phi(s_i vert z) big]ϕmax​Ez∼qϕ​(z)​[logqϕ​(τ∣z)]=ϕmax​Ez∼qϕ​(z)​[si​∈τ∑​logqϕ​(si​∣z)] . A simplifying assumption is made here to ignore the order of states in one trajectory. (2) M-Step: This is the stage for doing meta-RL training with $ pi_ theta$. The learned skill space is considered as a training task distribution. CARML is agnostic to the type of meta-RL algorithm for policy parameter updates. Given a trajectory $ tau$, it makes sense for the policy to maximize the mutual information between $ tau$ and $z$, $I( tau;z) = H( tau) - H( tau vert z)$, because: . maximizing $H( tau)$ =&gt; diversity in the policy data space; expected to be large. | minimizing $H( tau vert z)$ =&gt; given a certain skill, the behavior should be restricted; expected to be small. | . Then we have, I(τ;z)=H(z)−H(z∣s1,…,sT)≥Es∈τ[H(z)−H(z∣s)]; discard the order of states.=Es∈τ[H(st)−H(s∣z)]; by definition of MI.=Ez∼qϕ(z),s∼πθ(s∣z)[log⁡qϕ(s∣z)−log⁡πθ(s)]≈Ez∼qϕ(z),s∼πθ(s∣z)[log⁡qϕ(s∣z)−log⁡qϕ(s)]; assume learned marginal distr. matches policy. begin{aligned} I( tau; z) &amp;= mathcal{H}(z) - mathcal{H}(z vert s_1, dots, s_T) &amp; geq mathbb{E}_{s in tau} [ mathcal{H}(z) - mathcal{H}(z vert s)] &amp; scriptstyle{ text{; discard the order of states.}} &amp;= mathbb{E}_{s in tau} [ mathcal{H}(s_t) - mathcal{H}(s vert z)] &amp; scriptstyle{ text{; by definition of MI.}} &amp;= mathbb{E}_{z sim q_ phi(z), s sim pi_ theta(s|z)} [ log q_ phi(s|z) - log pi_ theta(s)] &amp; approx mathbb{E}_{z sim q_ phi(z), s sim pi_ theta(s|z)} [ color{green}{ log q_ phi(s|z) - log q_ phi(s)}] &amp; scriptstyle{ text{; assume learned marginal distr. matches policy.}} end{aligned}I(τ;z)​=H(z)−H(z∣s1​,…,sT​)≥Es∈τ​[H(z)−H(z∣s)]=Es∈τ​[H(st​)−H(s∣z)]=Ez∼qϕ​(z),s∼πθ​(s∣z)​[logqϕ​(s∣z)−logπθ​(s)]≈Ez∼qϕ​(z),s∼πθ​(s∣z)​[logqϕ​(s∣z)−logqϕ​(s)]​; discard the order of states.; by definition of MI.; assume learned marginal distr. matches policy.​ . We can set the reward as $ log q_ phi(s vert z) - log q_ phi(s)$, as shown in the green part in the equation above. In order to balance between task-specific exploration (as in red below) and latent skill matching (as in blue below) , a parameter $ lambda in [0, 1]$ is added. Each realization of $z sim q_ phi(z)$ induces a reward function $r_z(s)$ (remember that reward + CMP =&gt; MDP) as follows: . rz(s)=λlog⁡qϕ(s∣z)−log⁡qϕ(s)=λlog⁡qϕ(s∣z)−log⁡qϕ(s∣z)qϕ(z)qϕ(z∣s)=λlog⁡qϕ(s∣z)−log⁡qϕ(s∣z)−log⁡qϕ(z)+log⁡qϕ(z∣s)=(λ−1)log⁡qϕ(s∣z)+log⁡qϕ(z∣s)+C begin{aligned} r_z(s) &amp;= lambda log q_ phi(s|z) - log q_ phi(s) &amp;= lambda log q_ phi(s|z) - log frac{q_ phi(s|z) q_ phi(z)}{q_ phi(z|s)} &amp;= lambda log q_ phi(s|z) - log q_ phi(s|z) - log q_ phi(z) + log q_ phi(z|s) &amp;= ( lambda - 1) log color{red}{q_ phi(s|z)} + color{blue}{ log q_ phi(z|s)} + C end{aligned}rz​(s)​=λlogqϕ​(s∣z)−logqϕ​(s)=λlogqϕ​(s∣z)−logqϕ​(z∣s)qϕ​(s∣z)qϕ​(z)​=λlogqϕ​(s∣z)−logqϕ​(s∣z)−logqϕ​(z)+logqϕ​(z∣s)=(λ−1)logqϕ​(s∣z)+logqϕ​(z∣s)+C​ . Fig. 9. The algorithm of CARML. (Image source: Jabri, et al 2019) Learning a latent skill space can be done in different ways, such as in Hausman, et al. 2018. The goal of their approach is to learn a task-conditioned policy, $ pi(a vert s, t^{(i)})$, where $t^{(i)}$ is from a discrete list of $N$ tasks, $ mathcal{T} = [t^{(1)}, dots, t^{(N)}]$. However, rather than learning $N$ separate solutions, one per task, it would be nice to learn a latent skill space so that each task could be represented in a distribution over skills and thus skills are reused between tasks. The policy is defined as $ pi_ theta(a vert s,t) = int pi_ theta(a vert z,s,t) p_ phi(z vert t) mathrm{d}z$, where $ pi_ theta$ and $p_ phi$ are policy and embedding networks to learn, respectively. If $z$ is discrete, i.e. drawn from a set of $K$ skills, then the policy becomes a mixture of $K$ sub-policies. The policy training uses SAC and the dependency on $z$ is introduced in the entropy term. Curriculum through Distillation The motivation for the progressive neural network (Rusu et al. 2016) architecture is to efficiently transfer learned skills between different tasks and in the meantime avoid catastrophic forgetting. The curriculum is realized through a set of progressively stacked neural network towers (or “columns”, as in the paper). A progressive network has the following structure: . It starts with a single column containing $L$ layers of neurons, in which the corresponding activation layers are labelled as $h^{(1)}_i, i=1, dots, L$. We first train this single-column network for one task to convergence, achieving parameter config $ theta^{(1)}$. | Once switch to the next task, we need to add a new column to adapt to the new context while freezing $ theta^{(1)}$ to lock down the learned skills from the previous task. The new column has activation layers labelled as $h^{(2)}_i, i=1, dots, L$, and parameters $ theta^{(2)}$. | Step 2 can be repeated with every new task. The $i$-th layer activation in the $k$-th column depends on the previous activation layers in all the existing columns: | . h(k)_i=f(Wi(k)hi−1(k)+∑j&lt;kUi(k:j)h(j)_i−1)h^{(k)} _i = f(W^{(k)}_i h^{(k)}_{i-1} + sum_{j &lt; k} U_i^{(k:j)} h^{(j)} _{i-1})h(k)_i=f(Wi(k)​hi−1(k)​+j&lt;k∑​Ui(k:j)​h(j)_i−1) . where $W^{(k)}_i$ is the weight matrix of the layer $i$ in the column $k$; $U_i^{(k:j)}, j &lt; k$ are the weight matrices for projecting the layer $i-1$ of the column $j$ to the layer $i$ of column $k$ ($ j &lt; k $). The above weights matrices should be learned. $f(.)$ is a non-linear activation function by choice. . Fig. 10. The progressive neural network architecture. (Image source: Rusu, et al. 2017) The paper experimented with Atari games by training a progressive network on multiple games to check whether features learned in one game can transfer to another. That is indeed the case. Though interestingly, learning a high dependency on features in the previous columns does not always indicate good transfer performance on the new task. One hypothesis is that features learned from the old task might introduce biases into the new task, leading to policy getting trapped in a sub-optimal solution. Overall, the progressive network works better than only fine-tuning the top layer and can achieve similar transfer performance as fine-tuning the entire network. One use case for the progressive network is to do sim2real transfer (Rusu, et al. 2017), in which the first column is trained in simulator with a lot of samples and then the additional columns (could be for different real-world tasks) are added and trained with a few real data samples. Czarnecki, et al. (2018) proposed another RL training framework, Mix and Match (short for M&amp;M) to provide curriculum through coping knowledge between agents. Given a sequence of agents from simple to complex, $ pi_1, dots, pi_K$, each parameterized with some shared weights (e.g. by shared some lower common layers). M&amp;M trains a mixture of agents, but only the final performance of the most complex one $ pi_K$ matters. In the meantime, M&amp;M learns a categorical distribution $c sim text{Categorical}(1, dots, K vert alpha)$ with pmf $p(c=i) = alpha_i$ probability to pick which policy to use at a given time. The mixed M&amp;M policy is a simple weighted sum: $ pi_ text{mm}(a vert s) = sum_{i=1}^K alpha_i pi_i(a vert s)$. Curriculum learning is realized by dynamically adjusting $ alpha_i$, from $ alpha_K=0$ to $ alpha_K=1$. The tuning of $ alpha$ can be manual or through population-based training. To encourage cooperation rather than competition among policies, besides the RL loss $ mathcal{L}_ text{RL}$, another distillation -like loss $ mathcal{L}_ text{mm}( theta)$ is added. The knowledge transfer loss $ mathcal{L}_ text{mm}( theta)$ measures the KL divergence between two policies, $ propto D_ text{KL}( pi_{i}(. vert s) | pi_j(. vert s))$ for $i &lt; j$. It encourages complex agents to match the simpler ones early on. The final loss is $ mathcal{L} = mathcal{L}_ text{RL}( theta vert pi_ text{mm}) + lambda mathcal{L}_ text{mm}( theta)$. . Fig. 11. The Mix &amp; Match architecture for training a mixture of policies. (Image source:Czarnecki, et al., 2018) .",
            "url": "https://tayalmanan28.github.io/my_blogs/robotics/2022/09/13/Curriculum-for-Reinforcement-Learning.html",
            "relUrl": "/robotics/2022/09/13/Curriculum-for-Reinforcement-Learning.html",
            "date": " • Sep 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Some Tips for working with people who are smarter than you",
            "content": "It’s entrepreneurship gospel–hire those who are smarter than you. . There’s no mystery to this wisdom. Clever, skilled employees require less management, constantly teach you new things, and shore up your weaknesses. But while the rationale behind this oft-repeated tip is simple, executing it isn’t. . The problem is people have egos and insecurities, and being surrounded by folks who are smarter than you can make you feel bad. Few would freely admit it, of course, but this is often the reason people fail to heed this obvious advice. Even the most assured among us can see their confidence erode when they’re surrounded by geniuses. . So is it possible to rewire yourself to reap the benefits of a team of super-achievers without suffering self-esteem erosion, or does enjoying the company of a room full of brainiacs require an ego transplant and or years of therapy? It’s possible indeed, answered a parade of responders on question-and-answer site Quora recently. Veterans of some of the brainiest companies in the world shared their experiences of feeling like the team dunce, as well as their advice for others who are suffering from self doubt. Here are some of their top tips. . Know Your Strengths . If you’ve been hired (or gotten to be the owner of a business), you must have something to offer the team. Focus on the strengths you possess, rather than the skills or knowledge you lack. . Andy Johns, an early Facebook employee, offers a greater metaphor for this bit of advice: “If a punter/field goal kicker showed up to practice with a new football team and thought ‘Crap, all of these guys are bigger and more athletic than me!’ and tried to outperform the wide receivers and running backs, they would fail miserably. But they don’t. They focus on the intersection of their skills and experience and they focus on being the best punter/field goal kicker in the game. Within that more tightly defined role, they aim to perform.” . Be the King (or Queen) of Questions . When surrounded by smart people, your first impulse may be to hide your ignorance, but that’s the wrong way to go, according to Doug Edwards, Google’s first director of marketing and brand management, who joined the company in 1999. If you don’t ask questions, you’ll never learn. . “It’s much better to appear uninformed than to give the impression you know something you don’t, which can come back to haunt you. I used to ask the engineers to explain things to me ‘in little baby words that I can understand,’” he remembers. Engineer B. Nguyen agrees in snappy style: “The credo here really is, ‘The only stupid question is the one not asked’. So ask and ask often.” . Take Your Time . Getting comfortable among a team of whiz kids isn’t something that happens over night, many of the responders warn. Getting to know, and learning from, truly smart collaborators can be a lengthy process, so don’t expect to wake up a week later and feel totally comfortable. . “It took several years to be one of the top people,” says Quora employee Jay Wacker of his experience arriving at the University of California, Berkeley, for grad school. . Imagine the Alternative . Leo Polovets, an ex-LinkedIn and Google employee who has a resume full of genius-filled environments, offers a simple but powerful trick to keep your perspective. . “How did I adjust?” he asks, “I considered the alternative to working with smarter people, and that was even less enticing. In my experience, working with people who are less smart/experienced than you is less educational, less rewarding, and more frustrating than working with those who are smarter/experienced. Working with great peers will help you up your game.” . Remember What You Can Control . Sure, you were dealt whatever genetic hand you have when it comes to innate intellectual horsepower (though smarts may be more malleable than many of us believe), but there is still one giant factor 100% under your own control. As Gwynne Shotwell reminded the audience recently from the Women 2.0 Conference stage: “You can’t control whether you’re the smartest person in the room, but you can certainly control whether you’re the most prepared.” . “You can’t get smarter. But you can always work harder than someone else,” agrees Farhan Thawar, VP Engineeringat at XtremeLabs, on Quora, “so the adjustment is to work extremely hard at your craft until you feel like you fit in.” Christina Bonnington, a writer for Wired, also concurs: “Pedigree doesn’t mean anything. Work ethic is everything.” . Read . This one may be simple, but it was one of the most common bits of advice. Charles Martin, for example, recalls that “a few year ago, I had a chance to work as a quant in a very large and successful hedge fund, and with former professors from MIT. The first thing I did was read the thesis of the managing director, so I could get ‘into his head’ and learn how he thinks. This made it much easier to work with him as a colleague.” Consultant Mark Simchock boils it down to “read a lot.” . Also, aim for diversity in what you pick up. You probably aren’t going to out-expert the experts, but you could contribute that key piece of out-of-left-field knowledge. “I tried to absorb all the information I could from outside so that occasionally I could contribute a perspective that was different, without being completely idiotic,” reports Edwards. . Don’t Compete . The more you’re competing the less you’re learning and accomplishing. “Don’t start competing. The day you’ll accept the fact that there will always exist smarter people, learning will become much easier,” advises Rajay Chamria. “Don’t compete, contemplate,” agrees Saraswati Chandra. . Credits . The Blog is adopted from here .",
            "url": "https://tayalmanan28.github.io/my_blogs/self%20development/management/2022/07/15/Tips-for-working-with-people-smarter-than-you.html",
            "relUrl": "/self%20development/management/2022/07/15/Tips-for-working-with-people-smarter-than-you.html",
            "date": " • Jul 15, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Domain Randomization",
            "content": "In Robotics, one of the hardest problems is how to make your model transfer to the real world. Due to the sample inefficiency of deep RL algorithms and the cost of data collection on real robots, we often need to train models in a simulator which theoretically provides an infinite amount of data. However, the reality gap between the simulator and the physical world often leads to failure when working with physical robots. The gap is triggered by an inconsistency between physical parameters (i.e. friction, kp, damping, mass, density) and, more fatally, the incorrect physical modeling (i.e. collision between soft surfaces). To close the sim2real gap, we need to improve the simulator and make it closer to reality. A couple of approaches: . System identification System identification is to build a mathematical model for a physical system; in the context of RL, the mathematical model is the simulator. To make the simulator more realistic, careful calibration is necessary. | Unfortunately, calibration is expensive. Furthermore, many physical parameters of the same machine might vary significantly due to temperature, humidity, positioning or its wear-and-tear in time. | . | Domain adaptation Domain adaptation (DA) refers to a set of transfer learning techniques developed to update the data distribution in sim to match the real one through a mapping or regularization enforced by the task model. | Many DA models, especially for image classification or end-to-end image-based RL task, are built on adversarial loss or GAN. | . | Domain randomization With Domain randomization (DR), we are able to create a variety of simulated environments with randomized properties and train a model that works across all of them. | Likely this model can adapt to the real-world environment, as the real system is expected to be one sample in that rich distribution of training variations. | . | . Both DA and DR are unsupervised. Compared to DA which requires a decent amount of real data samples to capture the distribution, DR may need only a little or no real data. DR is the focus of this post. . What is Domain Randomization? . To make the definition more general, let us call the environment that we have full access to (i.e. simulator) source domain and the environment that we would like to transfer the model to target domain (i.e. physical world). Training happens in the source domain. We can control a set of $N$ randomization parameters in the source domain $e_ xi$ with a configuration $ xi$, sampled from a randomization space, $ xi in Xi subset mathbb{R}^N$. During policy training, episodes are collected from source domain with randomization applied. Thus the policy is exposed to a variety of environments and learns to generalize. The policy parameter $ theta$ is trained to maximize the expected reward $R(.)$ average across a distribution of configurations: θ∗=arg⁡max⁡θEξ∼Ξ[Eπθ,τ∼eξ[R(τ)]] theta^* = arg max_ theta mathbb{E}_{ xi sim Xi} [ mathbb{E}_{ pi_ theta, tau sim e_ xi} [R( tau)]]θ∗=argmaxθ​Eξ∼Ξ​[Eπθ​,τ∼eξ​​[R(τ)]] where $ tau_ xi$ is a trajectory collected in source domain randomized with $ xi$. In a way, “discrepancies between the source and target domains are modeled as variability in the source domain.” (quote from Peng et al. 2018). . Uniform Domain Randomization . In the original form of DR (Tobin et al, 2017; Sadeghi et al. 2016), each randomization parameter $ xi_i$ is bounded by an interval, $ xi_i in [ xi_i^ text{low}, xi_i^ text{high}], i=1, dots,N$ and each parameter is uniformly sampled within the range. The randomization parameters can control appearances of the scene, including but not limited to the followings (see Fig. 2). A model trained on simulated and randomized images is able to transfer to real non-randomized images. . Position, shape, and color of objects, | Material texture, | Lighting condition, | Random noise added to images, | Position, orientation, and field of view of the camera in the simulator. | . . Physical dynamics in the simulator can also be randomized (Peng et al. 2018). Studies have showed that a recurrent policy can adapt to different physical dynamics including the partially observable reality. A set of physical dynamics features include but are not limited to: . Mass and dimensions of objects, | Mass and dimensions of robot bodies, | Damping, kp, friction of the joints, | Gains for the PID controller (P term), | Joint limit, | Action delay, | Observation noise. | . With visual and dynamics DR, at OpenAI Robotics, we were able to learn a policy that works on real dexterous robot hand (OpenAI, 2018). Our manipulation task is to teach the robot hand to rotate an object continously to achieve 50 successive random target orientations. The sim2real gap in this task is very large, due to (a) a high number of simultaneous contacts between the robot and the object and (b) imperfect simulation of object collision and other motions. At first, the policy could barely survive for more than 5 seconds without dropping the object. But with the help of DR, the policy evolved to work surprisingly well in reality eventually. . Why does Domain Randomization Work? . Now you may ask, why does domain randomization work so well? The idea sounds really simple. Here are two non-exclusive explanations I found most convincing. . DR as Optimization . One idea (Vuong, et al, 2019) is to view learning randomization parameters in DR as a bilevel optimization. Assuming we have access to the real environment $e_ text{real}$ and the randomization config is sampled from a distribution parameterized by $ phi$, $ xi sim P_ phi( xi)$, we would like to learn a distribution on which a policy $ pi_ theta$ is trained on can achieve maximal performance in $e_ text{real}$: . ϕ∗=arg⁡min⁡ϕL(πθ∗(ϕ);ereal)where θ∗(ϕ)=arg⁡min⁡θEξ∼Pϕ(ξ)[L(πθ;eξ)] begin{aligned} &amp; phi^* = arg min_{ phi} mathcal{L}( pi_{ theta^*( phi)}; e_ text{real}) text{where } &amp; theta^*( phi) = arg min_ theta mathbb{E}_{ xi sim P_ phi( xi)}[ mathcal{L}( pi_ theta; e_ xi)] end{aligned}where ​ϕ∗=argϕmin​L(πθ∗(ϕ)​;ereal​)θ∗(ϕ)=argθmin​Eξ∼Pϕ​(ξ)​[L(πθ​;eξ​)]​ . where $ mathcal{L}( pi; e)$ is the loss function of policy $ pi$ evaluated in the environment $e$. Although randomization ranges are hand-picked in uniform DR, it often involves domain knowledge and a couple rounds of trial-and-error adjustment based on the transfer performance. Essentially this is a manual optimization process on tuning $ phi$ for the optimal $ mathcal{L}( pi_{ theta^*( phi)}; e_ text{real})$. Guided domain randomization in the next section is largely inspired by this view, aiming to do bilevel optimization and learn the best parameter distribution automatically. . DR as Meta-Learning . In our learning dexterity project (OpenAI, 2018), we trained an LSTM policy to generalize across different environmental dynamics. We observed that once a robot achieved the first rotation, the time it needed for the following successes was much shorter. Also, a FF policy without memory was found not able to transfer to a physical robot. Both are evidence of the policy dynamically learning and adapting to a new environment. In some ways, domain randomization composes a collection of different tasks. Memory in the recurrent network empowers the policy to achieve meta-learning across tasks and further work on a real-world setting. . Guided Domain Randomization . The vanilla DR assumes no access to the real data, and thus the randomization config is sampled as broadly and uniformly as possible in sim, hoping that the real environment could be covered under this broad distribution. It is reasonable to think of a more sophisticated strategy — replacing uniform sampling with guidance from task performance, real data, or simulator. One motivation for guided DR is to save computation resources by avoiding training models in unrealistic environments. Another is to avoid infeasible solutions that might arise from overly wide randomization distributions and thus might hinder successful policy learning. . Optimization for Task Performance . Say we train a family of policies with different randomization parameters $ xi sim P_ phi( xi)$, where $P_ xi$ is the distribution for $ xi$ parameterized by $ phi$. Later we decide to try every one of them on the downstream task in the target domain (i.e. control a robot in reality or evaluate on a validation set) to collect feedback. This feedback tells us how good a configuration $ xi$ is and provides signals for optimizing $ phi$. Inspired by NAS, AutoAugment (Cubuk, et al. 2018) frames the problem of learning best data augmentation operations (i.e. shearing, rotation, invert, etc.) for image classification as an RL problem. Note that AutoAugment is not proposed for sim2real transfer, but falls in the bucket of DR guided by task performance. Individual augmentation configuration is tested on the evaluation set and the performance improvement is used as a reward to train a PPO policy. This policy outputs different augmentation strategies for different datasets; for example, for CIFAR-10 AutoAugment mostly picks color-based transformations, while ImageNet prefers geometric based. Ruiz (2019) considered the task feedback as reward in RL problem and proposed a RL-based method, named “learning to simulate”, for adjusting $ xi$. A policy is trained to predict $ xi$ using performance metrics on the validation data of the main task as rewards, which is modeled as a multivariate Gaussian. Overall the idea is similar to AutoAugment, applying NAS on data generation. According to their experiments, even if the main task model is not converged, it still can provide a reasonable signal to the data generation policy. . Evolutionary algorithm is another way to go, where the feedback is treated as fitness for guiding evolution (Yu et al, 2019). In this study, they used CMA-ES (covariance matrix adaptation evolution strategy) while fitness is the performance of a $ xi$-conditional policy in target environment. In the appendix, they compared CMA-ES with other ways of modeling the dynamics of $ xi$, including Bayesian optimization or a neural network. The main claim was those methods are not as stable or sample efficient as CMA-ES. Interestly, when modeling $P( xi)$ as a neural network, LSTM is found to notably outperform FF. Some believe that sim2real gap is a combination of appearance gap and content gap; i.e. most GAN-inspired DA models focus on appearance gap. Meta-Sim (Kar, et al. 2019) aims to close the content gap by generating task-specific synthetic datasets. Meta-Sim uses self-driving car training as an example and thus the scene could be very complicated. In this case, the synthetic scenes are parameterized by a hierarchy of objects with properties (i.e., location, color) as well as relationships between objects. The hierarchy is specified by a probabilistic scene grammar akin to structure domain randomization (SDR; Prakash et al., 2018) and it is assumed to be known beforehand. A model $G$ is trained to augment the distribution of scene properties $s$ by following: . Learn the prior first: pre-train $G$ to learn the identity function $G(s) = s$. | Minimize MMD loss between the real and sim data distributions. This involves backpropagation through non-differentiable renderer. The paper computes it numerically by perturbing the attributes of $G(s)$. | Minimize REINFORCE task loss when trained on synthetic data but evaluated on real data. Again, very similar to AutoAugment. | . Unfortunately, this family of methods are not suitable for sim2real case. Either an RL policy or an EA model requires a large number of real samples. And it is really expensive to include real-time feedback collection on a physical robot into the training loop. Whether you want to trade less computation resource for real data collection would depend on your task. . Match Real Data Distribution . Using real data to guide domain randomization feels a lot like doing system identification or DA. The core idea behind DA is to improve the synthetic data to match the real data distribution. In the case of real-data-guided DR, we would like to learn the randomization parameters $ xi$ that bring the state distribution in simulator close to the state distribution in the real world. The SimOpt model (Chebotar et al, 2019) is trained under an initial randomization distribution $P_ phi( xi)$ first, getting a policy $ pi_{ theta, P_ phi}$. Then this policy is deployed on both simulator and physical robot to collect trajectories $ tau_ xi$ and $ tau_ text{real}$ respectively. The optimization objective is to minimize the discrepancy between sim and real trajectories: . ϕ∗=arg⁡min⁡ϕEξ∼Pϕ(ξ)[Eπθ,Pϕ[D(τsim,τreal)]] phi^* = arg min_{ phi} mathbb{E}_{ xi sim P_ phi( xi)} [ mathbb{E}_{ pi_{ theta, P_ phi}} [D( tau_ text{sim}, tau_ text{real})]]ϕ∗=argϕmin​Eξ∼Pϕ​(ξ)​[Eπθ,Pϕ​​​[D(τsim​,τreal​)]] . where $D(.)$ is a trajectory-based discrepancy measure. Like the “Learning to simulate” paper, SimOpt also has to solve the tricky problem of how to propagate gradient through non-differentiable simulator. It used a method called relative entropy policy search, see paper for more details. . . RCAN (James et al., 2019), short for “Randomized-to-Canonical Adaptation Networks”, is a nice combination of DA and DR for end-to-end RL tasks. An image-conditional GAN (cGAN) is trained in sim to translate a domain-randomized image into a non-randomized version (aka “canonical version”). Later the same model is used to translate real images into corresponding simulated version so that the agent would consume consistent observation as what it has encountered in training. Still, the underlying assumption is that the distribution of domain-randomized sim images is broad enough to cover real-world samples. . . The RL model is trained end-to-end in a simulator to do vision-based robot arm grasping. Randomization is applied at each timestep, including the position of tray divider, objects to grasp, random textures, as well as the position, direction, and color of the lighting. The canonical version is the default simulator look. RCAN is trying to learn a generator $G$: randomized image $ to$ {canonical image, segmentation, depth} where segmentation masks and depth images are used as auxiliary tasks. RCAN had a better zero-shot transfer compared to uniform DR, although both were shown to be worse than the model trained on only real images. Conceptually, RCAN operates in a reverse direction of GraspGAN which translates synthetic images into real ones by domain adaptation. . Guided by Data in Simulator . Network-driven domain randomization (Zakharov et al., 2019), also known as DeceptionNet, is motivated by learning which randomizations are actually useful to bridge the domain gap for image classification tasks. Randomization is applied through a set of deception modules with encoder-decoder architecture. The deception modules are specifically designed to transform images; such as change backgrounds, add distortion, change lightings, etc. The other recognition network handles the main task by running classification on transformed images. The training involves two steps: . With the recognition network fixed, maximize the difference between the prediction and the labels by applying reversed gradients during backpropagation. So that the deception module can learn the most confusing tricks. | With the deception modules fixed, train the recognition network with input images altered. | . . The feedback for training deception modules is provided by the downstream classifier. But rather than trying to maximize the task performance like the section above, the randomization modules aim to create harder cases. One big disadvantage is you need to manually design different deception modules for different datasets or tasks, making it not easily scalable. Given the fact that it is zero-shot, the results are still worse than SOTA DA methods on MNIST and LineMOD. Similarly, Active domain randomization (ADR; Mehta et al., 2019) also relies on sim data to create harder training samples. ADR searches for the most informative environment variations within the given randomization ranges, where the informativeness is measured as the discrepancies of policy rollouts in randomized and reference (original, non-randomized) environment instances. Sounds a bit like SimOpt? Well, noted that SimOpt measures the discrepancy between sim and real rollouts, while ADR measures between randomized and non-randomized sim, avoiding the expensive real data collection part. . . Precisely the training happens as follows: . Given a policy, run it on both reference and randomized envs and collect two sets of trajectories respectively. | Train a discriminator model to tell whether a rollout trajectory is randomized apart from reference run. The predicted $ log p$ (probability of being randomized) is used as reward. The more different randomized and reference rollouts, the easier the prediction, the higher the reward. | The intuition is that if an environment is easy, the same policy agent can produce similar trajectories as in the reference one. Then the model should reward and explore hard environments by encouraging different behaviors. | The reward by discriminator is fed into Stein Variational Policy Gradient SVPG particles, outputting a diverse set of randomization configurations. | The idea of ADR is very appealing with two small concerns. The similarity between trajectories might not be a good way to measure the env difficulty when running a stochastic policy. The sim2real results look unfortunately not as exciting, but the paper pointed out the win being ADR explores a smaller range of randomization parameters. | .",
            "url": "https://tayalmanan28.github.io/my_blogs/raspberry-pi/robotics/2022/07/01/domain-randomization.html",
            "relUrl": "/raspberry-pi/robotics/2022/07/01/domain-randomization.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Safe Reinforcement Learning",
            "content": "Safe Reinforcement Learning . Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. . SAC-RCBF . “Safe Model-Based Reinforcement Learning using Robust Control Barrier Functions”. Specifically, an implementation of SAC + Robust Control Barrier Functions (RCBFs) for safe reinforcement learning in two custom environments. . While exploring, an RL agent can take actions that lead the system to unsafe states. Here, we use a differentiable RCBF safety layer that minimially alters (in the least-squares sense) the actions taken by the RL agent to ensure the safety of the agent. . Robust Control Barrier Functions (RCBFs) . In this work, we focus on RCBFs that are formulated with respect to differential inclusions of the following form: . Here D(x) is a disturbance set unkown apriori to the robot, which we learn online during traing via Gaussian Processes (GPs). The underlying library is GPyTorch. . The QP used to ensure the system’s safety is given by: . . where h(x) is the RCBF, and u_RL is the action outputted by the RL policy. As such, the final (safe) action taken in the environment is given by u = u_RL + u_RCBF as shown in the following diagram: . . Coupling RL &amp; RCBFs to Improve Training Performance . The above is sufficient to ensure the safety of the system, however, we would also like to improve the performance of the learning by letting the RCBF layer guide the training. This is achieved via: . Using a differentiable version of the safety layer that allows us to backpropagte through the RCBF based Quadratic Program (QP) resulting in an end-to-end policy. | Using the GPs and the dynamics prior to generate synthetic data (model-based RL). | .",
            "url": "https://tayalmanan28.github.io/my_blogs/reinforcement-learning/robotics/2022/05/02/Safe-Reinforcement-Learning.html",
            "relUrl": "/reinforcement-learning/robotics/2022/05/02/Safe-Reinforcement-Learning.html",
            "date": " • May 2, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Meta Learning",
            "content": "A good machine learning model often requires training with a large number of samples. Humans, in contrast, learn new concepts and skills much faster and more efficiently. Kids who have seen cats and birds only a few times can quickly tell them apart. People who know how to ride a bike are likely to discover the way to ride a motorcycle fast with little or even no demonstration. Is it possible to design a machine learning model with similar properties - learning new concepts and skills fast with a few training examples? That’s essentially what meta-learning aims to solve. . We expect a good meta-learning model capable of well adapting or generalizing to new tasks and new environments that have never been encountered during training time. The adaptation process, essentially a mini learning session, happens during test but with a limited exposure to the new task configurations. Eventually, the adapted model can complete new tasks. This is why meta-learning is also known as learning to learn. . The tasks can be any well-defined family of machine learning problems: supervised learning, reinforcement learning, etc. For example, here are a couple concrete meta-learning tasks: . A classifier trained on non-cat images can tell whether a given image contains a cat after seeing a handful of cat pictures. | A game bot is able to quickly master a new game. | A mini robot completes the desired task on an uphill surface during test even through it was only trained in a flat surface environment. | . Define the Meta-Learning Problem . In this post, we focus on the case when each desired task is a supervised learning problem like image classification. There is a lot of interesting literature on meta-learning with reinforcement learning problems (aka “Meta Reinforcement Learning”), but we would not cover them here. . A Simple View . A good meta-learning model should be trained over a variety of learning tasks and optimized for the best performance on a distribution of tasks, including potentially unseen tasks. Each task is associated with a dataset $ mathcal{D}$, containing both feature vectors and true labels. The optimal model parameters are: . θ∗=arg⁡min⁡θED∼p(D)[Lθ(D)] theta^* = arg min_ theta mathbb{E}_{ mathcal{D} sim p( mathcal{D})} [ mathcal{L}_ theta( mathcal{D})]θ∗=argθmin​ED∼p(D)​[Lθ​(D)] . It looks very similar to a normal learning task, but one dataset is considered as one data sample. . Few-shot classification is an instantiation of meta-learning in the field of supervised learning. The dataset $ mathcal{D}$ is often split into two parts, a support set $S$ for learning and a prediction set $B$ for training or testing, $ mathcal{D}= langle S, B rangle$. Often we consider a K-shot N-class classification task: the support set contains K labelled examples for each of N classes. . . Fig. 1. An example of 4-shot 2-class image classification. (Image thumbnails are from Pinterest) . Training in the Same Way as Testing . A dataset $ mathcal{D}$ contains pairs of feature vectors and labels, $ mathcal{D} = {( mathbf{x}_i, y_i)}$ and each label belongs to a known label set $ mathcal{L}^ text{label}$. Let’s say, our classifier $f_ theta$ with parameter $ theta$ outputs a probability of a data point belonging to the class $y$ given the feature vector $ mathbf{x}$, $P_ theta(y vert mathbf{x})$. . The optimal parameters should maximize the probability of true labels across multiple training batches $B subset mathcal{D}$: . begin{aligned} theta^* &amp;= { arg max}_{ theta} mathbb{E}_{( mathbf{x}, y) in mathcal{D}}[P_ theta(y vert mathbf{x})] &amp; theta^* &amp;= { arg max}_{ theta} mathbb{E}_{B subset mathcal{D}}[ sum_{( mathbf{x}, y) in B}P_ theta(y vert mathbf{x})] &amp; scriptstyle{ text{; trained with mini-batches.}} end{aligned} . In few-shot classification, the goal is to reduce the prediction error on data samples with unknown labels given a small support set for “fast learning” (think of how “fine-tuning” works). To make the training process mimics what happens during inference, we would like to “fake” datasets with a subset of labels to avoid exposing all the labels to the model and modify the optimization procedure accordingly to encourage fast learning: . Sample a subset of labels, $L subset mathcal{L}^ text{label}$. | Sample a support set $S^L subset mathcal{D}$ and a training batch $B^L subset mathcal{D}$. Both of them only contain data points with labels belonging to the sampled label set $L$, $y in L, forall (x, y) in S^L, B^L$. | The support set is part of the model input. | The final optimization uses the mini-batch $B^L$ to compute the loss and update the model parameters through backpropagation, in the same way as how we use it in the supervised learning. | You may consider each pair of sampled dataset $(S^L, B^L)$ as one data point. The model is trained such that it can generalize to other datasets. Symbols in red are added for meta-learning in addition to the supervised learning objective. . theta = arg max_ theta color{red}{E_{L subset mathcal{L}}[} E_{ color{red}{S^L subset mathcal{D}, }B^L subset mathcal{D}} [ sum_{(x, y) in B^L} P_ theta(x, y color{red}{, S^L})] color{red}{]} . The idea is to some extent similar to using a pre-trained model in image classification (ImageNet) or language modeling (big text corpora) when only a limited set of task-specific data samples are available. Meta-learning takes this idea one step further, rather than fine-tuning according to one down-steam task, it optimizes the model to be good at many, if not all. . Learner and Meta-Learner . Another popular view of meta-learning decomposes the model update into two stages: . A classifier $f_ theta$ is the “learner” model, trained for operating a given task; | In the meantime, a optimizer $g_ phi$ learns how to update the learner model’s parameters via the support set $S$, $ theta’ = g_ phi( theta, S)$. | . Then in final optimization step, we need to update both $ theta$ and $ phi$ to maximize: . mathbb{E}_{L subset mathcal{L}}[ mathbb{E}_{S^L subset mathcal{D}, B^L subset mathcal{D}} [ sum_{( mathbf{x}, y) in B^L} P_{g_ phi( theta, S^L)}(y vert mathbf{x})]] . Common Approaches . There are three common approaches to meta-learning: metric-based, model-based, and optimization-based. Oriol Vinyals has a nice summary in his talk at meta-learning symposium @ NIPS 2018: . Model-based . Metric-based . Optimization-based . Key idea . RNN; memory . Metric learning . Gradient descent . How $P_ theta(y vert mathbf{x})$ is modeled? . $f_ theta( mathbf{x}, S)$ . $ sum_{( mathbf{x}_i, y_i) in S} k_ theta( mathbf{x}, mathbf{x}_i)y_i$ (*) . $P_{g_ phi( theta, S^L)}(y vert mathbf{x})$ . (*) $k_ theta$ is a kernel function measuring the similarity between $ mathbf{x}_i$ and $ mathbf{x}$. . Next we are gonna review classic models in each approach. . Metric-Based . The core idea in metric-based meta-learning is similar to nearest neighbors algorithms (i.e., k-NN classificer and k-means clustering) and kernel density estimation. The predicted probability over a set of known labels $y$ is a weighted sum of labels of support set samples. The weight is generated by a kernel function $k_ theta$, measuring the similarity between two data samples. . P_θ(y∣x,S)=∑_(x_i,y_i)∈Sk_θ(x,x_i)y_iP _ theta(y vert mathbf{x}, S) = sum _{( mathbf{x} _i, y _i) in S} k _ theta( mathbf{x}, mathbf{x} _i)y _iP_θ(y∣x,S)=∑_(x_i,y_i)∈Sk_θ(x,x_i)y_i . To learn a good kernel is crucial to the success of a metric-based meta-learning model. Metric learning is well aligned with this intention, as it aims to learn a metric or distance function over objects. The notion of a good metric is problem-dependent. It should represent the relationship between inputs in the task space and facilitate problem solving. . All the models introduced below learn embedding vectors of input data explicitly and use them to design proper kernel functions. . Convolutional Siamese Neural Network . The Siamese Neural Network is composed of two twin networks and their outputs are jointly trained on top with a function to learn the relationship between pairs of input data samples. The twin networks are identical, sharing the same weights and network parameters. In other words, both refer to the same embedding network that learns an efficient embedding to reveal relationship between pairs of data points. . Koch, Zemel &amp; Salakhutdinov (2015) proposed a method to use the siamese neural network to do one-shot image classification. First, the siamese network is trained for a verification task for telling whether two input images are in the same class. It outputs the probability of two images belonging to the same class. Then, during test time, the siamese network processes all the image pairs between a test image and every image in the support set. The final prediction is the class of the support image with the highest probability. . . Fig. 2. The architecture of convolutional siamese neural network for few-show image classification. . First, convolutional siamese network learns to encode two images into feature vectors via a embedding function $f_ theta$ which contains a couple of convolutional layers. | The L1-distance between two embeddings is $ vert f_ theta( mathbf{x}_i) - f_ theta( mathbf{x}_j) vert$. | The distance is converted to a probability $p$ by a linear feedforward layer and sigmoid. It is the probability of whether two images are drawn from the same class. | Intuitively the loss is cross entropy because the label is binary. | p(x_i,x_j)=σ(W∣f_θ(x_i)−f_θ(x_j)∣)L(B)=∑_(x_i,x_j,y_i,y_j)∈B1_y_i=y_jlog⁡p(x_i,x_j)+(1−1_y_i=y_j)log⁡(1−p(x_i,x_j)) begin{aligned} p( mathbf{x} _i, mathbf{x} _j) &amp;= sigma( mathbf{W} vert f _ theta( mathbf{x} _i) - f _ theta( mathbf{x} _j) vert) mathcal{L}(B) &amp;= sum _{( mathbf{x} _i, mathbf{x} _j, y _i, y _j) in B} mathbf{1} _{y _i=y _j} log p( mathbf{x} _i, mathbf{x} _j) + (1- mathbf{1} _{y _i=y _j}) log (1-p( mathbf{x} _i, mathbf{x} _j)) end{aligned}p(x_i,x_j)​=σ(W∣f_θ(x_i)−f_θ(x_j)∣)L(B)​=∑_(x_i,x_j,y_i,y_j)∈B1_y_i=y_jlogp(x_i,x_j)+(1−1_y_i=y_j)log(1−p(x_i,x_j))​ . Images in the training batch $B$ can be augmented with distortion. Of course, you can replace the L1 distance with other distance metric, L2, cosine, etc. Just make sure they are differential and then everything else works the same. . Given a support set $S$ and a test image $ mathbf{x}$, the final predicted class is: . c^_S(x)=c(arg⁡max⁡_x_i∈SP(x,x_i)) hat{c} _S( mathbf{x}) = c( arg max _{ mathbf{x} _i in S} P( mathbf{x}, mathbf{x} _i))c^_S(x)=c(argmax_x_i∈SP(x,x_i)) . where $c( mathbf{x})$ is the class label of an image $ mathbf{x}$ and $ hat{c}(.)$ is the predicted label. . The assumption is that the learned embedding can be generalized to be useful for measuring the distance between images of unknown categories. This is the same assumption behind transfer learning via the adoption of a pre-trained model; for example, the convolutional features learned in the model pre-trained with ImageNet are expected to help other image tasks. However, the benefit of a pre-trained model decreases when the new task diverges from the original task that the model was trained on. . Matching Networks . The task of Matching Networks (Vinyals et al., 2016) is to learn a classifier $c_S$ for any given (small) support set $S={x_i, y_i}_{i=1}^k$ (k-shot classification). This classifier defines a probability distribution over output labels $y$ given a test example $ mathbf{x}$. Similar to other metric-based models, the classifier output is defined as a sum of labels of support samples weighted by attention kernel $a( mathbf{x}, mathbf{x}_i)$ - which should be proportional to the similarity between $ mathbf{x}$ and $ mathbf{x}_i$. . . Fig. 3. The architecture of Matching Networks. (Image source: original paper) . c_S(x)=P(y∣x,S)=∑_i=1ka(x,x_i)y_i, where S=(x_i,y_i)_i=1kc _S( mathbf{x}) = P(y vert mathbf{x}, S) = sum _{i=1}^k a( mathbf{x}, mathbf{x} _i) y _i text{, where }S={( mathbf{x} _i, y _i)} _{i=1}^kc_S(x)=P(y∣x,S)=∑_i=1ka(x,x_i)y_i, where S=(x_i,y_i)_i=1k . The attention kernel depends on two embedding functions, $f$ and $g$, for encoding the test sample and the support set samples respectively. The attention weight between two data points is the cosine similarity, $ text{cosine}(.)$, between their embedding vectors, normalized by softmax: . a(x,x_i)=exp⁡(cosine(f(x),g(x_i))∑_j=1kexp⁡(cosine(f(x),g(x_j))a( mathbf{x}, mathbf{x} _i) = frac{ exp( text{cosine}(f( mathbf{x}), g( mathbf{x} _i))}{ sum _{j=1}^k exp( text{cosine}(f( mathbf{x}), g( mathbf{x} _j))}a(x,x_i)=∑_j=1kexp(cosine(f(x),g(x_j))exp(cosine(f(x),g(x_i))​ . Simple Embedding . In the simple version, an embedding function is a neural network with a single data sample as input. Potentially we can set $f=g$. . Full Context Embeddings . The embedding vectors are critical inputs for building a good classifier. Taking a single data point as input might not be enough to efficiently gauge the entire feature space. Therefore, the Matching Network model further proposed to enhance the embedding functions by taking as input the whole support set $S$ in addition to the original input, so that the learned embedding can be adjusted based on the relationship with other support samples. . $g_ theta( mathbf{x}_i, S)$ uses a bidirectional LSTM to encode $ mathbf{x}_i$ in the context of the entire support set $S$. . | $f_ theta( mathbf{x}, S)$ encodes the test sample $ mathbf{x}$ visa an LSTM with read attention over the support set $S$. . First the test sample goes through a simple neural network, such as a CNN, to extract basic features, $f’( mathbf{x})$. | Then an LSTM is trained with a read attention vector over the support set as part of the hidden state: | begin{aligned} hat{ mathbf{h}} _t, mathbf{c} _t &amp;= text{LSTM}(f&amp;#x27;( mathbf{x}), [ mathbf{h} _{t-1}, mathbf{r} _{t-1} ], mathbf{c} _{t-1}) mathbf{h} _t &amp;= hat{ mathbf{h}} _t + f&amp;#x27;( mathbf{x}) mathbf{r} _{t-1} &amp;= sum _{i=1}^k a( mathbf{h} _{t-1}, g( mathbf{x} _i)) g( mathbf{x} _i) a( mathbf{h} _{t-1}, g( mathbf{x} _i)) &amp;= text{softmax}( mathbf{h} _{t-1}^ top g( mathbf{x} _i)) = frac{ exp( mathbf{h} _{t-1}^ top g( mathbf{x} _i))}{ sum _{j=1}^k exp( mathbf{h} _{t-1}^ top g( mathbf{x} _j))} end{aligned} Eventually $f( mathbf{x}, S)= mathbf{h}_K$ if we do K steps of “read”. | | . This embedding method is called “Full Contextual Embeddings (FCE)”. Interestingly it does help improve the performance on a hard task (few-shot classification on mini ImageNet), but makes no difference on a simple task (Omniglot). . The training process in Matching Networks is designed to match inference at test time, see the details in the earlier section. It is worthy of mentioning that the Matching Networks paper refined the idea that training and testing conditions should match. . theta^* = arg max_ theta mathbb{E}_{L subset mathcal{L}}[ mathbb{E}_{S^L subset mathcal{D}, B^L subset mathcal{D}} [ sum_{( mathbf{x}, y) in B^L} P_ theta(y vert mathbf{x}, S^L)]] . Relation Network . . Relation Network (RN) (Sung et al., 2018) is similar to siamese network but with a few differences: . The relationship is not captured by a simple L1 distance in the feature space, but predicted by a CNN classifier $g_ phi$. The relation score between a pair of inputs, $ mathbf{x}_i$ and $ mathbf{x}_j$, is $r_{ij} = g_ phi([ mathbf{x}_i, mathbf{x}_j])$ where $[.,.]$ is concatenation. | The objective function is MSE loss instead of cross-entropy, because conceptually RN focuses more on predicting relation scores which is more like regression, rather than binary classification, $ mathcal{L}(B) = sum_{( mathbf{x}_i, mathbf{x}_j, y_i, y_j) in B} (r_{ij} - mathbf{1}_{y_i=y_j})^2$. | . Fig. 4. Relation Network architecture for a 5-way 1-shot problem with one query example. (Image source: original paper) . (Note: There is another Relation Network for relational reasoning, proposed by DeepMind. Don’t get confused.) . Prototypical Networks . Prototypical Networks (Snell, Swersky &amp; Zemel, 2017) use an embedding function $f_ theta$ to encode each input into a $M$-dimensional feature vector. A prototype feature vector is defined for every class $c in mathcal{C}$, as the mean vector of the embedded support data samples in this class. . v_c=1∣S_c∣∑_(x_i,y_i)∈S_cf_θ(x_i) mathbf{v} _c = frac{1}{|S _c|} sum _{( mathbf{x} _i, y _i) in S _c} f _ theta( mathbf{x} _i)v_c=∣S_c∣1​∑_(x_i,y_i)∈S_cf_θ(x_i) . . Fig. 5. Prototypical networks in the few-shot and zero-shot scenarios. (Image source: original paper) . The distribution over classes for a given test input $ mathbf{x}$ is a softmax over the inverse of distances between the test data embedding and prototype vectors. . P(y=c∣x)=softmax(−d_φ(f_θ(x),v_c))=exp⁡(−d_φ(f_θ(x),v_c))∑_c′∈Cexp⁡(−d_φ(f_θ(x),v_c′))P(y=c vert mathbf{x})= text{softmax}(-d _ varphi(f _ theta( mathbf{x}), mathbf{v} _c)) = frac{ exp(-d _ varphi(f _ theta( mathbf{x}), mathbf{v} _c))}{ sum _{c&amp;#x27; in mathcal{C}} exp(-d _ varphi(f _ theta( mathbf{x}), mathbf{v} _{c&amp;#x27;}))}P(y=c∣x)=softmax(−d_φ(f_θ(x),v_c))=∑_c′∈Cexp(−d_φ(f_θ(x),v_c′))exp(−d_φ(f_θ(x),v_c))​ . where $d_ varphi$ can be any distance function as long as $ varphi$ is differentiable. In the paper, they used the squared euclidean distance. . The loss function is the negative log-likelihood: $ mathcal{L}( theta) = - log P_ theta(y=c vert mathbf{x})$. . Model-Based . Model-based meta-learning models make no assumption on the form of $P_ theta(y vert mathbf{x})$. Rather it depends on a model designed specifically for fast learning — a model that updates its parameters rapidly with a few training steps. This rapid parameter update can be achieved by its internal architecture or controlled by another meta-learner model. . Memory-Augmented Neural Networks . A family of model architectures use external memory storage to facilitate the learning process of neural networks, including Neural Turing Machines and Memory Networks. With an explicit storage buffer, it is easier for the network to rapidly incorporate new information and not to forget in the future. Such a model is known as MANN, short for “Memory-Augmented Neural Network”. Note that recurrent neural networks with only internal memory such as vanilla RNN or LSTM are not MANNs. . Because MANN is expected to encode new information fast and thus to adapt to new tasks after only a few samples, it fits well for meta-learning. Taking the Neural Turing Machine (NTM) as the base model, Santoro et al. (2016) proposed a set of modifications on the training setup and the memory retrieval mechanisms (or “addressing mechanisms”, deciding how to assign attention weights to memory vectors). Please go through the NTM section in my other post first if you are not familiar with this matter before reading forward. . As a quick recap, NTM couples a controller neural network with external memory storage. The controller learns to read and write memory rows by soft attention, while the memory serves as a knowledge repository. The attention weights are generated by its addressing mechanism: content-based + location based. . . Fig. 6. The architecture of Neural Turing Machine (NTM). The memory at time t, $ mathbf{M}_t$ is a matrix of size $N times M$, containing N vector rows and each has M dimensions. . MANN for Meta-Learning . To use MANN for meta-learning tasks, we need to train it in a way that the memory can encode and capture information of new tasks fast and, in the meantime, any stored representation is easily and stably accessible. . The training described in Santoro et al., 2016 happens in an interesting way so that the memory is forced to hold information for longer until the appropriate labels are presented later. In each training episode, the truth label $y_t$ is presented with one step offset, $( mathbf{x}_{t+1}, y_t)$: it is the true label for the input at the previous time step t, but presented as part of the input at time step t+1. . . Fig. 7. Task setup in MANN for meta-learning (Image source: original paper). . In this way, MANN is motivated to memorize the information of a new dataset, because the memory has to hold the current input until the label is present later and then retrieve the old information to make a prediction accordingly. . Next let us see how the memory is updated for efficient information retrieval and storage. . Addressing Mechanism for Meta-Learning . Aside from the training process, a new pure content-based addressing mechanism is utilized to make the model better suitable for meta-learning. . How to read from memory? The read attention is constructed purely based on the content similarity. . First, a key feature vector $ mathbf{k}_t$ is produced at the time step t by the controller as a function of the input $ mathbf{x}$. Similar to NTM, a read weighting vector $ mathbf{w}_t^r$ of N elements is computed as the cosine similarity between the key vector and every memory vector row, normalized by softmax. The read vector $ mathbf{r}_t$ is a sum of memory records weighted by such weightings: . r_i=∑_i=1Nw_tr(i)mathbfM_t(i), where w_tr(i)=softmax(k_t⋅M_t(i)∥k_t∥⋅∥M_t(i)∥) mathbf{r} _i = sum _{i=1}^N w _t^r(i) mathbf{M} _t(i) text{, where } w _t^r(i) = text{softmax}( frac{ mathbf{k} _t cdot mathbf{M} _t(i)}{ | mathbf{k} _t | cdot | mathbf{M} _t(i) |})r_i=∑_i=1Nw_tr(i)mathbfM_t(i), where w_tr(i)=softmax(∥k_t∥⋅∥M_t(i)∥k_t⋅M_t(i)​) . where $M_t$ is the memory matrix at time t and $M_t(i)$ is the i-th row in this matrix. . » How to write into memory? The addressing mechanism for writing newly received information into memory operates a lot like the cache replacement policy. The Least Recently Used Access (LRUA) writer is designed for MANN to better work in the scenario of meta-learning. A LRUA write head prefers to write new content to either the least used memory location or the most recently used memory location. . Rarely used locations: so that we can preserve frequently used information (see LFU); | The last used location: the motivation is that once a piece of information is retrieved once, it probably won’t be called again for a while (see MRU). | . There are many cache replacement algorithms and each of them could potentially replace the design here with better performance in different use cases. Furthermore, it would be a good idea to learn the memory usage pattern and addressing strategies rather than arbitrarily set it. . The preference of LRUA is carried out in a way that everything is differentiable: . The usage weight $ mathbf{w}^u_t$ at time t is a sum of current read and write vectors, in addition to the decayed last usage weight, $ gamma mathbf{w}^u_{t-1}$, where $ gamma$ is a decay factor. | The write vector is an interpolation between the previous read weight (prefer “the last used location”) and the previous least-used weight (prefer “rarely used location”). The interpolation parameter is the sigmoid of a hyperparameter $ alpha$. | The least-used weight $ mathbf{w}^{lu}$ is scaled according to usage weights $ mathbf{w}_t^u$, in which any dimension remains at 1 if smaller than the n-th smallest element in the vector and 0 otherwise. | begin{aligned} mathbf{w}_t^u &amp;= gamma mathbf{w}_{t-1}^u + mathbf{w}_t^r + mathbf{w}_t^w mathbf{w}_t^r &amp;= text{softmax}( text{cosine}( mathbf{k}_t, mathbf{M}_t(i))) mathbf{w}_t^w &amp;= sigma( alpha) mathbf{w}_{t-1}^r + (1- sigma( alpha)) mathbf{w}^{lu}_{t-1} mathbf{w}_t^{lu} &amp;= mathbf{1}_{w_t^u(i) leq m( mathbf{w}_t^u, n)} text{, where }m( mathbf{w}_t^u, n) text{ is the }n text{-th smallest element in vector } mathbf{w}_t^u text{.} end{aligned} . Finally, after the least used memory location, indicated by $ mathbf{w}_t^{lu}$, is set to zero, every memory row is updated: . mathbfM_t(i)=mathbfM_t−1(i)+w_tw(i)mathbfk_t,foralli mathbf{M} _t(i) = mathbf{M} _{t-1}(i) + w _t^w(i) mathbf{k} _t, forall imathbfM_t(i)=mathbfM_t−1(i)+w_tw(i)mathbfk_t,foralli . Meta Networks . Meta Networks (Munkhdalai &amp; Yu, 2017), short for MetaNet, is a meta-learning model with architecture and training process designed for rapid generalization across tasks. . Fast Weights# . The rapid generalization of MetaNet relies on “fast weights”. There are a handful of papers on this topic, but I haven’t read all of them in detail and I failed to find a very concrete definition, only a vague agreement on the concept. Normally weights in the neural networks are updated by stochastic gradient descent in an objective function and this process is known to be slow. One faster way to learn is to utilize one neural network to predict the parameters of another neural network and the generated weights are called fast weights. In comparison, the ordinary SGD-based weights are named slow weights. . In MetaNet, loss gradients are used as meta information to populate models that learn fast weights. Slow and fast weights are combined to make predictions in neural networks. . . Fig. 8. Combining slow and fast weights in a MLP. $ bigoplus$ is element-wise sum. (Image source: original paper). . Model Components . Disclaimer: Below you will find my annotations are different from those in the paper. imo, the paper is poorly written, but the idea is still interesting. So I’m presenting the idea in my own language. . Key components of MetaNet are: . An embedding function $f_ theta$, parameterized by $ theta$, encodes raw inputs into feature vectors. Similar to Siamese Neural Network, these embeddings are trained to be useful for telling whether two inputs are of the same class (verification task). | A base learner model $g_ phi$, parameterized by weights $ phi$, completes the actual learning task. | . If we stop here, it looks just like Relation Network. MetaNet, in addition, explicitly models the fast weights of both functions and then aggregates them back into the model (See Fig. 8). . Therefore we need additional two functions to output fast weights for $f$ and $g$ respectively. . $F_w$: a LSTM parameterized by $w$ for learning fast weights $ theta^+$ of the embedding function $f$. It takes as input gradients of $f$’s embedding loss for verification task. | $G_v$: a neural network parameterized by $v$ learning fast weights $ phi^+$ for the base learner $g$ from its loss gradients. In MetaNet, the learner’s loss gradients are viewed as the meta information of the task. | . Ok, now let’s see how meta networks are trained. The training data contains multiple pairs of datasets: a support set $S= { mathbf{x}’_i, y’_i }_{i=1}^K$ and a test set $U= { mathbf{x}_i, y_i }_{i=1}^L$. Recall that we have four networks and four sets of model parameters to learn, $( theta, phi, w, v)$. . . Fig.9. The MetaNet architecture. . Training Process# . Sample a random pair of inputs at each time step t from the support set $S$, $( mathbf{x}’_i, y’_i)$ and $( mathbf{x}’_j, y_j)$. Let $ mathbf{x}_{(t,1)}= mathbf{x}’_i$ and $ mathbf{x}_{(t,2)}= mathbf{x}’_j$. for $t = 1, dots, K$: . a. Compute a loss for representation learning; i.e., cross entropy for the verification task: $ mathcal{L}^ text{emb}_t = mathbf{1}_{y’_i=y’_j} log P_t + (1 - mathbf{1}_{y’_i=y’_j}) log(1 - P_t) text{, where }P_t = sigma( mathbf{W} vert f_ theta( mathbf{x}_{(t,1)}) - f_ theta( mathbf{x}_{(t,2)}) vert)$ . | Compute the task-level fast weights: $ theta^+ = F_w( nabla_ theta mathcal{L}^ text{emb}_1, dots, mathcal{L}^ text{emb}_T)$ . | Next go through examples in the support set $S$ and compute the example-level fast weights. Meanwhile, update the memory with learned representations. for $i=1, dots, K$: . a. The base learner outputs a probability distribution: $P( hat{y}_i vert mathbf{x}_i) = g_ phi( mathbf{x}_i)$ and the loss can be cross-entropy or MSE: $ mathcal{L}^ text{task}_i = y’_i log g_ phi( mathbf{x}’_i) + (1- y’_i) log (1 - g_ phi( mathbf{x}’_i))$ b. Extract meta information (loss gradients) of the task and compute the example-level fast weights: $ phi_i^+ = G_v( nabla_ phi mathcal{L}^ text{task}_i)$ c. Then store $ phi^+_i$ into $i$-th location of the “value” memory $ mathbf{M}$. . d. Encode the support sample into a task-specific input representation using both slow and fast weights: $r’_i = f_{ theta, theta^+}( mathbf{x}’_i)$ Then store $r’_i$ into $i$-th location of the “key” memory $ mathbf{R}$. . | Finally it is the time to construct the training loss using the test set $U= { mathbf{x}_i, y_i }_{i=1}^L$. Starts with $ mathcal{L}_ text{train}=0$: for $j=1, dots, L$: . a. Encode the test sample into a task-specific input representation: $r_j = f_{ theta, theta^+}( mathbf{x}_j)$ b. The fast weights are computed by attending to representations of support set samples in memory $ mathbf{R}$. The attention function is of your choice. Here MetaNet uses cosine similarity: . begin{aligned} a _j &amp;= text{cosine}( mathbf{R}, r _j) = [ frac{r&amp;#x27; _1 cdot r _j}{ |r&amp;#x27; _1 | cdot |r _j |}, dots, frac{r&amp;#x27; _N cdot r _j}{ |r&amp;#x27; _N | cdot |r _j |} ] phi^+ _j &amp;= text{softmax}(a _j)^ top mathbf{M} end{aligned} c. Update the training loss: $ mathcal{L}_ text{train} leftarrow mathcal{L}_ text{train} + mathcal{L}^ text{task}(g_{ phi, phi^+}( mathbf{x}_i), y_i) $ . | Update all the parameters $( theta, phi, w, v)$ using $ mathcal{L}_ text{train}$. . | Optimization-Based . Deep learning models learn through backpropagation of gradients. However, the gradient-based optimization is neither designed to cope with a small number of training samples, nor to converge within a small number of optimization steps. Is there a way to adjust the optimization algorithm so that the model can be good at learning with a few examples? This is what optimization-based approach meta-learning algorithms intend for. . LSTM Meta-Learner . The optimization algorithm can be explicitly modeled. Ravi &amp; Larochelle (2017) did so and named it “meta-learner”, while the original model for handling the task is called “learner”. The goal of the meta-learner is to efficiently update the learner’s parameters using a small support set so that the learner can adapt to the new task quickly. . Let’s denote the learner model as $M_ theta$ parameterized by $ theta$, the meta-learner as $R_ Theta$ with parameters $ Theta$, and the loss function $ mathcal{L}$. . Why LSTM? . The meta-learner is modeled as a LSTM, because: . There is similarity between the gradient-based update in backpropagation and the cell-state update in LSTM. | Knowing a history of gradients benefits the gradient update; think about how momentum works. | The update for the learner’s parameters at time step t with a learning rate $ alpha_t$ is: . theta_t=theta_t−1−alpha_tnabla_theta_t−1mathcalL_t theta _t = theta _{t-1} - alpha _t nabla _{ theta _{t-1}} mathcal{L} _ttheta_t=theta_t−1−alpha_tnabla_theta_t−1mathcalL_t . It has the same form as the cell state update in LSTM, if we set forget gate $f_t=1$, input gate $i_t = alpha_t$, cell state $c_t = theta_t$, and new cell state $ tilde{c}_t = - nabla_{ theta_{t-1}} mathcal{L}_t$: . begin{aligned} c_t &amp;= f_t odot c_{t-1} + i_t odot tilde{c}_t &amp;= theta_{t-1} - alpha_t nabla_{ theta_{t-1}} mathcal{L}_t end{aligned} . While fixing $f_t=1$ and $i_t= alpha_t$ might not be the optimal, both of them can be learnable and adaptable to different datasets. . begin{aligned} f_t &amp;= sigma( mathbf{W}_f cdot [ nabla_{ theta_{t-1}} mathcal{L}_t, mathcal{L}_t, theta_{t-1}, f_{t-1}] + mathbf{b}_f) &amp; scriptstyle{ text{; how much to forget the old value of parameters.}} i_t &amp;= sigma( mathbf{W}_i cdot [ nabla_{ theta_{t-1}} mathcal{L}_t, mathcal{L}_t, theta_{t-1}, i_{t-1}] + mathbf{b}_i) &amp; scriptstyle{ text{; corresponding to the learning rate at time step t.}} tilde{ theta}_t &amp;= - nabla_{ theta_{t-1}} mathcal{L}_t &amp; theta_t &amp;= f_t odot theta_{t-1} + i_t odot tilde{ theta}_t &amp; end{aligned} . Model Setup . . Fig. 10. How the learner $M _ theta$ and the meta-learner $R _ Theta$ are trained. (Image source: original paper) with more annotations) . The training process mimics what happens during test, since it has been proved to be beneficial in Matching Networks. During each training epoch, we first sample a dataset $ mathcal{D} = ( mathcal{D}_ text{train}, mathcal{D}_ text{test}) in hat{ mathcal{D}}_ text{meta-train}$ and then sample mini-batches out of $ mathcal{D}_ text{train}$ to update $ theta$ for $T$ rounds. The final state of the learner parameter $ theta_T$ is used to train the meta-learner on the test data $ mathcal{D}_ text{test}$. . Two implementation details to pay extra attention to: . How to compress the parameter space in LSTM meta-learner? As the meta-learner is modeling parameters of another neural network, it would have hundreds of thousands of variables to learn. Following the idea of sharing parameters across coordinates, | To simplify the training process, the meta-learner assumes that the loss $ mathcal{L}_t$ and the gradient $ nabla_{ theta_{t-1}} mathcal{L}_t$ are independent. | . MAML . MAML, short for Model-Agnostic Meta-Learning (Finn, et al. 2017) is a fairly general optimization algorithm, compatible with any model that learns through gradient descent. . Let’s say our model is $f_ theta$ with parameters $ theta$. Given a task $ tau_i$ and its associated dataset $( mathcal{D}^{(i)}_ text{train}, mathcal{D}^{(i)}_ text{test})$, we can update the model parameters by one or more gradient descent steps (the following example only contains one step): . theta′_i=theta−alphanabla_thetamathcalL(0)_tau_i(f_theta) theta&amp;#x27; _i = theta - alpha nabla _ theta mathcal{L}^{(0)} _{ tau _i}(f _ theta)theta′_i=theta−alphanabla_thetamathcalL(0)_tau_i(f_theta) . where $ mathcal{L}^{(0)}$ is the loss computed using the mini data batch with id (0). . . Fig. 11. Diagram of MAML. (Image source: original paper) . Well, the above formula only optimizes for one task. To achieve a good generalization across a variety of tasks, we would like to find the optimal $ theta^*$ so that the task-specific fine-tuning is more efficient. Now, we sample a new data batch with id (1) for updating the meta-objective. The loss, denoted as $ mathcal{L}^{(1)}$, depends on the mini batch (1). The superscripts in $ mathcal{L}^{(0)}$ and $ mathcal{L}^{(1)}$ only indicate different data batches, and they refer to the same loss objective for the same task. . begin{aligned} theta^* &amp;= arg min_ theta sum_{ tau_i sim p( tau)} mathcal{L}_{ tau_i}^{(1)} (f_{ theta&#39;_i}) = arg min_ theta sum_{ tau_i sim p( tau)} mathcal{L}_{ tau_i}^{(1)} (f_{ theta - alpha nabla_ theta mathcal{L}_{ tau_i}^{(0)}(f_ theta)}) &amp; theta &amp; leftarrow theta - beta nabla_{ theta} sum_{ tau_i sim p( tau)} mathcal{L}_{ tau_i}^{(1)} (f_{ theta - alpha nabla_ theta mathcal{L}_{ tau_i}^{(0)}(f_ theta)}) &amp; scriptstyle{ text{; updating rule}} end{aligned} . . Fig. 12. The general form of MAML algorithm. (Image source: original paper) . First-Order MAML# . The meta-optimization step above relies on second derivatives. To make the computation less expensive, a modified version of MAML omits second derivatives, resulting in a simplified and cheaper implementation, known as First-Order MAML (FOMAML). . Let’s consider the case of performing $k$ inner gradient steps, $k geq1$. Starting with the initial model parameter $ theta_ text{meta}$: . begin{aligned} theta_0 &amp;= theta_ text{meta} theta_1 &amp;= theta_0 - alpha nabla_ theta mathcal{L}^{(0)}( theta_0) theta_2 &amp;= theta_1 - alpha nabla_ theta mathcal{L}^{(0)}( theta_1) &amp; dots theta_k &amp;= theta_{k-1} - alpha nabla_ theta mathcal{L}^{(0)}( theta_{k-1}) end{aligned} . Then in the outer loop, we sample a new data batch for updating the meta-objective. . begin{aligned} theta_ text{meta} &amp; leftarrow theta_ text{meta} - beta g_ text{MAML} &amp; scriptstyle{ text{; update for meta-objective}} [2mm] text{where } g_ text{MAML} &amp;= nabla_{ theta} mathcal{L}^{(1)}( theta_k) &amp; [2mm] &amp;= nabla_{ theta_k} mathcal{L}^{(1)}( theta_k) cdot ( nabla_{ theta_{k-1}} theta_k) dots ( nabla_{ theta_0} theta_1) cdot ( nabla_{ theta} theta_0) &amp; scriptstyle{ text{; following the chain rule}} &amp;= nabla_{ theta_k} mathcal{L}^{(1)}( theta_k) cdot Big( prod_{i=1}^k nabla_{ theta_{i-1}} theta_i Big) cdot I &amp; &amp;= nabla_{ theta_k} mathcal{L}^{(1)}( theta_k) cdot prod_{i=1}^k nabla_{ theta_{i-1}} ( theta_{i-1} - alpha nabla_ theta mathcal{L}^{(0)}( theta_{i-1})) &amp; &amp;= nabla_{ theta_k} mathcal{L}^{(1)}( theta_k) cdot prod_{i=1}^k (I - alpha nabla_{ theta_{i-1}}( nabla_ theta mathcal{L}^{(0)}( theta_{i-1}))) &amp; end{aligned} . The MAML gradient is: . g_textMAML=nabla_theta_kmathcalL(1)(theta_k)cdotprod_i=1k(I−alphacolorrednabla_theta_i−1(nabla_thetamathcalL(0)(theta_i−1)))g _ text{MAML} = nabla _{ theta _k} mathcal{L}^{(1)}( theta _k) cdot prod _{i=1}^k (I - alpha color{red}{ nabla _{ theta _{i-1}}( nabla _ theta mathcal{L}^{(0)}( theta _{i-1}))})g_textMAML=nabla_theta_kmathcalL(1)(theta_k)cdotprod_i=1k(I−alphacolorrednabla_theta_i−1(nabla_thetamathcalL(0)(theta_i−1))) . The First-Order MAML ignores the second derivative part in red. It is simplified as follows, equivalent to the derivative of the last inner gradient update result. . g_textFOMAML=nabla_theta_kmathcalL(1)(theta_k)g _ text{FOMAML} = nabla _{ theta _k} mathcal{L}^{(1)}( theta _k)g_textFOMAML=nabla_theta_kmathcalL(1)(theta_k) . Reptile . Reptile (Nichol, Achiam &amp; Schulman, 2018) is a remarkably simple meta-learning optimization algorithm. It is similar to MAML in many ways, given that both rely on meta-optimization through gradient descent and both are model-agnostic. . The Reptile works by repeatedly: . sampling a task, | training on it by multiple gradient descent steps, | and then moving the model weights towards the new parameters. | See the algorithm below: $ text{SGD}( mathcal{L}_{ tau_i}, theta, k)$ performs stochastic gradient update for k steps on the loss $ mathcal{L}_{ tau_i}$ starting with initial parameter $ theta$ and returns the final parameter vector. The batch version samples multiple tasks instead of one within each iteration. The reptile gradient is defined as $( theta - W)/ alpha$, where $ alpha$ is the stepsize used by the SGD operation. . . Fig. 13. The batched version of Reptile algorithm. (Image source: original paper) . At a glance, the algorithm looks a lot like an ordinary SGD. However, because the task-specific optimization can take more than one step. it eventually makes text{SGD}( mathbb{E} _ tau [ mathcal{L}_{ tau} ], theta, k)$ diverge from $ mathbb{E}_ tau [ text{SGD}( mathcal{L}_{ tau}, theta, k) ] when k &gt; 1. . The Optimization Assumption . Assuming that a task $ tau sim p( tau)$ has a manifold of optimal network configuration, $ mathcal{W}_{ tau}^*$. The model $f_ theta$ achieves the best performance for task $ tau$ when $ theta$ lays on the surface of $ mathcal{W}_{ tau}^*$. To find a solution that is good across tasks, we would like to find a parameter close to all the optimal manifolds of all tasks: . theta^* = arg min_ theta mathbb{E}_{ tau sim p( tau)} [ frac{1}{2} text{dist}( theta, mathcal{W}_ tau^*)^2] . . Fig. 14. The Reptile algorithm updates the parameter alternatively to be closer to the optimal manifolds of different tasks. (Image source: original paper) . Let’s use the L2 distance as $ text{dist}(.)$ and the distance between a point $ theta$ and a set $ mathcal{W}_ tau^*$ equals to the distance between $ theta$ and a point $W_{ tau}^*( theta)$ on the manifold that is closest to $ theta$: . textdist(theta,mathcalW_tau *)=textdist(theta,W_tau *(theta))text,whereW_tau *(theta)=argmin_WinmathcalW_tau *textdist(theta,W) text{dist}( theta, mathcal{W} _{ tau}^ *) = text{dist}( theta, W _{ tau}^ *( theta)) text{, where }W _{ tau}^ *( theta) = arg min _{W in mathcal{W} _{ tau}^ *} text{dist}( theta, W)textdist(theta,mathcalW_tau*)=textdist(theta,W_tau*(theta))text,whereW_tau*(theta)=argmin_WinmathcalW_tau*textdist(theta,W) . The gradient of the squared euclidean distance is: . begin{aligned} nabla_ theta[ frac{1}{2} text{dist}( theta, mathcal{W}_{ tau_i}^*)^2] &amp;= nabla_ theta[ frac{1}{2} text{dist}( theta, W_{ tau_i}^*( theta))^2] &amp; &amp;= nabla_ theta[ frac{1}{2}( theta - W_{ tau_i}^*( theta))^2] &amp; &amp;= theta - W_{ tau_i}^*( theta) &amp; scriptstyle{ text{; See notes.}} end{aligned} . Notes: According to the Reptile paper, “the gradient of the squared euclidean distance between a point Θ and a set S is the vector 2(Θ − p), where p is the closest point in S to Θ”. Technically the closest point in S is also a function of Θ, but I’m not sure why the gradient does not need to worry about the derivative of p. (Please feel free to leave me a comment or send me an email about this if you have ideas.) . Thus the update rule for one stochastic gradient step is: . theta = theta - alpha nabla_ theta[ frac{1}{2} text{dist}( theta, mathcal{W}_{ tau_i}^*)^2] = theta - alpha( theta - W_{ tau_i}^*( theta)) = (1- alpha) theta + alpha W_{ tau_i}^*( theta) . The closest point on the optimal task manifold $W_{ tau_i}^*( theta)$ cannot be computed exactly, but Reptile approximates it using $ text{SGD}( mathcal{L}_ tau, theta, k)$. . Reptile vs FOMAML . To demonstrate the deeper connection between Reptile and MAML, let’s expand the update formula with an example performing two gradient steps, k=2 in $ text{SGD}(.)$. Same as defined above, $ mathcal{L}^{(0)}$ and $ mathcal{L}^{(1)}$ are losses using different mini-batches of data. For ease of reading, we adopt two simplified annotations: $g^{(i)}_j = nabla_{ theta} mathcal{L}^{(i)}( theta_j)$ and $H^{(i)}_j = nabla^2_{ theta} mathcal{L}^{(i)}( theta_j)$. . begin{aligned} theta_0 &amp;= theta_ text{meta} theta_1 &amp;= theta_0 - alpha nabla_ theta mathcal{L}^{(0)}( theta_0)= theta_0 - alpha g^{(0)}_0 theta_2 &amp;= theta_1 - alpha nabla_ theta mathcal{L}^{(1)}( theta_1) = theta_0 - alpha g^{(0)}_0 - alpha g^{(1)}_1 end{aligned} . According to the early section, the gradient of FOMAML is the last inner gradient update result. Therefore, when k=1: . begin{aligned} g_ text{FOMAML} &amp;= nabla_{ theta_1} mathcal{L}^{(1)}( theta_1) = g^{(1)}_1 g_ text{MAML} &amp;= nabla_{ theta_1} mathcal{L}^{(1)}( theta_1) cdot (I - alpha nabla^2_{ theta} mathcal{L}^{(0)}( theta_0)) = g^{(1)}_1 - alpha H^{(0)}_0 g^{(1)}_1 end{aligned} . The Reptile gradient is defined as: . g_textReptile=(theta_0−theta_2)/alpha=g(0)_0+g(1)_1g _ text{Reptile} = ( theta _0 - theta _2) / alpha = g^{(0)} _0 + g^{(1)} _1g_textReptile=(theta_0−theta_2)/alpha=g(0)_0+g(1)_1 . Up to now we have: . . Fig. 15. Reptile versus FOMAML in one loop of meta-optimization. (Image source: slides on Reptile by Yoonho Lee.) . begin{aligned} g_ text{FOMAML} &amp;= g^{(1)}_1 g_ text{MAML} &amp;= g^{(1)}_1 - alpha H^{(0)}_0 g^{(1)}_1 g_ text{Reptile} &amp;= g^{(0)}_0 + g^{(1)}_1 end{aligned} . Next let’s try further expand $g^{(1)}_1$ using Taylor expansion. Recall that Taylor expansion of a function $f(x)$ that is differentiable at a number $a$ is: . f(x) = f(a) + frac{f&#39;(a)}{1!}(x-a) + frac{f&#39;&#39;(a)}{2!}(x-a)^2 + dots = sum_{i=0}^ infty frac{f^{(i)}(a)}{i!}(x-a)^i . We can consider $ nabla_{ theta} mathcal{L}^{(1)}(.)$ as a function and $ theta_0$ as a value point. The Taylor expansion of $g_1^{(1)}$ at the value point $ theta_0$ is: . begin{aligned} g_1^{(1)} &amp;= nabla_{ theta} mathcal{L}^{(1)}( theta_1) &amp;= nabla_{ theta} mathcal{L}^{(1)}( theta_0) + nabla^2_ theta mathcal{L}^{(1)}( theta_0)( theta_1 - theta_0) + frac{1}{2} nabla^3_ theta mathcal{L}^{(1)}( theta_0)( theta_1 - theta_0)^2 + dots &amp; &amp;= g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} + frac{ alpha^2}{2} nabla^3_ theta mathcal{L}^{(1)}( theta_0) (g_0^{(0)})^2 + dots &amp; scriptstyle{ text{; because } theta_1- theta_0=- alpha g_0^{(0)}} &amp;= g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} + O( alpha^2) end{aligned} . Plug in the expanded form of $g_1^{(1)}$ into the MAML gradients with one step inner gradient update: . begin{aligned} g_ text{FOMAML} &amp;= g^{(1)}_1 = g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} + O( alpha^2) g_ text{MAML} &amp;= g^{(1)}_1 - alpha H^{(0)}_0 g^{(1)}_1 &amp;= g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} + O( alpha^2) - alpha H^{(0)}_0 (g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} + O( alpha^2)) &amp;= g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} - alpha H^{(0)}_0 g_0^{(1)} + alpha^2 alpha H^{(0)}_0 H^{(1)}_0 g_0^{(0)} + O( alpha^2) &amp;= g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} - alpha H^{(0)}_0 g_0^{(1)} + O( alpha^2) end{aligned} . The Reptile gradient becomes: . begin{aligned} g_ text{Reptile} &amp;= g^{(0)}_0 + g^{(1)}_1 &amp;= g^{(0)}_0 + g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} + O( alpha^2) end{aligned} . So far we have the formula of three types of gradients: . begin{aligned} g_ text{FOMAML} &amp;= g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} + O( alpha^2) g_ text{MAML} &amp;= g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} - alpha H^{(0)}_0 g_0^{(1)} + O( alpha^2) g_ text{Reptile} &amp;= g^{(0)}_0 + g_0^{(1)} - alpha H^{(1)}_0 g_0^{(0)} + O( alpha^2) end{aligned} . During training, we often average over multiple data batches. In our example, the mini batches (0) and (1) are interchangeable since both are drawn at random. The expectation $ mathbb{E}_{ tau,0,1}$ is averaged over two data batches, ids (0) and (1), for task $ tau$. . Let, . $A = mathbb{E}_{ tau,0,1} [g_0^{(0)}] = mathbb{E}_{ tau,0,1} [g_0^{(1)}]$; it is the average gradient of task loss. We expect to improve the model parameter to achieve better task performance by following this direction pointed by $A$. | $B = mathbb{E}_{ tau,0,1} [H^{(1)}_0 g_0^{(0)}] = frac{1}{2} mathbb{E}_{ tau,0,1} [H^{(1)}_0 g_0^{(0)} + H^{(0)}_0 g_0^{(1)}] = frac{1}{2} mathbb{E}_{ tau,0,1} [ nabla_ theta(g^{(0)}_0 g_0^{(1)})]$; it is the direction (gradient) that increases the inner product of gradients of two different mini batches for the same task. We expect to improve the model parameter to achieve better generalization over different data by following this direction pointed by $B$. | . To conclude, both MAML and Reptile aim to optimize for the same goal, better task performance (guided by A) and better generalization (guided by B), when the gradient update is approximated by first three leading terms. . begin{aligned} mathbb{E}_{ tau,1,2}[g_ text{FOMAML}] &amp;= A - alpha B + O( alpha^2) mathbb{E}_{ tau,1,2}[g_ text{MAML}] &amp;= A - 2 alpha B + O( alpha^2) mathbb{E}_{ tau,1,2}[g_ text{Reptile}] &amp;= 2A - alpha B + O( alpha^2) end{aligned} . It is not clear to me whether the ignored term $O( alpha^2)$ might play a big impact on the parameter learning. But given that FOMAML is able to obtain a similar performance as the full version of MAML, it might be safe to say higher-level derivatives would not be critical during gradient descent update. .",
            "url": "https://tayalmanan28.github.io/my_blogs/robotics/2022/05/02/Meta-Learning.html",
            "relUrl": "/robotics/2022/05/02/Meta-Learning.html",
            "date": " • May 2, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Offline Reinforcement Learning",
            "content": "Offline Reinforcement Learning . Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. . . This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function. Then, we extract the policy via advantage-weighted behavioral cloning. We dub our method implicit Q-learning (IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization. .",
            "url": "https://tayalmanan28.github.io/my_blogs/reinforcement-learning/robotics/2022/04/01/Offline-Reinforcement-Learning.html",
            "relUrl": "/reinforcement-learning/robotics/2022/04/01/Offline-Reinforcement-Learning.html",
            "date": " • Apr 1, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Getting Started with MuJoCo",
            "content": "MuJoCo . MuJoCo stands for Multi-Joint dynamics with Contact. It is a general purpose physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, machine learning, and other areas that demand fast and accurate simulation of articulated structures interacting with their environment. Initially developed by Roboti LLC, it was acquired and made freely available by DeepMind in October 2021, with the goal of making MuJoCo an open-source project. . MuJoCo is a physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, and other areas where fast and accurate simulation is needed. . MuJoCo offers a unique combination of speed, accuracy and modeling power, yet it is not merely a better simulator. Instead it is the first full-featured simulator designed from the ground up for the purpose of model-based optimization, and in particular optimization through contacts. . MuJoCo makes it possible to scale up computationally-intensive techniques such optimal control, physically-consistent state estimation, system identification and automated mechanism design, and apply them to complex dynamical systems in contact-rich behaviors. It also has more traditional applications such as testing and validation of control schemes before deployment on physical robots, interactive scientific visualization, virtual environments, animation and gaming. . The Documentation for MuJoCo is available here . Installation . Let us now follow the following steps to setup MuJoCo on our Linux System . Install anaconda Download . sudo chmod +x Anaconda3-2021.11-Linux-x86_64.sh ./Anaconda3-2021.11-Linux-x86_64.sh . | install git sudo apt install git . | install the mujoco library . Download the Mujoco library from here | create a hidden folder : mkdir /home/username/.mujoco . | extract the library to the .mujoco folder | include these lines in .bashrc file: export LD_LIBRARY_PATH=/home/tayal/.mujoco/mujoco210/bin export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia export PATH=&quot;$LD_LIBRARY_PATH:$PATH&quot; export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so . | source .bashrc . | Test that the library is installed by going into: cd ~/.mujoco/mujoco210/bin ./simulate ../model/humanoid.xml . | . | Install mujoco-py: conda create --name mujoco_py python=3.8 conda activate mujoco_py sudo apt update sudo apt-get install patchelf sudo apt-get install python3-dev build-essential libssl-dev libffi-dev libxml2-dev sudo apt-get install libxslt1-dev zlib1g-dev libglew1.5 libglew-dev python3-pip git clone https://github.com/openai/mujoco-py cd mujoco-py pip install -r requirements.txt pip install -r requirements.dev.txt pip3 install -e . --no-cache . | reboot your machine . | run these commands conda activate mujoco_py sudo apt install libosmesa6-dev libgl1-mesa-glx libglfw3 sudo ln -s /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/x86_64-linux-gnu/libGL.so pip3 install -U &#39;mujoco-py&lt;2.2,&gt;=2.1&#39; cd examples python3 setting_state.py . |",
            "url": "https://tayalmanan28.github.io/my_blogs/mujoco/simulations/robotics/2022/01/21/MuJoCo.html",
            "relUrl": "/mujoco/simulations/robotics/2022/01/21/MuJoCo.html",
            "date": " • Jan 21, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Optimization for Machine Learning",
            "content": "Motor . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/optimization/machine-learning/robotics/2021/12/18/Optimization-for-ML.html",
            "relUrl": "/optimization/machine-learning/robotics/2021/12/18/Optimization-for-ML.html",
            "date": " • Dec 18, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Probability and Stochastics",
            "content": "Motor . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/probability/stochastics/robotics/2021/10/03/probability-and-stochastics.html",
            "relUrl": "/probability/stochastics/robotics/2021/10/03/probability-and-stochastics.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "How to Contribute to Open Source",
            "content": "Welcome newbie Open Source contributors! . This is a list of resources for people who are new to contributing to Open Source. . If you find additional resources, please contribute a pull request. . If you have questions or comments, please create an issue. . Contents . Contributing to Open Source in general | Direct GitHub searches | Mozilla’s contributor ecosystem | Useful articles for new Open Source contributors | Using Version Control | Open Source books | Open Source contribution initiatives | Open Source programs to participate in | License | . Contributing to Open Source in general . Articles and resources that discuss the world and culture of open source. . The Definitive Guide to Contributing to Open Source by @DoomHammerNG | An Intro to Open Source - Tutorials by DigitalOcean to guide you on your way to contribution success here on GitHub. | Code Triage - another, really nice, tool for finding popular repositories and issues filtered by language. | Forge Your Future with Open Source ($) - Book devoted to explaining open source, how to find a project, and how to start contributing. Inclusive of all roles in software development, not just programmers. | Awesome-for-beginners - a GitHub repo that amasses projects with good bugs for new contributors, and applies labels to describe them. | Open Source Guides - Collection of resources for individuals, communities, and companies who want to learn how to run and contribute to an Open Source project. | 45 Github Issues Dos and Don’ts - Do’s and Don’ts on GitHub. | GitHub Guides - basic guides on how to use GitHub effectively. | Contribute to Open Source - Learn the GitHub workflow by contributing code to a simulation project. | Linux Foundation’s Open Source Guides for the Enterprise - The Linux Foundation’s guides to Open Source projects. | CSS Tricks An Open Source Etiquette Guidebook - An Open Source Etiquette Guidebook, written by Kent C. Dodds And Sarah Drasner. | A to Z Resources for Students - Curated list of resources and opportunities for college students to learn a new coding language. | Pull Request Roulette - This site has a list of pull requests submitted for review belonging to Open Source projects hosted on Github. | “How to Contribute to an Open Source Project on GitHub” by Egghead.io - A step-by-step video guide of how to start contributing to Open Source projects on GitHub. | Contributing to Open Source: A Live Walkthrough from Beginning to End - This walkthrough of an open source contribution covers everything from picking a suitable project, working on an issue, to getting the PR merged in. | “How to Contribute to Open Source Project” by Sarah Drasner - They are focusing on the nitty-gritty of contributing a pull request (PR) to someone else’s project on GitHub. | “How to get started with Open Source” by Sayan Chowdhury - This article covers the resources for contributing to open source for beginners based on their favorite language interest. | “Browse good first issues to start contributing to open source” - GitHub now helps you find good first issues to start contributing to open source. | “How to Contribute to Open Source Project” by Maryna Z - This comprehensive article is directed towards businesses (but still useful for individual contributors) where it talks about why, how, and what open-source projects to contribute to. | “start-here-guidelines” by Andrei - Lets Git started in the world of opensource, starting in the opensource playground. Especially designed for education and practical experience purposes. | “Getting Started with Open Source” by NumFocus - a GitHub repo that helps contributors overcome barriers to entry in open-source. | “Opensoure-4-everyone” by Chryz-hub - A repository on everything related to open source. This is a project to help with GitHub membership visibility, practice with basic and advance git commands, getting started with open source, and more. | “Open Advice” - Knowledge collection from a wide variety of Free Software projects. It answers the question what 42 prominent contributors would have liked to know when they started so you can get a head-start no matter how and where you contribute. | “GitHub Skills” - Level up your skills with GitHub Skills. Our friendly bot will take you through a series of fun, practical projects to learn the skills you need in no time—and share helpful feedback along the way. | “Ten simple rules for helping newcomers become contributors to open projects” - This article covers rules based on studies of many communities and experiences of members, leaders, and observers. | “Step-by-Step guide to contributing on GitHub” - a step-by-step guide with supporting visuals and links regarding the whole process of contributing to an open source project. | Open Source with Pradumna - This repo contains resources and materials to learn and get yourself started with Open Source, Git, and GitHub. | “FOSS Community Acronyms” - This repo contains a list of abbreviations used within the FOSS (Free and Open Source) community, along with their definitions and usages. | “Open Source Fiesta - Open Source Fiesta” - Step-by-step instruction on how to contribute to GitHub repositories, and includes a git command line cheatsheet. | . Direct GitHub searches . Search links that point directly to suitable issues to contribute to on GitHub. . is:issue is:open label:beginner | is:issue is:open label:easy | is:issue is:open label:first-timers-only | is:issue is:open label:good-first-bug | is:issue is:open label:”good first issue” | is:issue is:open label:starter | is:issue is:open label:up-for-grabs | is:issue is:open label:easy-fix | is:issue is:open label:”beginner friendly” | is:issue is:open label:your-first-pr | . Mozilla’s contributor ecosystem . Mozilla pledges for a healthy internet and with it, has opportunities to contribute to its open-source projects. . Good First Bugs - bugs that developers have identified as a good introduction to the project. | MDN Web Docs - help the MDN Web Docs team in documenting the web platform by fixing content issues and platform bugs. | Mentored Bugs - bugs that have a mentor assigned who will be there on IRC to help you when you get stuck while working on a fix. | Bugs Ahoy - a site dedicated to finding bugs on Bugzilla. | Firefox DevTools - a site dedicated to bugs filed for the developer tools in the Firefox browser. | What Can I Do For Mozilla - figure out what you can work on by answering a bunch of questions about your skill set and interests. | Start Mozilla - a Twitter account that tweets about issues fit for contributors new to the Mozilla ecosystem. | . Useful articles for new Open Source contributors . Helpful articles and blogs directed at new contributors on how to get started. . Finding ways to contribute to open source on GitHub by @GitHub | How to choose (and contribute to) your first Open Source project by @GitHub | How to find your first Open Source bug to fix by @Shubheksha | First Timers Only by @kentcdodds | Bring Kindness Back to Open Source by @shanselman | Getting into Open Source for the First Time by @mcdonnelldean | How to Contribute to Open Source by @GitHub | How to Find a Bug in Your Code by @dougbradbury | Mastering Markdown by @GitHub | First mission: Contributors page by @forCrowd | How to make your first Open Source contribution in just 5 minutes by @roshanjossey | I just got my free Hacktoberfest shirt. Here’s a quick way you can get yours. by @quincylarson | A Bitter Guide To Open Source by @ken_wheeler | A junior developer’s step-by-step guide to contributing to Open Source for the first time by @LetaKeane | Learn Git and GitHub Step By Step (on Windows) by @ows-ali | Why Open Source and How? by @james-gallagher | How to get started with Open Source - By Sayan Chowdhury | What open-source should I contribute to by @kentcdodds | An immersive introductory guide to Open-source by Franklin Okolie | Getting started with contributing to open source by Zara Cooper | Beginner’s guide to open-source contribution by Sudipto Ghosh | 8 non-code ways to contribute to open source by OpenSource | What is Open Source Software? OSS Explained in Plain English by Jessica Wilkins | How to Start an Open Source Project on GitHub – Tips from Building My Trending Repo by @Rishit-dagli | . Using Version Control . Tutorials and resources of varying levels on using version control, typically Git and GitHub. . Video tutorial for Git and Github by Harvard University - Tutorial by Harvard University, part of their CS50 Web Development course on understanding Git and GitHub and working with Git commands. | Think Like (a) Git - Git introduction for “advanced beginners”, but are still struggling, in order to give you a simple strategy to safely experiment with git. | Quickstart - Set up Git - Learn how to set up Git locally and set up authentication, along with next steps on your learning journey. | Everyday Git - A useful minimum set of commands for Everyday Git. | Oh shit, git! - how to get out of common git mistakes described in plain English; also see Dangit, git! for the page without swears. | Atlassian Git Tutorials - various tutorials on using git. | GitHub Git Cheat Sheet (PDF) | freeCodeCamp’s Wiki on Git Resources | GitHub Flow (42:06) - GitHub talk on how to make a pull request. | Quickstart - GitHub Learning Resources - Git and GitHub learning resources. | Pro Git - The entire Pro Git book, written by Scott Chacon and Ben Straub and published by Apress. | Git-it - Step by step Git tutorial desktop app. | Flight Rules for Git - A guide about what to do when things go wrong. | Git Guide for Beginners in Spanish - A complete guide of slides about git and GitHub explained in Spanish. Una guía completa de diapositivas sobre git y GitHub explicadas en Español. | Git Kraken - Visual, cross-platform, and interactive git desktop application for version control. | Git Tips - Collection of most commonly used git tips and tricks. | Git Best Practices - Commit Often, Perfect Later, Publish Once: Git Best Practices. | Git Interactive Tutorial - Learn Git in the most visual and interactive way. | Git Cheat Sheets - A set of graphical cheat sheets on git. | Complete Git and GitHub Tutorial (1:12:39) - Full Git and GitHub walkthrough by Kunal Kushwaha. | A Tutorial Introduction to Git - A Tutorial for Beginners by Git. | First Aid Git - A searchable collection of the most frequently asked Git questions. Answers for these questions were collected from personal experience, Stackoverflow, and the official Git documentation. | Git by Susan Potter - Show how various technical aspects of Git work under the covers to enable distributed workflows, and how it differs from other version control systems (VCSs). | . Open Source books . Books on all things open source: the culture, history, best practices, etc. . Producing Open Source Software - Producing Open Source Software is a book about the human side of Open Source development. It describes how successful projects operate, the expectations of users and developers, and the culture of free software. | The Architecture of Open Source Applications - The authors of twenty-four open source applications explain how their software is structured, and why. From web servers and compilers to health record management systems, they are covered here to help you become a better developer. | Open Source Book Series - Learn more about Open Source and the growing Open Source movement with a comprehensive list of free eBooks from https://opensource.com. | Software Release Practice HOWTO - This HOWTO describes good release practices for Linux and other Open-Source projects. By following these practices, you will make it as easy as possible for users to build your code and use it, and for other developers to understand your code and cooperate with you to improve it. | Open Sources 2.0 : The Continuing Evolution (2005) - Open Sources 2.0 is a collection of insightful and thought-provoking essays from today’s technology leaders that continues painting the evolutionary picture that developed in the 1999 book, Open Sources: Voices from the Revolution. | Open Sources: Voices from the Open Source Revolution - Essays from open-source pioneers such as Linus Torvalds (Linux), Larry Wall (Perl), and Richard Stallman (GNU). | . Open Source contribution initiatives . List of initiatives that aggregate beginner friendly issues to work on or seasonal events. . Up For Grabs - Contains projects with beginner-friendly issues. | First Timers Only - A list of bugs that are labelled “first-timers-only”. | First Contributions - Make your first Open Source contribution in 5 minutes. A tool and tutorial to help beginners get started with contributions. Here is the GitHub source code for the site and opportunity to make a contribution to the repository itself. | Hacktoberfest - A program to encourage Open Source contributions. Earn gifts like t-shirts and stickers by making at least 4 pull requests in the month of October. | 24 Pull Requests - 24 Pull Requests is a project to promote Open Source collaboration during the month of December. | Ovio - A platform with a curated selection of contributor-friendly projects. It has a powerful issue search tool and let’s you save projects and issues for later. | Contribute-To-This-Project - This is a tutorial to help first-time contributors to participate in a simple and easy project and get comfortable in using GitHub. | Open Source Welcome Committee - The Open Source Welcome Committee (OSWC) helps newcomers join the extraordinary world of Open Source. Come submit your open-source projects with us! | . Open Source programs to participate in . A program, internship, or fellowship hosted by a community to help match beginning contributors with mentors and resources to contribute to open source software projects. . All Linux Foundation (LF) Mentorships | Cloud Native Computing Foundation | Beginner friendly Open Source programs with their timelines | FossAsia | Free Software Foundation (FSF) Internship | Google Summer of Code - An annually run paid program by Google focused on bringing more student developers into open-source software development. | Hacktoberfest | LF Networking Mentorship | Microsoft Reinforcement Learning | Open Summer of Code | Outreachy | Processing Foundation Internship | Social Summer of Code - Social foundation offers this two-month long summer program for students to learn about the open-source culture and get involved in the community. Participants contribute to real-life projects under the guidance of experienced mentors. | Girlscript Summer of Code - A three-month-long Open-Source Program conducted every summer by the Girlscript Foundation. With constant efforts, participants contribute to numerous projects under the extreme guidance of skilled mentors over these months. With such exposure, students begin to contribute to real-world projects from the comfort of their homes. | Rails Girls Summer of Code - A global fellowship program for women and non-binary coders where they work on existing open-source projects and expand their skillset. | Major League Hacking (MLH) Fellowship - A remote internship alternative for aspiring technologists where they build, or contribute to open-source projects. | . Credits . https://github.com/freeCodeCamp/how-to-contribute-to-open-source .",
            "url": "https://tayalmanan28.github.io/my_blogs/open-source/2021/09/14/How-to-Contribute-to-Open-Source.html",
            "relUrl": "/open-source/2021/09/14/How-to-Contribute-to-Open-Source.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Resources and Roadmap for Control Theory",
            "content": "Robot kinematics and dynamics . Courses . Introduction to Robotics by Oussama Khatib: robotics foundations in kinematics, dynamics and control The purpose of this course is to introduce you to basics of modeling, design, planning, and control of robot systems | In essence, the material treated in this course is a brief survey of relevant results from geometry, kinematics, statics, dynamics, and control | . | . Books for reference . A Mathematical Introduction to Robotic Manipulation - Richard M. Murray, CalTech | Robot Dynamics Lecture Notes - Robotic Systems Lab, ETH Zurich | Nonlinear dynamics and Chaos by Steven Strogatz: This is a gem of a book. Not focused on robotic systems but the underlying concepts are universal | . Control Theory . Classical Control . Classical Control Theory by Brian Douglas. Why this lecture series? : This collection of videos is intended to supplement a first-year controls class, not replace it. Here the goal will be to take specific concepts in controls and expand on them to provide an intuitive understanding which will ultimately make one a better controls engineer. Let’s take an example of a system that we want to control, say a simple pendulum (the most basic example), suppose we want the pendulum to go upright and stay there, well how do we achieve that? The most basic approach is using the knowledge from control systems to make it stay upright and be able to handle external disturbances for robustness. . | Control Bootcamp by Steve Brunton. Series info : This course provides a rapid overview of optimal control (controllability, observability, LQR, Kalman filter, etc.). It is not meant to be an exhaustive treatment, but instead provides a high-level overview of some of the main approaches, applied to simple examples in Matlab. . | . Note: For beginners, It is recommended to first go through Brian Douglas (till video 27) course before starting Steve Brunton’s course. . Control of Mobile Robots by Dr. Magnus Egerstedt. For whom ?: Whoever wants to explore the field of Control Theory, Motion, or path planning should surely have a look at this course to get a basic overview. This course will focus more on differential drive robots. What’s good about this course ?: It is fairly different from the general long lectured continuous courses, it maintains a very good balance between theory and its implementation. The concepts are explained excellently using some elegant analogies. Course info: Course focuses on the problem of establishing control over a robot with the pursuit of making it move safely and optimally. The course covers the classic PID control and its applications in controlling a differential drive robot and then moves on to cover few concepts from the Classical Control Theory which helps span principles and fundamentals applicable for control of almost all types of dynamical systems. | . Advanced . Note: the focus of these courses is on math and algorithms. You will not study mechanical or electrical design of robots . Underactuated Robotics by Russ Tedrake [textbook] [videos]: Algorithms for Walking, Running, Swimming, Flying, and Manipulation This course introduces nonlinear dynamics and control of underactuated mechanical systems, with an emphasis on computational methods | Topics include the nonlinear dynamics of robotic manipulators, applied optimal and robust control and motion planning | Discussions include examples from biology and applications to legged locomotion, compliant manipulation, underwater robots, and flying machines | Main topics covered Dynamic Programming, LQR, Lyapunov Analysis, Trajectory Optimization, Model Predictive Control, Motion Planning as Search, Pixels to Torques, Robust and Stochastic Control to State Estimation, Reinforcement Learning and few other | . | Advanced Robotics by Peiter Abbeel: (optional, only for more conceptual understanding and depth) Learn the math and algorithms underneath state-of-the-art robotic systems The majority of these techniques are heavily based on optimization and probabilistic reasoning — both areas with wide applicability in modern Artificial Intelligence. | An intended side-effect of the course is to generally strengthen your expertise in these areas | . | Be able to understand research papers in the field of robotics: Main conferences: ICRA, IROS, RSS, CoRL, ISER, ISRR | Main journals: IJRR, T-RO, Autonomous Robots | . | Main topics covered: MDPs, Discretization of Continuous State Space MDPs, Function Approximation / Feature-based Representations, LQR, iterative LQR / Differential Dynamic Programming, Unconstrained Optimization, Constrained Optimization, Optimization-based Control: Collocation, Shooting, MPC, Contact-Invariant Optimization, Motion Planning: RRT, PRM, Trajopt, 3-d poses, Probability Review, Bayes Filters, Multivariate Gaussians, Kalman Filtering, EKF, UKF, Smoother, MAP, Maximum Likelihood, EM, KF parameter estimation, Particle Filters, POMDPs, Imitation Learning, Policy Gradients, Off-policy RL, Model-based RL, Physics simulators working, Sim2Real and few other | . | . Specific Control Methods . PID control . Understanding PID control from MATLAB. | . Sliding Mode Control . Lecture by Sarah Spurgeon. | . Adaptive Control . Non-Linear &amp; Adaptive Control (Nptel) lecture by Dr. Shubendu Bhasin. | . Pure Pursuit controller . Pure pursuit explanation | Coursera lecture | . Books for Reference . Robotics Modeling, Planning, and Controls By Bruno Siciliano, Lorenzo Sciavicco, Luigi Villani, Giuseppe Oriolo. | Modern Control Engineering by Kastukio Ogota. | Sliding Mode Control by Christopher Edwards and Sarah Spurgeon. | Non-Linear and adaptive control by Zhengato Din. (Good explanation of Backstepping). | . Libraries, Tools and Frameworks . ROS | Matlab &amp; Simulink (For Windows) | CasADi | Python Control Systems package | C++ control toolbox | Drake | OpenAI Gym (Some environments will need to be modified to implement control algorithms) | . Some Interesting Repositories to check out . Python - Robotic Algorithms | Robotics Planning, Dynamics and Control | MATLAB - Trajectory Optimization | TOWR (Trajectory Optimization Library for legged Robots) | . Credits . IvLabs .",
            "url": "https://tayalmanan28.github.io/my_blogs/control/robotics/2021/01/13/Resources-and-Roadmap-for-control-theory.html",
            "relUrl": "/control/robotics/2021/01/13/Resources-and-Roadmap-for-control-theory.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Reinforcement Learning",
            "content": "What is reinforcement learning? . Reinforcement learning (RL) is a machine learning technique that focuses on training an algorithm following the cut-and-try approach. The algorithm (agent) evaluates a current situation (state), takes an action, and receives feedback (reward) from the environment after each act. Positive feedback is a reward (in its usual meaning for us), and negative feedback is punishment for making a mistake. . . RL algorithm learns how to act best through many attempts and failures. Trial-and-error learning is connected with the so-called long-term reward. This reward is the ultimate goal the agent learns while interacting with an environment through numerous trials and errors. The algorithm gets short-term rewards that together lead to the cumulative, long-term one. . So, the key goal of reinforcement learning used today is to define the best sequence of decisions that allow the agent to solve a problem while maximizing a long-term reward. And that set of coherent actions is learned through the interaction with environment and observation of rewards in every state. . Difference between reinforcement learning, supervised learning, and unsupervised learning . Reinforcement learning is distinguished from other training styles, including supervised and unsupervised learning, by its goal and, consequently, the learning approach. . Reinforcement learning vs supervised learning. . In supervised learning, an agent “knows” what task to perform and which set of actions is correct. Data scientists train the agent on historical data with target variables (desired answers with predictive analysis) AKA labeled data. The agent receives direct feedback. As a result of training, an agent can forecast whether there will be target variables in new data or not. Supervised learning allows for solving classification and regression tasks. . Reinforcement learning doesn’t rely on labeled datasets: The agent isn’t told which actions to take or the optimal way of performing a task. RL uses rewards and penalties instead of labels associated with each decision in datasets to signal whether a taken action is good or bad. So, the agent only gets feedback once it completes the task. That’s how time-delayed feedback and the trial-and-error principle differentiate reinforcement learning from supervised learning. . Since one of the goals of RL is to find a set of consecutive actions that maximize a reward, sequential decision making is another significant difference between these algorithm training styles. Each agent’s decision can affect its future actions. . Reinforcement learning vs unsupervised learning. . In unsupervised learning, the algorithm analyzes unlabeled data to find hidden interconnections between data points and structures them by similarities or differences. RL aims at defining the best action model to get the biggest long-term reward, differentiating it from unsupervised learning in terms of the key goal. . Reinforcement and deep learning. . Most of reinforcement learning implementations employ deep learning models. They involve the use of deep neural networks as the core method for agent training. Unlike other machine learning methods, deep learning fits best for recognizing complex patterns in images, sounds, and texts. Additionally, neural networks allow data scientists to fit all processes into a single model without breaking down the agent’s architecture into multiple modules. . Reinforcement learning use cases . Reinforcement learning is applicable in numerous industries, including internet advertising and eCommerce, finance, robotics, and manufacturing. Let’s take a closer look at these use cases. . Personalization . News recommendation. . Machine learning has made it possible for businesses to personalize customer interactions at scale through the analysis of data on their preferences, background, and online behavior patterns. . However, recommending such content type as online news is still a complex task. News features are dynamic by nature and become rapidly irrelevant. User preferences in topics change as well. . Authors of the research paper DRN: A Deep Reinforcement Learning Framework for News Recommendation discuss three main challenges related to news recommendation methods. First, these methods only try to model current (short-term) reward (e.g., click-through rate that shows the ratios page/ad/email viewers that click on a link). The second issue is that current recommendation methods usually take into account the click/no click labels or ratings as users’ feedback. And third, these methods typically continue suggesting similar news to readers, so users can get bored. . The researchers used the Deep Q-Learning based recommendation framework that considers current reward and future reward simultaneously in addition to user return as feedback rather than clicks data. . Games personalization. . Gaming companies also have joined the personalization party. Really, why not tailor a video game experience (rules and content) taking into account an individual player’s skill level, playing style, or preferred gameplay? Personalization of game experience is done through player modeling with the goal of increasing their enjoyment. A player model is an abstract description of a player based on their behavior in a game. . Game components that can be adapted include space, mission, character, narrative, music and sound, game mechanics, difficulty scaling, and player matching (in multiplayer games). . RL can be used for optimizing game experience in real-time. In Reinforcement learning for game personalization on edge devices, researchers demonstrate capabilities of this machine learning technique using Pong, last century’s arcade game, as the example. . Unity provides an ML toolset for researchers and developers that allows for training intelligent agents with reinforcement learning and “evolutionary methods via a simple Python API.” . It’s worth mentioning that we haven’t found any application of RL agents in production. . eCommerce and internet advertising . Specialists are experimenting with reinforcement learning algorithms to solve a problem of impressions allocation on eCommerce sites like eBay, Taobao, and Amazon. Impressions refer to the number of times a visitor sees some element of a web page, an ad or a product link with a description. Impressions are often used to calculate how much an advertiser has to pay to show his message on a website. Each time a user loads a page and the ad pops up, it counts as one impression. . These platforms aim to generate maximum total revenue from transactions, that’s why they must use algorithms that will allocate buyer impressions (show buyer requests on items) to the most appropriate potential merchants. . Most platforms use such recommendation methods as collaborative filtering or content-based filtering. These algorithms rank merchants using their “historical scores” that rely on the transaction history of the sellers with customers of similar characteristics. Sellers experiment with prices to get higher ranking positions, and these algorithms don’t take into account changes in pricing schemes. . As a solution to this problem, researchers applied a general framework of reinforcement mechanism design. The framework uses deep reinforcement learning to develop efficient algorithms that evaluate sellers’ behavior. . Online merchants can also conduct fraudulent transactions to improve their rating on eCommerce platforms to draw more buyers. And that, according to researchers, decreases the efficiency of use of buyer impressions and threatens the business environment. However, it’s possible to improve the platform’s impression allocation mechanism while increasing its profit and minimizing fraudulent activities with reinforcement learning. . In the article on AI and DS advances and trends, we discussed another RL use case – real-time bidding strategy optimization. It allows businesses to dynamically allocate the advertisement campaign budget “across all the available impressions on the basis of both the immediate and future rewards.” During real-time bidding, an advertiser bids on an impression, and their ad is displayed on a publisher’s platform if they win an auction. . Trading in financial industry . Financial institutions use AI-driven systems to automate trading tasks. Generally, these systems use supervised learning to forecast stock prices. What they can’t do is to decide what action to take in a specific situation: to buy, sell, or hold. Traders still must make business rules that are trend-following, pattern-based, or counter-trend to govern system choices. Because analysts may define patterns and confirmation conditions in different ways, there is a need for consistency. . Michael Kearns, computer science professor at the University of Pennsylvania, hired by Morgan Stanley, stock trading firm, in June 2018 noted that RL models allow for making predictions that take into account outcomes of one’s actions on the market. In addition, traders may also learn about the most appropriate time for action and/or the optimum size of a trade. . IBM built a financial trading system on its Data Science Experience platform that utilizes reinforcement learning. “The model winds around training on the historical stock price data using stochastic actions at each time step, and we calculate the reward function based on the profit or loss for each trade,” said Aishwarya Srinivasan from IBM. Developers use active return on investment to evaluate the model’s performance. An active return is the difference between the benchmark and the actual return expressed as a percentage. . Specialists also evaluate the performance of the investment against the market index that represents market movement in general. “Finally, we assess the model against a simple Buy-&amp;-Hold strategy and against ARIMA-GARCH. We found that the model had much-refined moderation according to the market movements, and could even capture the head-and-shoulder patterns, which are non-trivial trends that can signal reversals in the market,” added Srinivasan. . Autonomous vehicles training . Reinforcement learning has proven to be an effective method for training deep learning networks that power self-driving car systems. UK company Wayve claims to be the first one to develop a driverless car that works with the help of RL. . Wayve specialists train a self-driving car with reinforcement learning . Developers generally write a large list of hand-written rules to tell autonomous vehicles how to drive. And, that has led to slow development cycles. Wayve specialists chose the other way. They spent only 15-20 minutes to teach a car from scratch to follow a lane through trial and error. A human driver that was in the vehicle during an experiment intervened when the algorithm made a mistake and the car was going off track. The algorithm was rewarded for a distance driven without intervention. That way a car has learned online getting better in driving safely with every exploration episode. Researchers explain the technical side of training in their blog post. . Robotics . Numerous problems in robotics can be formulated as reinforcement learning ones. A robot learns optimal sequential actions to complete a task with a maximum cumulative reward through exploration by receiving feedback from the environment. Developers don’t give it detailed instructions for solving a problem. . The RL in robotics survey authors point out that reinforcement learning provides a framework and a range of tools for the design of sophisticated and hard-to-engineer behaviors. . Specialists from the Google Brain Team and X company introduced a scalable reinforcement learning approach to solving a problem of training vision-based dynamic manipulation skills in robots. The goal was to train robots to grasp various objects, including objects unseen during training. . They combined deep learning and RL technique to enable robots to continuously learn from their experience and improve their basic sensorimotor skills. Specialists didn’t have to engineer behaviors themselves: Robots automatically learned how to complete this task. Specialists designed a deep Q-learning algorithm (QT-Opt) that employs data collected during past training episodes (grasping attempts). . Seven robots have been trained with more than 1000 visually and physically diverse objects for 800 hours over four months. A camera image was analyzed to suggest how a robot should move its arm and gripper. . The novel approach led to a 96 percent success rate of the grasp attempts across 700 test grasps on previously unseen objects. A supervised-learning based approach the specialists used before showed a 78 percent-success rate. . It also turned out that the algorithm achieves that accuracy while requiring less training data (although the training time was longer). . Industrial automation . RL has a potential to be widely used in industrial settings for machinery and equipment tuning supplementing human operators. Bonsai is one of the startups that provides a deep reinforcement learning platform for building autonomous industrial solutions to control and optimize the work of systems. . For instance, customers can improve energy efficiency, reduce downtime, increase equipment longevity, and control vehicles and robots in real time. You can listen to the O’Reilly Data Show podcast in which Bonsai CEO and founder describes various possible RL use cases for companies and enterprises. . Google uses the power of reinforcement learning to become more environmentally friendly. The tech company’s IA research group, DeepMind, developed and deployed RL models that helped reduce energy consumption for cooling data centers by up to 40 percent and decreased total energy overhead by 15 percent. . Challenges to implementing reinforcement learning in business . The application of RL for solving business problems may pose serious challenges. That’s because this technique is exploratory in nature. The agent collects data on the go since there is no labeled or unlabeled data to guide it with a task goal. The decisions taken influence the data received. That’s why the agent may need to try out different actions to get new data. . Environment unpredictability. . An RL algorithm may perform exceptionally when trained in closed, synthetic environments. In video games, for example, conditions under which the agent repeats its decision process don’t change. That’s not the case for the real world. It’s for these reasons that industries like finance, insurance, or healthcare think twice before investing their money into trials of RL-based systems. . Delayed feedback. . In real-life applications, it’s uncertain how much time would be required to realize the outcome of a specific decision. For example, if an AI trading system predicts that the investment in some assets (real estate) would be beneficial, we’ll need to wait a month, year, or several years until we figure out whether that was a good idea. . Infinite time horizons. . In RL, an agent’s number one goal is to get the highest reward possible. As we don’t know how much time or tries it will take, we have to establish an infinite horizon objective. For instance, if we were testing a self-driving car (that uses RL) to switch lanes, we couldn’t tell how many times it will hit other vehicles on the road until it does it right. . Defining a precise reward function. . Data scientists may struggle with expressing the definition of good or bad action mathematically, computing a reward for the action. The advice is to think about reward functions in terms of current states, allowing the agent to know whether the action it is about to take will help it get closer to a final goal. For example, if there is the need to train a self-driving car to turn right without hitting a fence, sizes of reward functions would depend on the distance between a car and a fence and the start of steering. . Data problem and exploration risks. . RL requires even more data than supervised learning. “It is really difficult to get enough data for reinforcement learning algorithms. There’s more work to be done to translate this to businesses and practice,” said computer scientist and entrepreneur Andrew Ng during his speech at the Artificial Intelligence Conference in San Francisco 2017. Just imagine what chaos the self-driving vehicle’s system could cause if it was tested solely on a street: It can hit neighbor cars, pedestrians, or smash into a guardrail. So, testing devices or systems using RL in a real environment can be difficult, financially irrational, and dangerous. One of the solutions is to test it on synthetic data (3D environments) while taking into account all the possible variables that may influence the agent’s decision at each situation or time step (pedestrians, road type and quality, weather conditions, etc.) . Conclusion . Despite training difficulties, reinforcement learning finds its way to be effectively used in real business scenarios. Generally, RL is valuable when searching for optimal solutions in a constantly changing environment is needed. . Reinforcement learning is used for operations automation, machinery and equipment control and maintenance, energy consumption optimization. The finance industry also acknowledged the capabilities of reinforcement learning for powering AI-based training systems. Although trial-and-error training of robots is time-consuming, it allows robots to better evaluate real-world situations, use their skills for completing tasks, or reacting to unexpected consequences appropriately. In addition, RL provides opportunities for eCommerce players in terms of revenue optimization, fraud prevention, and customer experience enhancement via personalization. .",
            "url": "https://tayalmanan28.github.io/my_blogs/reinforcement-learning/robotics/2020/08/23/reinforcement-learning.html",
            "relUrl": "/reinforcement-learning/robotics/2020/08/23/reinforcement-learning.html",
            "date": " • Aug 23, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Tutorial on Pybullet",
            "content": "Introduction to Pybullet . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/pybullet/robotics/2020/06/16/Tutorial-on-Pybullet.html",
            "relUrl": "/pybullet/robotics/2020/06/16/Tutorial-on-Pybullet.html",
            "date": " • Jun 16, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Introduction to URDF",
            "content": "Describing a Robot . When we create a robotic system, there may be many different software components that need to know about the physical characteristics of the robot. For consistency and simplicity, it is good to keep all of this information in one common location, where any code can reference it. In ROS, we call this the robot description and the information is stored in a URDF (Unified Robot Description Format) file. We encountered URDF files briefly in the last tutorial and in this one we’re going to dig deeper into how to write our own. When we open up a complete URDF file it can look REALLY confusing - they are usually long, with a lot of symbols and words and it can be overwhelming. But if we take a moment to look more closely and break it down, we see that it is made up of a few simple structures repeated over and over. We’ll take a look at these structures and then go through an example file. . Overall structure - Links and Joints . URDF describes a robot as a tree of links, that are connected by joints. The links represent the physical components of the robot, and the joints represent how one link moves relative to another link, effectively defining the location of the links in space. . When we write our URDF file we need to figure out how to split the robot up sensibly into links (and joints). There are two main reasons we would designate a link/joint: . A part of the robot is moving relative to another part (e.g. each segment of a robotic arm) | A part that is not moving relative to another part, but it is convenient to have its own reference point and transform (a common example would be the location of a sensor such as a camera or lidar) | . To cover these scenarios, when we define a joint (the connection between two links) we need to choose what type of joint it is. There are quite a few types, but the most common are: . Revolute - A rotational motion, with minimum/maximum angle limits. | Continuous - A rotational motion with no limit (e.g. a wheel). | Prismatic - A linear sliding motion, with minimum/maximum position limits. | Fixed - The child link is rigidly connected to the parent link. This is what we use for those “convenience” links. | . To understand these a little better, let’s dive into the actual format of URDF. . The URDF Syntax . URDF is based on XML, so everything is represented as a series of tags, which can be nested. There are many different tags we can use, but there are three main ones we need to know about. . The robot tag and XML declaration . A proper XML file should have an XML declaration/prolog in the first line, and then after that will be a single tag (called the root tag), which ALL the other tags live inside of. For a URDF file, this root tag will be the robot tag, and the only thing to note here for now is that we can set the name attribute which lets us (unsurprisingly) specify the name of our robot. . Link tags . A link tag lets us firstly specify the name of a link, as well as some additional characteristics - the visual, collision, and inertial properties. These additional tags are generally optional, however they will be required for certain situations such as simulations (which we’ll cover in a later tutorial). . Visual - This is what we see in RViz and Gazebo. We can specify three aspects: | Geometry - box/ cylinder / sphere with size parameters, or a mesh | Origin - an offset for the geometry so it doesn’t need to be centred around the link origin | Material - Basically, the colour. We can specify the name of a declared material, or describe the colour directly. (Note that this will set the colour in RViz but not Gazebo, more on that in the next tutorial) | Collision - This is used for physics collision calculations. We can set the: | Geometry and Origin - Same options as for visual. This will often be copy-pasted from the Visual tag, however we may want a simpler collision geometry (e.g. box instead of a mesh) for computational reasons. | Inertial - This is also used for physics calculations, but determines how the link responds to forces. The inertial properties are: | Mass - Mass of the link | Origin - The centre of mass (a.k.a centre of gravity). This is the point the link could “balance” on (a slightly more confusing concept in 3D). For most simple cases this will just be the centre (same origin as visual/collision). | Inertia - The rotational inertia matrix. This is probably the most confusing part of this section. It describes how the distribution of the mass will affect rotation. A list of the matrices for common shapes is available here. A fairly accurate simulation can be achieved by approximating our links as prisms, ellipsoids, or cylinders, however for most purposes a very rough guess will do. Also, don’t get inertial and inertia mixed up - the inertia matrix is just one part of the inertial properties. | . Unless we are designing a very precise controller, there will be a fair bit of wiggle room in these parameters, especially the inertial ones. And remember, not every link needs all of these! Also, we can have multiple visual and collision tags for a single link if we want, so we can combine them to make more complex shapes. . This image shows the overall structure of a link tag, at the end of the tutorial we’ll look in more detail at a working example. . Joint tags . Although we usually think of the robot as being made up of links, the joints are actually where all the detail is in terms of the robot’s structure, as they define the link locations, and how they move relative to each other. This is similar to the previous tutorial on the TF system, where although we ultimately want to interact with frames, it’s actually the transforms that define where the frames are, so it’s important to get them right. . Each joint will need to have the following specified: . Name - A name for the joint. If we don’t have this, some things will still work, but other things won’t. It’s good to name them all just in case. | Type - The joint type as mentioned earlier (the four most popular being fixed, prismatic, revolute, and continuous). | Parent and child links - Which links this joint defines a relationship between | Origin - The relationship between the two links, before any motion is applied | . For a fixed joint these parameters will be sufficient, but for non-fixed joints we will usually want to specify some other characteristics. There are a few to choose from, but the two most common will be: . Axis - Which axis to move along or around | Limits - Physical actuation limits, which may be expected by other parts of the system. These can include: | Upper and Lower position limits - in metres/radians | Velocity limits - in m/s or rad/s | Effort limits - in N or Nm | . Below is a snippet of one of the joints that we’ll look at in the example at the end. . Extra Tags . The robot, link, and joint tags are the main tags that make up a URDF file, but we’re not just limited to these! There are a few other tags that are part of the URDF spec, but we can also add any other tags we like and they will just be ignored if not needed. Certain nodes will expect these extra tags and can make use of them. . Some common extra tags that you might come across are: . material - Allows us to give a “name” to a colour once, and then reuse that name in as many links as we like | gazebo - Lets us specify certain parameters that are used in the Gazebo simulation environment (more on this in the next tutorial!) | transmission - Provides more detail about how the joints are driven by physical actuators | . Naming conventions . We need to name all our links to work with them, however it’s good practice to name all our joints too. It’s also good to keep consistency within your naming by following conventions, whether these are conventions set by ROS or just our own ones. Some examples of naming conventions we can follow are: . Keeping our links/joints paired, and using the suffix _link and _joint (e.g. arm_link and arm_joint) Following the ROS conventions for humanoid robots Following the ROS conventions for mobile platforms . Credits . ROS-URDF | ROS-URDF Tutorials | .",
            "url": "https://tayalmanan28.github.io/my_blogs/robotics/2020/05/16/Introduction-to-URDF.html",
            "relUrl": "/robotics/2020/05/16/Introduction-to-URDF.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "3D Printing",
            "content": "Introduction . 3D printing, or additive manufacturing, is the construction of a three-dimensional object from a CAD model or a digital 3D model. The term “3D printing” can refer to a variety of processes in which material is deposited, joined or solidified under computer control to create a three-dimensional object, with material being added together (such as plastics, liquids or powder grains being fused together), typically layer by layer. . . In the 1980s, 3D printing techniques were considered suitable only for the production of functional or aesthetic prototypes, and a more appropriate term for it at the time was rapid prototyping. As of 2019, the precision, repeatability, and material range of 3D printing have increased to the point that some 3D printing processes are considered viable as an industrial-production technology, whereby the term additive manufacturing can be used synonymously with 3D printing. One of the key advantages of 3D printing is the ability to produce very complex shapes or geometries that would be otherwise impossible to construct by hand, including hollow parts or parts with internal truss structures to reduce weight. Fused deposition modeling (FDM), which uses a continuous filament of a thermoplastic material, is the most common 3D printing process in use as of 2020. . General Principles . Modeling . 3D printable models may be created with a computer-aided design (CAD) package, via a 3D scanner, or by a plain digital camera and photogrammetry software. 3D printed models created with CAD result in relatively fewer errors than other methods. Errors in 3D printable models can be identified and corrected before printing. The manual modeling process of preparing geometric data for 3D computer graphics is similar to plastic arts such as sculpting. 3D scanning is a process of collecting digital data on the shape and appearance of a real object, creating a digital model based on it. . CAD models can be saved in the stereolithography file format (STL), a de facto CAD file format for additive manufacturing that stores data based on triangulations of the surface of CAD models. STL is not tailored for additive manufacturing because it generates large file sizes of topology optimized parts and lattice structures due to the large number of surfaces involved. A newer CAD file format, the Additive Manufacturing File format (AMF) was introduced in 2011 to solve this problem. It stores information using curved triangulations. . Printing . Before printing a 3D model from an STL file, it must first be examined for errors. Most CAD applications produce errors in output STL files,[49][50] of the following types: . holes | faces normals | self-intersections | noise shells | manifold errors | overhang issues | . A step in the STL generation known as “repair” fixes such problems in the original model. Generally STLs that have been produced from a model obtained through 3D scanning often have more of these errors as 3D scanning is often achieved by point to point acquisition/mapping. 3D reconstruction often includes errors. . Once completed, the STL file needs to be processed by a piece of software called a “slicer”, which converts the model into a series of thin layers and produces a G-code file containing instructions tailored to a specific type of 3D printer (FDM printers). This G-code file can then be printed with 3D printing client software (which loads the G-code, and uses it to instruct the 3D printer during the 3D printing process). . Printer resolution describes layer thickness and X–Y resolution in dots per inch (dpi) or micrometers (µm). Typical layer thickness is around 100 μm (250 DPI), although some machines can print layers as thin as 16 μm (1,600 DPI). X–Y resolution is comparable to that of laser printers. The particles (3D dots) are around 50 to 100 μm (510 to 250 DPI) in diameter. For that printer resolution, specifying a mesh resolution of 0.01–0.03 mm and a chord length ≤ 0.016 mm generates an optimal STL output file for a given model input file. Specifying higher resolution results in larger files without increase in print quality. . Construction of a model with contemporary methods can take anywhere from several hours to several days, depending on the method used and the size and complexity of the model. Additive systems can typically reduce this time to a few hours, although it varies widely depending on the type of machine used and the size and number of models being produced simultaneously. . Finishing . Though the printer-produced resolution is sufficient for many applications, greater accuracy can be achieved by printing a slightly oversized version of the desired object in standard resolution and then removing material using a higher-resolution subtractive process. . The layered structure of all additive manufacturing processes leads inevitably to a stair-stepping effect on part surfaces which are curved or tilted in respect to the building platform. The effects strongly depend on the orientation of a part surface inside the building process. . Some printable polymers such as ABS, allow the surface finish to be smoothed and improved using chemical vapor processes based on acetone or similar solvents. . Some additive manufacturing techniques are capable of using multiple materials in the course of constructing parts. These techniques are able to print in multiple colors and color combinations simultaneously, and would not necessarily require painting. . Some printing techniques require internal supports to be built for overhanging features during construction. These supports must be mechanically removed or dissolved upon completion of the print. . All of the commercialized metal 3D printers involve cutting the metal component off the metal substrate after deposition. A new process for the GMAW 3D printing allows for substrate surface modifications to remove aluminum or steel. . Materials . Traditionally, 3D printing focused on polymers for printing, due to the ease of manufacturing and handling polymeric materials. However, the method has rapidly evolved to not only print various polymers but also metals and ceramics, making 3D printing a versatile option for manufacturing. Layer-by-layer fabrication of three-dimensional physical models is a modern concept that “stems from the ever-growing CAD industry, more specifically the solid modeling side of CAD. Before solid modeling was introduced in the late 1980s, three-dimensional models were created with wire frames and surfaces.” but in all cases the layers of materials are controlled by the printer and the material properties. The three-dimensional material layer is controlled by deposition rate as set by the printer operator and stored in a computer file. The earliest printed patented material was a Hot melt type ink for printing patterns using a heated metal alloy. See 1970s history above. . Charles Hull filed the first patent on August 8, 1984, to use a UV-cured acrylic resin using a UV masked light source at UVP Corp to build a simple model. The SLA-1 was the first SL product announced by 3D Systems at Autofact Exposition, Detroit, November 1978 in Detroit. The SLA-1 Beta shipped in Jan 1988 to Baxter Healthcare, Pratt and Whitney, General Motors and AMP. The first production SLA-1 shipped to Precision Castparts in April 1988. The UV resin material changed over quickly to an epoxy-based material resin. In both cases, SLA-1 models needed UV oven curing after being rinsed in a solvent cleaner to remove uncured boundary resin. A Post Cure Apparatus (PCA) was sold with all systems. The early resin printers required a blade to move fresh resin over the model on each layer. The layer thickness was 0.006 inches and the HeCd Laser model of the SLA-1 was 12 watts and swept across the surface at 30 in per second. UVP was acquired by 3D Systems in Jan 1990. . A review in the history shows a number of materials (resins, plastic powder, plastic filament and hot-melt plastic ink) were used in the 1980s for patents in the rapid prototyping field. Masked lamp UV-cured resin was also introduced by Cubital’s Itzchak Pomerantz in the Soldier 5600, Carl Deckard’s (DTM) laser sintered thermoplastic powders, and adhesive-laser cut paper (LOM) stacked to form objects by Michael Feygin before 3D Systems made its first announcement. Scott Crump was also working with extruded “melted” plastic filament modeling (FDM) and Drop deposition had been patented by William E Masters a week after Charles Hull’s patent in 1984, but he had to discover Thermoplastic Inkjets introduced by Visual Impact Corporation 3D printer in 1992 using inkjets from Howtek, Inc., before he formed BPM to bring out his own 3D printer product in 1994. . Multi-material 3D printing . Efforts to achieve multi-material 3D printing range from enhanced FDM-like processes like VoxelJet, to novel voxel-based printing technologies like layered assembly. . A drawback of many existing 3D printing technologies is that they only allow one material to be printed at a time, limiting many potential applications which require the integration of different materials in the same object. Multi-material 3D printing solves this problem by allowing objects of complex and heterogeneous arrangements of materials to be manufactured using a single printer. Here, a material must be specified for each voxel (or 3D printing pixel element) inside the final object volume. . The process can be fraught with complications, however, due to the isolated and monolithic algorithms. Some commercial devices have sought to solve these issues, such as building a Spec2Fab translator, but the progress is still very limited. Nonetheless, in the medical industry, a concept of 3D printed pills and vaccines has been presented. With this new concept, multiple medications can be combined, which will decrease many risks. With more and more applications of multi-material 3D printing, the costs of daily life and high technology development will become inevitably lower. . Metallographic materials of 3D printing is also being researched. By classifying each material, CIMP-3D can systematically perform 3D printing with multiple materials. . 4D printing . Using 3D printing and multi-material structures in additive manufacturing has allowed for the design and creation of what is called 4D printing. 4D printing is an additive manufacturing process in which the printed object changes shape with time, temperature, or some other type of stimulation. 4D printing allows for the creation of dynamic structures with adjustable shapes, properties or functionality. The smart/stimulus responsive materials that are created using 4D printing can be activated to create calculated responses such as self-assembly, self-repair, multi-functionality, reconfiguration and shape shifting. This allows for customized printing of shape changing and shape-memory materials. . 4D printing has the potential to find new applications and uses for materials (plastics, composites, metals, etc.) and will create new alloys and composites that were not viable before. The versatility of this technology and materials can lead to advances in multiple fields of industry, including space, commercial and the medical field. The repeatability, precision, and material range for 4D printing must increase to allow the process to become more practical throughout these industries. . To become a viable industrial production option, there are a couple of challenges that 4D printing must overcome. The challenges of 4D printing include the fact that the microstructures of these printed smart materials must be close to or better than the parts obtained through traditional machining processes. New and customizable materials need to be developed that have the ability to consistently respond to varying external stimuli and change to their desired shape. There is also a need to design new software for the various technique types of 4D printing. The 4D printing software will need to take into consideration the base smart material, printing technique, and structural and geometric requirements of the design. .",
            "url": "https://tayalmanan28.github.io/my_blogs/3d-printing/robotics/2019/12/22/3D-Printing.html",
            "relUrl": "/3d-printing/robotics/2019/12/22/3D-Printing.html",
            "date": " • Dec 22, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "How to Choose Batteries",
            "content": "Batteries are the energy storage units of many devices that we come across every day; they are available in different forms, sizes, parameters, and shapes. You can commonly find them being used in automotive, Backup power supplies, mobile devices, laptops, iPads, and many other portable electronic devices. But, not all the devices can use the same kind of battery; each and every device has its own specifications and power supply requirements and you will need a battery selection guide to pick the right battery for your application. So in this article will look into the factors to consider while selecting a battery for your next electronic product design. If you are completely news to batteries then it is recommended to read this article on types of batteries and their applications to understand the basics of battery before you proceed further. . Factors for Choosing Batteries . While choosing a battery for your application you must know about the important parameters involved in its operation. The reality about the battery is that there is no common type of battery for all the applications since no battery is perfect. If you want to utilize one parameter of the battery you should be able to handle the depletion of other parameters. For example, if you want your battery to deliver lots of power for your application, the cell internal resistance should be minimized which is only possible by increasing the electrode surface area. This also increases inactive components such as current collectors and conductive aid, so energy density is traded off to gain power. In order to provide exactly what you want in your application, you must give up something to gain the other in a battery. The important battery parameters are given in the following image. . Now, let’s look into each battery parameter briefly to understand its importance and impact on battery performance during operation. . Rechargeable / Non-Rechargeable batteries . There might not be much confusion in choosing between a primary and secondary battery, you must only know if you want the battery to be used once or multiple times. The primary (Non-Rechargeable) battery can be used for occasional uses like toys, Flashlights, Smoke alarm, etc. They are also used in devices in which charging isn’t possible like pacemakers, wristwatches and hearing aids. The Secondary (rechargeable) batteries can be used in the applications where there is a need for a regular power source like mobile phones, laptops, Automotives, etc. The secondary batteries always have a higher self-discharge rate compared to primary batteries which are an ignorant fact due to its ability to be recharged. . Availability of Space . The Batteries are available in various shapes and size like button cells, cylindrical cells, Pouch cells &amp; prismatic cells. The size of the battery really matters in order to make your device easily portable. The standard sizes available are AA, AAA and 9V batteries suitable for portable devices. Commonly lithium batteries (pouch type) are preferred in applications where there is less space but more power requirement. If the power requirement is less then coin cells can also be considered since they are very compact and the smallest of battery types. . Different Shapes of Battery . System Operating Voltage . The battery voltage is one of the most important characteristics of the battery, which is determined based on the electrode &amp; electrolyte used (Chemical Reaction). There is a common misconception that a fully discharged battery will have 0V it is clearly not the case in any battery. In fact, if a battery reads 0V then it probably is dead. The output voltage of a battery should always read between its nominal voltage level. . The Zinc-Carbon battery and Nickel-metal hydride battery uses water as an electrolyte and delivers a nominal voltage of 1.2V to 2V, whereas the lithium-based batteries use organic electrolytes that can deliver a nominal voltage of 3.2 to 4V. Most of the electronic pieces of equipment operate in the voltage range of 3V. If you use a lithium-based battery a single cell battery will be enough to operate the equipment. Do remember that the voltage of the battery will not be stable and will vary between a minimum value and maximum value based on the available capacity in the battery. This minimum and maximum value of each battery is shown below. . Minimum and Maximum Value of Battery . If your circuit is operating at 5V and you are powering it with a lithium battery, then your nominal voltage will only be 3.2V to 4V. In these cases, boost converter circuits are used to convert the battery voltage to 5V required for the circuit. If your operating voltage is very high like 24V or 12V then you can either use a 12V lead-acid battery or if you need high power density then you can combine more than one lithium cells in series to increase the resulting output voltage. . Operating Temperature . The battery performance can be dramatically changed by the temperature, for instance the battery that is operating with aqueous electrolytes cannot be used in temperature conditions below 0°C as they aqueous electrolyte might get frozen under 0°C, in the same way, the lithium-based batteries might operate up to -40°C but the performance might be dropped. . Battery Performance . The lithium-ion batteries have the maximum charging rate between the temperature ranges of about 20°C to 45°C. If you want to charge beyond this temperature range lower current/voltage need to be used, this will result in longer charging time. If the temperature drops below 5°C or 10°C lithium dendrite plating will be formed in the electrolyte which needs to be prevented by trickle charge. . Capacity of the battery - Power &amp; Energy . The power of the battery determines the runtime of a battery. The power/Capacity of the battery is expressed in Watt-hours (Wh). The Watt-hour is calculated by multiplying the battery voltage (V) by the amount of current that a battery can deliver for a certain amount of time. The voltage of the battery is almost fixed and the current that a battery can deliver is printed on the battery, expressed in Ampere-hour rating (Ah or mAh). . Consider a battery of 5V with 2 Amp-hour (Ah) capacity, hence it has a power of 10Wh. A battery with the capacity of 2Ah can deliver 2 Amp for one hour or 0.2A for 10 hours or 0.02A (20mA) for 100 hours. Battery manufacturers always specify the capacity at a given discharge rate, temperature, and cut-off voltage, where the capacity always depends on all three factors. . The capacity of a battery will tell us how much power it can deliver to an application. For example, consider a 12V, 10Ah car battery, the actual capacity of the battery is 120Wh (12V x 10Ah), but in a laptop battery of 3.6V that has the same 10Ah dissipation will have a capacity of 36Wh (3.6Vx 10Ah). From the example you can see even they have the same Ah the amount of power that a car battery can store is three times higher than a laptop battery. . The following picture will give you more clarity about how the battery capacity differs in different types of batteries. . Capacity of Batteries . Batteries with high power always provide a faster discharge capability at high drain rates like power tools or automobile starter battery applications, most of the high power batteries will have a low energy density. . Battery Chemistry . By this time you would have understood that all the properties of a battery are always depending on the chemistry involved in the battery, so you should be more conscious while you choose the type of battery. On the basis of the chemistry involved in the operation, batteries are classified as Lead Acid Batteries, Alkaline Batteries, Ni-Cad Batteries (Nickel Cadmium), Ni- MH Batteries (Nickel Metal Hydride), Li-Ion (Lithium-Ion) and LiPoly (Lithium Polymer) Batteries . Different types of Battery . Cost of Battery . In most portable electronics products the battery will be one among the expensive item in the Bill of Materials (BOM), hence most of the time it will affect the overall cost of your electronic applications. Hence, you should know your needs and budget of your product and then choose the right battery for your product. . Shelf Life . Not all the batteries are used immediately after manufacturing, they stay on the shelf for a long time before it’s being used. The shelf life of a battery tells you how long a battery can be kept unused. The Shelf life is mostly considered as a fact in primary batteries only as the secondary batteries can be recharged whenever they are used. For example in a fire alarm siren system, the battery might sit there idle for years before it detects a fire and triggers the alarm. So care should be taken that the battery retains its performance even if it is kept unused for a long time. . Which battery should I choose? . Now that we have looked into the parameters you should consider before choosing the battery for a portable electronic application, let’s look into the common cases of choosing the battery. Note that these are just tips and not hard written rules. . For products that consume more power like projectors, large sound systems, and motorized projects you should use lead-acid batteries. If you are going to have heavy usage of the battery you should go for ‘Marine deep cycle’ batteries. | If your electronics need to be super small like an inch on each side you should go for the lithium coin cells or little lithium polymer cells. | If you are going to produce the component in large quantity use inexpensive alkaline batteries of popular sizes. So the customer finds it easy to replace them. | If you want the device to be user-serviceable, like the users can change the battery by themselves go for 9V or AA-size batteries. | Use 3 Alkaline (4.5V) or 4NiMH cells (4.8V) if the circuit needs approximately 5V input. | To build a rechargeable battery pack use a battery holder from your local shop and stick it with NiMH batteries and then start recharging your battery. | If you want to replace your alkaline battery with any of the rechargeable batteries, test your device to make sure that it can operate at lower voltage without any issue. | If you want your battery to have a longer life span always use a high-quality charger with sensors to maintain proper charging and trickle charging because using a cheap charger will kill off your cells in the battery pack. | .",
            "url": "https://tayalmanan28.github.io/my_blogs/battery/robotics/2019/11/19/how-to-choose-batteries.html",
            "relUrl": "/battery/robotics/2019/11/19/how-to-choose-batteries.html",
            "date": " • Nov 19, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "How to Choose Motors",
            "content": "How Motors Work and How to Choose the Right Motor . Motors can be found practically everywhere. This guide will help you learn the basics of electric motors, available types and how to choose the correct motor. The basic questions to answer while deciding which motor is most appropriate for an application are which type should I choose and which specifications matter. . How do motors work? . Electric motors work by converting electrical energy to mechanical energy in order to create motion. Force is generated within the motor through the interaction between a magnetic field and winding alternating (AC) or direct (DC) current. As the strength of a current increases so does the strength of the magnetic field. Keep Ohm’s law (V = I*R) in mind; voltage must increase in order to maintain the same current as resistance increases. . Electric Motors have an array of applications. Conventional industrial uses include blowers, machine and power tools, fans and pumps. Hobbyists generally use motors in smaller applications requiring movement such as robotics or modules with wheels. Types of motors: There are many types of DC motors, but the most common are brushed or brushless. There are also vibrating motors, stepper motors, and servo motors. . DC Brush Motors . DC brush motors are one of the most simple and are found in many appliances, toys, and automobiles. They use contact brushes that connect with a commutator to alter current direction. They are inexpensive to produce and simple to control and have excellent torque at low speeds (measured in revolutions per minute or RPM). A few downsides are that they require constant maintenance to replace worn out brushes, have limited in speed due to brush heating, and can generate electromagnetic noise from brush arcing. . Brushless DC Motors . Brushless DC motors use permanent magnets in their rotor assembly. They are popular in the hobby market for aircraft and ground vehicle applications. They are more efficient, require less maintenance, generate less noise, and have higher power density than brushed DC motors. They can also be mass-produced and resemble an AC motor with a constant RPM, except powered by DC current. There are a few disadvantages however, which include that they are difficult to control without a specialized regulator and they require low starting loads and specialized gearboxes in drive applications causing them to have a higher capital cost, complexity, and environmental limitations. . Vibrating Motors . Vibrating motors are used for applications requiring vibration such as cell phones or game controllers. They are generated by an electric motor and have an unbalanced mass on the drive shaft which causes the vibration. They can also be used in non-electronic buzzers that vibrate for the purpose of sound or for alarms or door bells. . Stepper Motor . Whenever precise positioning is involved, stepper motors are your friend. They’re found in printers, machine tools, and process control systems and are built for high-holding torque that gives the user the ability to move from one step to the next. They have a controller system that designates the position through signal pulses sent to a driver, which interprets them and sends proportional voltage to the motor. They are relatively simple to make and control, but they draw maximum current constantly. Small step distance limits top speed and steps can be skipped at high loads. . Servo Motor . Servo motors are another popular hobby market motor and are used for position control without precision. Their popular applications include remote control applications like RC toy vehicles and robotics. They consist of a motor, potentiometer, and a control circuit and are mostly controlled through pulse width modulation (PWM), through the sending of electrical pulses to the control wire. Servos can be either AC or DC. AC servos can handle higher current surges and are used for industrial machinery, whereas DC servos are for smaller hobbyist applications. To learn more about servos check out our How Servo Motors Work article. . AC Motors . There are three basic types of AC motors: induction, synchronous, and industrial. . Induction Motor . Induction motors are referred to as asynchronous motors, since they do not move at the same constant rate or turn slower than the frequency supplied. Slip, the difference between actual and synchronous speed, is needed to produce torque, the twisting force that causes rotation, in induction motors. The magnetic field that surrounds the rotor of these motors is caused by induced current. . Synchronus Motor . The rotor of synchronous motors spins at a constant rate as AC is supplied. Their magnetic field is created by permanent magnets. Industrial motors are designed for three-phase, high power applications such as conveyers or blowers. AC motors can also be found in home appliances and other applications such as clocks, fans, and disk drives. . What to consider when purchasing a motor: . There are several characteristics that you need pay attention to when selecting a motor but voltage, current, torque, and velocity (RPM) are most important. . Current is what powers the motor and too much current will damage the motor. For DC motors, operating and stall current are important. Operating current is the average amount of current the motor is expected to draw under typical torque. Stall current applies enough torque for the motor to run at stall speed, or 0RPM. This is the maximum amount of current the motor should be able to draw, as well as the maximum power when multiplied by the rated voltage. Heat sinks are important are constantly running the motor or are running it at higher than the rated voltage in order to keep the coils from melting. . | Voltage is used to keep net current flowing in one direction and to overcome back current. The higher the voltage, the higher the torque. The voltage rating of a DC motor indicates the most efficient voltage while running. Be sure to apply the recommended voltage. If you apply too few volts, the motor will not work, whereas too many volts can short windings resulting in power loss or complete destruction. . | Operating and stall values also need to be considered with torque. Operating torque is the amount of torque the motor was designed to give and stall torque is the amount of torque produced when power is applied from stall speed. You should always look at the required operating torque, but some applications will require you to know how far you can push the motor. For example, with a wheeled robot, good torque equals good acceleration but you must make sure the stall torque is strong enough to lift the weight of the robot. In this instance, torque is more important than speed. . | Velocity, or speed (RPM), can be complex regarding motors. The general rule is that motors run most efficiently at the highest speeds but it is not always possible if gearing is required. Adding gears will reduce the efficiency of the motor, so take into account speed and torque reduction as well. . | . These are the basics to consider while selecting a motor. Consider an application’s purpose and which current it uses to select the appropriate type of motor. An application’s specifications such as voltage, current, torque, and velocity will determine which motor is most appropriate so be sure to pay attention to its requirements. .",
            "url": "https://tayalmanan28.github.io/my_blogs/motor/robotics/2019/09/21/how-to-choose-motors.html",
            "relUrl": "/motor/robotics/2019/09/21/how-to-choose-motors.html",
            "date": " • Sep 21, 2019"
        }
        
    
  
    
        ,"post17": {
            "title": "Tutorial on Raspberry-Pi",
            "content": "This tutorial tries to take you through setup and using a Raspberry Pi (or RPi in short) from scratch. All you need is that you’ve bought an RPi and have an internet connection. Actually, I recommend reading this if you’re thinking of buying one, you’ll see some instructions on what you need to buy so it’s a good idea. Also the RPi documentation is the single best resource for starting with your RPi IMO, so you can also read that first. My post is somewhat complementary of that documentation. . Introduction to the RPi . I’ve often come across the question, when do I need an RPi? You’ll often need an RPi if you need a decent amount of portable processing. This is usually for complicated tasks like Machine Learning, Image Processing, running a server, etc. I’ve had mentees use RPi for various combinations of these too. . What’s so great about an RPi? . RPi fulfils the requirement of portable, high processing power. You can load an OS onto the system. It truly is the credit-card sized mini-computer. Added bonus is lots of support in the form of an active online community. RPi supports multiple popular operating systems too (eg. Ubuntu, Debian). This extends to support for multiple programming languages, which support operations on GPIO of RPi including Python and C/C++. With these, there are multiple plugin-like attachments, incuding a camera(RaspiCam). Support for multiple USB devices like a keyboard/mouse too! . Setting Up . Requirements . You should read this page before buying anything. Even if you don’t have everything listed below you can make do (for eg WiFi Router/Ethernet Cable) . Basic Setup:RPi, MicroUSB attachment(to power RPi), a microSD card(below 64GB, recommended 16GB), MicroSD Card Reader, a WiFi Router and an Ethernet Cable(if possible). . Easy Setup: Basic setup + HDMI Cable, Monitor with HDMI support, Keyboard+Mouse(Could do with one of the two, not very sure but prefer having both). . First Steps . First thing to do is choose an OS. If you’re reading this, the most logical choice for you is the Raspbian OS (It’s pretty nice, somewhat light GUI, most tutorials you find anywhere will work on Raspbian). Note that, Raspbian with Pixel DE(Desktop Environment) is the one that packs the User Interface, the other has only a terminal. If you are a complete n00b maybe you should download NOOBS (I’m kidding still use Raspbian). We start with burning an image file on the SD card. Crystal clear instructions are given at this link. . Connecting your RPi to a local network . So we have an OS on the RPi. Now what? We need to setup some sort of connection with the RPi. So here are your options: . Easy setup: You should be super comfortable. You can just connect your RPi with an HDMI cable to the monitor, add a keyboard+mouse, and there you’re done. Connect a good microUSB cable, insert the SD Card and switch the RPi’s power on. You should note you can face issues with dealt with here. It should show you something like this. Now you can use it like an OS! Connect to WiFi, scan the raspi-config for settings so you can setup whatever is appropriate for your case(don’t worry it sounds intimidating but its super easy - when in doubt you can just start exploring options in the menu. There are some games in there which aren’t super fun to play because of all the lag but you can try if you want! :) ) I think you’re good from now on! . Basic setup: No simple way to say this but I’ve failed at this method a few times. Sometimes it’s because of the terrible WiFi module on the RPi(or terrible WiFi I use), sometimes I mess up, sometimes its a plain old fashioned error I can’t understand how to debug because I can’t see anything. Easiest thing to do would be use the easy setup once, configure your RPi, then it’ll be fine to just use remote access. But if you can’t access equipment, here we go! . The crux is to get the RPi on the same local network as the PC. We can just use an ethernet cable and connect our RPi to our PCs. That is quite easy. If you don’t have one I’ll tell you what to do, keep following the post. . Start with your SD Card (plugged in to your PC). We need to edit some specific files in it to make the RPi start an SSH server on boot (IMPORTANT). SSH will enable access to your RPi remotely. You can add edit the file /etc/rc.local and add the following line at the end . /etc/init.d/ssh start . So rc.local is the “what-do-I-do-on-bootup” file. The code above starts an SSH server. . Alternatively, SSH can be enabled by placing a file called ssh in to the boot folder. This flags the Pi to enable the SSH system on the next boot. Make a blank file and name it ssh. Place it there, that’s it. HOWEVER, I DON’T RECOMMEND THIS. Simply because this file deletes on each boot(I think), so if you have to try booting twice maybe thrice, then this will kill. . If you have an ethernet cable you’re done. If not, you need to setup the RPi to connect to your wireless network on it’s own (because without an ethernet cable the RPi is not on a local network with you yet). ALTHOUGH, it’s better to do this step for both with/without ethernet since there is no harm in getting this to work. . I found a decent blog post for setting up the network here. I’m not leaving you out to dry yet don’t worry. BUT READ THAT LINK FIRST. . Edit: Found an even better one in RPi’s documentation. . So what you can do is edit this one file at /etc/wpa_supplicant/wpa_supplicant.conf and what you need to do is copy this part: . country=in update_config=1 ctrl_interface=/var/run/wpa_supplicant network={ ssid=&quot;replace_with_your_ssid&quot; psk=&quot;replace_with_your_password&quot; key_mgmt=WPA-PSK } . Since you are most probably using a WPA-PSK, this shouldn’t be a problem, it should work for all WiFis in general. If you’re not, this page can help you. If you don’t think that’s enough, then try this page. Don’t let it intimidate you, just find out the kind of network you have. You can even use your phone, try connecting to your WiFi and check the options it needs you to enter. Search these options and you should get a good lead on what kind of network your WiFi is. This is the hardest part but after this things should be breezy. . Now all you gotta do is connect to the same WiFi from your PC! . Okay, for people with an Ethernet cable who couldn’t make this work, there is some hope for you. You can connect to WiFi using your RPi terminal, but we’ll get to that shortly. . SSHing into the RPi . Easy setup: You can open the terminal and type sudo raspi-config. This will show you some settings where you may need to change the locale and keyboard layout. This is important because the layout can cause minor problems like in typing out passwords which you won’t be able to debug for a long time (happened to me). . Basic setup: Now that your RPi’s SSH is on, you can place the SD Card into the RPi, and allow it to boot by connecting a power cord. Some (read most) people may face this issue of RPi not booting up properly or at all. Please read this answer in full to resolve your issues, once and for all. Since phones are getting MicroUSB cords with better power every year, this may become redundant in the near future. After bootup fire up your SSH client: in linux/Mac you have an inbuilt one, in Windows you have to install Putty. . Working with Windows its better to follow the RPi docs, which comes packaged with instructions on getting your PI’s IP address. I use nmap for getting the IP generally. I have had many troubles trying to use raspberrypi.local, so I don’t recommend that. . Working on your RPi . Working on your RPi isn’t very difficult. I have tested a few ways and often for me, the way to go is to install git on your RPi and push changes online, while pulling changes into your RPi repo. For small changes, you can use nano/vim when SSHing into your RPi, and change it there. If you are looking for a git or a vim tutorial, you can find one at Grundy, the WnCC Wiki of IIT Bombay. . And this is it! You can now work on your RPi from your PC and hopefully use it in your projects! There are some useful things you can also lookup, like triggering a script on boot here &amp; here, building lighter OSes for RPi or using Raspbian(GUI) without the frizz, by installing PixelDE on the lite version (avoids installation of lots of software you probably don’t need), using RaspiCam and OpenCV with your RPi and doing some GPIO programming on Python. . Time to make your own projects! .",
            "url": "https://tayalmanan28.github.io/my_blogs/raspberry-pi/robotics/2019/06/20/Tutorial-on-Raspberry-Pi.html",
            "relUrl": "/raspberry-pi/robotics/2019/06/20/Tutorial-on-Raspberry-Pi.html",
            "date": " • Jun 20, 2019"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tayalmanan28.github.io/my_blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}