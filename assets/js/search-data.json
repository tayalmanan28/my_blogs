{
  
    
        "post0": {
            "title": "Safe Reinforcement Learning",
            "content": "Safe Reinforcement Learning . Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. . SAC-RCBF . “Safe Model-Based Reinforcement Learning using Robust Control Barrier Functions”. Specifically, an implementation of SAC + Robust Control Barrier Functions (RCBFs) for safe reinforcement learning in two custom environments. . While exploring, an RL agent can take actions that lead the system to unsafe states. Here, we use a differentiable RCBF safety layer that minimially alters (in the least-squares sense) the actions taken by the RL agent to ensure the safety of the agent. . Robust Control Barrier Functions (RCBFs) . In this work, we focus on RCBFs that are formulated with respect to differential inclusions of the following form: . Here D(x) is a disturbance set unkown apriori to the robot, which we learn online during traing via Gaussian Processes (GPs). The underlying library is GPyTorch. . The QP used to ensure the system’s safety is given by: . . where h(x) is the RCBF, and u_RL is the action outputted by the RL policy. As such, the final (safe) action taken in the environment is given by u = u_RL + u_RCBF as shown in the following diagram: . . Coupling RL &amp; RCBFs to Improve Training Performance . The above is sufficient to ensure the safety of the system, however, we would also like to improve the performance of the learning by letting the RCBF layer guide the training. This is achieved via: . Using a differentiable version of the safety layer that allows us to backpropagte through the RCBF based Quadratic Program (QP) resulting in an end-to-end policy. | Using the GPs and the dynamics prior to generate synthetic data (model-based RL). | .",
            "url": "https://tayalmanan28.github.io/my_blogs/reinforcement-learning/robotics/2022/05/02/Safe-Reinforcement-Learning.html",
            "relUrl": "/reinforcement-learning/robotics/2022/05/02/Safe-Reinforcement-Learning.html",
            "date": " • May 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Offline Reinforcement Learning",
            "content": "Offline Reinforcement Learning . Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. . . This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function. Then, we extract the policy via advantage-weighted behavioral cloning. We dub our method implicit Q-learning (IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization. .",
            "url": "https://tayalmanan28.github.io/my_blogs/reinforcement-learning/robotics/2022/04/01/Offline-Reinforcement-Learning.html",
            "relUrl": "/reinforcement-learning/robotics/2022/04/01/Offline-Reinforcement-Learning.html",
            "date": " • Apr 1, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Tutorial on MuJoCo",
            "content": "Motor . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/mujoco/simulations/robotics/2022/01/21/MuJoCo.html",
            "relUrl": "/mujoco/simulations/robotics/2022/01/21/MuJoCo.html",
            "date": " • Jan 21, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Optimization for Machine Learning",
            "content": "Motor . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/optimization/machine-learning/robotics/2021/12/18/Optimization-for-ML.html",
            "relUrl": "/optimization/machine-learning/robotics/2021/12/18/Optimization-for-ML.html",
            "date": " • Dec 18, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Probability and Stochastics",
            "content": "Motor . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/probability/stochastics/robotics/2021/10/03/probability-and-stochastics.html",
            "relUrl": "/probability/stochastics/robotics/2021/10/03/probability-and-stochastics.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Tutorial on Control Theory",
            "content": "Motor . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/pybullet/robotics/2021/01/13/Tutorial-on-control-theory.html",
            "relUrl": "/pybullet/robotics/2021/01/13/Tutorial-on-control-theory.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Reinforcement Learning",
            "content": "Motor . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/reinforcement-learning/robotics/2020/08/23/reinforcement-learning.html",
            "relUrl": "/reinforcement-learning/robotics/2020/08/23/reinforcement-learning.html",
            "date": " • Aug 23, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Tutorial on Pybullet",
            "content": "Motor . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/pybullet/robotics/2020/05/16/Tutorial-on-Pybullet.html",
            "relUrl": "/pybullet/robotics/2020/05/16/Tutorial-on-Pybullet.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "3D Printing",
            "content": "3D Printing . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/3d-printing/robotics/2019/12/22/3D-Printing.html",
            "relUrl": "/3d-printing/robotics/2019/12/22/3D-Printing.html",
            "date": " • Dec 22, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "How to Choose Batteries",
            "content": "Batteries . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/battery/robotics/2019/11/19/how-to-choose-batteries.html",
            "relUrl": "/battery/robotics/2019/11/19/how-to-choose-batteries.html",
            "date": " • Nov 19, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "How to Choose Motors",
            "content": "Motor . Basic setup .",
            "url": "https://tayalmanan28.github.io/my_blogs/motor/robotics/2019/09/21/how-to-choose-motors.html",
            "relUrl": "/motor/robotics/2019/09/21/how-to-choose-motors.html",
            "date": " • Sep 21, 2019"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tayalmanan28.github.io/my_blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}