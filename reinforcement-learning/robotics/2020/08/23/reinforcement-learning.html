<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reinforcement Learning | Manan Tayal</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Reinforcement Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Reinforcement Learning" />
<meta property="og:description" content="Reinforcement Learning" />
<link rel="canonical" href="https://tayalmanan28.github.io/my_blogs/reinforcement-learning/robotics/2020/08/23/reinforcement-learning.html" />
<meta property="og:url" content="https://tayalmanan28.github.io/my_blogs/reinforcement-learning/robotics/2020/08/23/reinforcement-learning.html" />
<meta property="og:site_name" content="Manan Tayal" />
<meta property="og:image" content="https://tayalmanan28.github.io/my_blogs/images/rl.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-23T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://tayalmanan28.github.io/my_blogs/images/rl.png" />
<meta property="twitter:title" content="Reinforcement Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-08-23T00:00:00-05:00","datePublished":"2020-08-23T00:00:00-05:00","description":"Reinforcement Learning","headline":"Reinforcement Learning","image":"https://tayalmanan28.github.io/my_blogs/images/rl.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://tayalmanan28.github.io/my_blogs/reinforcement-learning/robotics/2020/08/23/reinforcement-learning.html"},"url":"https://tayalmanan28.github.io/my_blogs/reinforcement-learning/robotics/2020/08/23/reinforcement-learning.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/my_blogs/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://tayalmanan28.github.io/my_blogs/feed.xml" title="Manan Tayal" /><link rel="shortcut icon" type="image/x-icon" href="/my_blogs/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/my_blogs/">Manan Tayal</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/my_blogs/search/">Search</a><a class="page-link" href="/my_blogs/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Reinforcement Learning</h1><p class="page-description">Reinforcement Learning</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-23T00:00:00-05:00" itemprop="datePublished">
        Aug 23, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      14 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/my_blogs/categories/#Reinforcement-Learning">Reinforcement-Learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/my_blogs/categories/#Robotics">Robotics</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#what-is-reinforcement-learning">What is reinforcement learning?</a></li>
<li class="toc-entry toc-h1"><a href="#difference-between-reinforcement-learning-supervised-learning-and-unsupervised-learning">Difference between reinforcement learning, supervised learning, and unsupervised learning</a>
<ul>
<li class="toc-entry toc-h2"><a href="#reinforcement-learning-vs-supervised-learning">Reinforcement learning vs supervised learning.</a></li>
<li class="toc-entry toc-h2"><a href="#reinforcement-learning-vs-unsupervised-learning">Reinforcement learning vs unsupervised learning.</a></li>
<li class="toc-entry toc-h2"><a href="#reinforcement-and-deep-learning">Reinforcement and deep learning.</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#reinforcement-learning-use-cases">Reinforcement learning use cases</a>
<ul>
<li class="toc-entry toc-h2"><a href="#personalization">Personalization</a>
<ul>
<li class="toc-entry toc-h3"><a href="#news-recommendation">News recommendation.</a></li>
<li class="toc-entry toc-h3"><a href="#games-personalization">Games personalization.</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#ecommerce-and-internet-advertising">eCommerce and internet advertising</a></li>
<li class="toc-entry toc-h2"><a href="#trading-in-financial-industry">Trading in financial industry</a></li>
<li class="toc-entry toc-h2"><a href="#autonomous-vehicles-training">Autonomous vehicles training</a></li>
<li class="toc-entry toc-h2"><a href="#robotics">Robotics</a></li>
<li class="toc-entry toc-h2"><a href="#industrial-automation">Industrial automation</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#challenges-to-implementing-reinforcement-learning-in-business">Challenges to implementing reinforcement learning in business</a>
<ul>
<li class="toc-entry toc-h2"><a href="#environment-unpredictability">Environment unpredictability.</a></li>
<li class="toc-entry toc-h2"><a href="#delayed-feedback">Delayed feedback.</a></li>
<li class="toc-entry toc-h2"><a href="#infinite-time-horizons">Infinite time horizons.</a></li>
<li class="toc-entry toc-h2"><a href="#defining-a-precise-reward-function">Defining a precise reward function.</a></li>
<li class="toc-entry toc-h2"><a href="#data-problem-and-exploration-risks">Data problem and exploration risks.</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#conclusion">Conclusion</a></li>
</ul><h1 id="what-is-reinforcement-learning">
<a class="anchor" href="#what-is-reinforcement-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is reinforcement learning?</h1>
<p>Reinforcement learning (RL) is a machine learning technique that focuses on training an algorithm following the cut-and-try approach. The algorithm (agent) evaluates a current situation (state), takes an action, and receives feedback (reward) from the environment after each act. Positive feedback is a reward (in its usual meaning for us), and negative feedback is punishment for making a mistake.</p>

<p><img src="https://user-images.githubusercontent.com/42448031/169251804-d745e6d8-249d-459c-b296-af121c5128bb.png" alt="rl"></p>

<p>RL algorithm learns how to act best through many attempts and failures. Trial-and-error learning is connected with the so-called long-term reward. This reward is the ultimate goal the agent learns while interacting with an environment through numerous trials and errors. The algorithm gets short-term rewards that together lead to the cumulative, long-term one.</p>

<p>So, the key goal of reinforcement learning used today is to define the best sequence of decisions that allow the agent to solve a problem while maximizing a long-term reward. And that set of coherent actions is learned through the interaction with environment and observation of rewards in every state.</p>

<h1 id="difference-between-reinforcement-learning-supervised-learning-and-unsupervised-learning">
<a class="anchor" href="#difference-between-reinforcement-learning-supervised-learning-and-unsupervised-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Difference between reinforcement learning, supervised learning, and unsupervised learning</h1>
<p>Reinforcement learning is distinguished from other training styles, including supervised and unsupervised learning, by its goal and, consequently, the learning approach.</p>

<h2 id="reinforcement-learning-vs-supervised-learning">
<a class="anchor" href="#reinforcement-learning-vs-supervised-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reinforcement learning vs supervised learning.</h2>
<p>In supervised learning, an agent “knows” what task to perform and which set of actions is correct. Data scientists train the agent on historical data with target variables (desired answers with predictive analysis) AKA labeled data. The agent receives direct feedback. As a result of training, an agent can forecast whether there will be target variables in new data or not. Supervised learning allows for solving classification and regression tasks.</p>

<p>Reinforcement learning doesn’t rely on labeled datasets: The agent isn’t told which actions to take or the optimal way of performing a task. RL uses rewards and penalties instead of labels associated with each decision in datasets to signal whether a taken action is good or bad. So, the agent only gets feedback once it completes the task. That’s how time-delayed feedback and the trial-and-error principle differentiate reinforcement learning from supervised learning.</p>

<p>Since one of the goals of RL is to find a set of consecutive actions that maximize a reward, sequential decision making is another significant difference between these algorithm training styles. Each agent’s decision can affect its future actions.</p>

<h2 id="reinforcement-learning-vs-unsupervised-learning">
<a class="anchor" href="#reinforcement-learning-vs-unsupervised-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reinforcement learning vs unsupervised learning.</h2>
<p>In unsupervised learning, the algorithm analyzes unlabeled data to find hidden interconnections between data points and structures them by similarities or differences. RL aims at defining the best action model to get the biggest long-term reward, differentiating it from unsupervised learning in terms of the key goal.</p>

<h2 id="reinforcement-and-deep-learning">
<a class="anchor" href="#reinforcement-and-deep-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reinforcement and deep learning.</h2>
<p>Most of reinforcement learning implementations employ deep learning models. They involve the use of deep neural networks as the core method for agent training. Unlike other machine learning methods, deep learning fits best for recognizing complex patterns in images, sounds, and texts. Additionally, neural networks allow data scientists to fit all processes into a single model without breaking down the agent’s architecture into multiple modules.</p>

<h1 id="reinforcement-learning-use-cases">
<a class="anchor" href="#reinforcement-learning-use-cases" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reinforcement learning use cases</h1>
<p>Reinforcement learning is applicable in numerous industries, including internet advertising and eCommerce, finance, robotics, and manufacturing. Let’s take a closer look at these use cases.</p>

<h2 id="personalization">
<a class="anchor" href="#personalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Personalization</h2>
<h3 id="news-recommendation">
<a class="anchor" href="#news-recommendation" aria-hidden="true"><span class="octicon octicon-link"></span></a>News recommendation.</h3>
<p>Machine learning has made it possible for businesses to personalize customer interactions at scale through the analysis of data on their preferences, background, and online behavior patterns.</p>

<p>However, recommending such content type as online news is still a complex task. News features are dynamic by nature and become rapidly irrelevant. User preferences in topics change as well.</p>

<p>Authors of the research paper DRN: A Deep Reinforcement Learning Framework for News Recommendation discuss three main challenges related to news recommendation methods. First, these methods only try to model current (short-term) reward (e.g., click-through rate that shows the ratios page/ad/email viewers that click on a link). The second issue is that current recommendation methods usually take into account the click/no click labels or ratings as users’ feedback. And third, these methods typically continue suggesting similar news to readers, so users can get bored.</p>

<p>The researchers used the Deep Q-Learning based recommendation framework that considers current reward and future reward simultaneously in addition to user return as feedback rather than clicks data.</p>

<h3 id="games-personalization">
<a class="anchor" href="#games-personalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Games personalization.</h3>
<p>Gaming companies also have joined the personalization party. Really, why not tailor a video game experience (rules and content) taking into account an individual player’s skill level, playing style, or preferred gameplay? Personalization of game experience is done through player modeling with the goal of increasing their enjoyment. A player model is an abstract description of a player based on their behavior in a game.</p>

<p>Game components that can be adapted include space, mission, character, narrative, music and sound, game mechanics, difficulty scaling, and player matching (in multiplayer games).</p>

<p>RL can be used for optimizing game experience in real-time. In Reinforcement learning for game personalization on edge devices, researchers demonstrate capabilities of this machine learning technique using Pong, last century’s arcade game, as the example.</p>

<p>Unity provides an ML toolset for researchers and developers that allows for training intelligent agents with reinforcement learning and “evolutionary methods via a simple Python API.”</p>

<p>It’s worth mentioning that we haven’t found any application of RL agents in production.</p>

<h2 id="ecommerce-and-internet-advertising">
<a class="anchor" href="#ecommerce-and-internet-advertising" aria-hidden="true"><span class="octicon octicon-link"></span></a>eCommerce and internet advertising</h2>
<p>Specialists are experimenting with reinforcement learning algorithms to solve a problem of impressions allocation on eCommerce sites like eBay, Taobao, and Amazon. Impressions refer to the number of times a visitor sees some element of a web page, an ad or a product link with a description.   Impressions are often used to calculate how much an advertiser has to pay to show his message on a website. Each time a user loads a page and the ad pops up, it counts as one impression.</p>

<p>These platforms aim to generate maximum total revenue from transactions, that’s why they must use algorithms that will allocate buyer impressions (show buyer requests on items) to the most appropriate potential merchants.</p>

<p>Most platforms  use such recommendation methods as collaborative filtering or content-based filtering. These algorithms rank merchants using their “historical scores” that rely on the transaction history of the sellers with customers of similar characteristics. Sellers experiment with prices to get higher ranking positions, and these algorithms don’t take into account changes in pricing schemes.</p>

<p>As a solution to this problem, researchers applied a general framework of reinforcement mechanism design. The framework uses deep reinforcement learning to develop efficient algorithms that evaluate sellers’ behavior.</p>

<p>Online merchants can also conduct fraudulent transactions to improve their rating on eCommerce platforms to draw more buyers. And that, according to researchers, decreases the efficiency of use of buyer impressions and threatens the business environment. However, it’s possible to improve the platform’s impression allocation mechanism while increasing its profit and minimizing fraudulent activities with reinforcement learning.</p>

<p>In the article on AI and DS advances and trends, we discussed another RL use case – real-time bidding strategy optimization. It allows businesses to dynamically allocate the advertisement campaign budget “across all the available impressions on the basis of both the immediate and future rewards.” During real-time bidding, an advertiser bids on an impression, and their ad is displayed on a publisher’s platform if they win an auction.</p>

<h2 id="trading-in-financial-industry">
<a class="anchor" href="#trading-in-financial-industry" aria-hidden="true"><span class="octicon octicon-link"></span></a>Trading in financial industry</h2>
<p>Financial institutions use AI-driven systems to automate trading tasks. Generally, these systems use supervised learning to forecast stock prices. What they can’t do is to decide what action to take in a specific situation: to buy, sell, or hold. Traders still must make business rules that are trend-following, pattern-based, or counter-trend to govern system choices. Because analysts may define patterns and confirmation conditions in different ways, there is a need for consistency.</p>

<p>Michael Kearns, computer science professor at the University of Pennsylvania, hired by Morgan Stanley, stock trading firm, in June 2018 noted that RL models allow for making predictions that take into account outcomes of one’s actions on the market. In addition, traders may also learn about the most appropriate time for action and/or the optimum size of a trade.</p>

<p>IBM built a financial trading system on its Data Science Experience platform that utilizes reinforcement learning. “The model winds around training on the historical stock price data using stochastic actions at each time step, and we calculate the reward function based on the profit or loss for each trade,” said Aishwarya Srinivasan from IBM. Developers use active return on investment to evaluate the model’s performance. An active return is the difference between the benchmark and the actual return expressed as a percentage.</p>

<p>Specialists also evaluate the performance of the investment against the market index that represents market movement in general. “Finally, we assess the model against a simple Buy-&amp;-Hold strategy and against ARIMA-GARCH. We found that the model had much-refined moderation according to the market movements, and could even capture the head-and-shoulder patterns, which are non-trivial trends that can signal reversals in the market,” added Srinivasan.</p>

<h2 id="autonomous-vehicles-training">
<a class="anchor" href="#autonomous-vehicles-training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Autonomous vehicles training</h2>
<p>Reinforcement learning has proven to be an effective method for training deep learning networks that power self-driving car systems. UK company Wayve claims to be the first one to develop a driverless car that works with the help of RL.</p>

<p>Wayve specialists train a self-driving car with reinforcement learning</p>

<p>Developers generally write a large list of hand-written rules to tell autonomous vehicles how to drive. And, that has led to slow development cycles. Wayve specialists chose the other way. They spent only 15-20 minutes to teach a car from scratch to follow a lane through trial and error. A human driver that was in the vehicle during an experiment intervened when the algorithm made a mistake and the car was going off track. The algorithm was rewarded for a distance driven without intervention. That way a car has learned online getting better in driving safely with every exploration episode. Researchers explain the technical side of training in their blog post.</p>

<h2 id="robotics">
<a class="anchor" href="#robotics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Robotics</h2>
<p>Numerous problems in robotics can be formulated as reinforcement learning ones. A robot learns optimal sequential actions to complete a task with a maximum cumulative reward through exploration by receiving feedback from the environment. Developers don’t give it detailed instructions for solving a problem.</p>

<p>The RL in robotics survey authors point out that reinforcement learning provides a framework and a range of tools for the design of sophisticated and hard-to-engineer behaviors.</p>

<p>Specialists from the Google Brain Team and X company introduced a scalable reinforcement learning approach to solving a problem of training vision-based dynamic manipulation skills in robots. The goal was to train robots to grasp various objects, including objects unseen during training.</p>

<p>They combined deep learning and RL technique to enable robots to continuously learn from their experience and improve their basic sensorimotor skills. Specialists didn’t have to engineer behaviors themselves: Robots automatically learned how to complete this task. Specialists designed a deep Q-learning algorithm (QT-Opt) that employs data collected during past training episodes (grasping attempts).</p>

<p>Seven robots have been trained with more than 1000 visually and physically diverse objects for 800 hours over four months. A camera image was analyzed to suggest how a robot should move its arm and gripper.</p>

<p>The novel approach led to a 96 percent success rate of the grasp attempts across 700 test grasps on previously unseen objects. A supervised-learning based approach the specialists used before showed a 78 percent-success rate.</p>

<p>It also turned out that the algorithm achieves that accuracy while requiring less training data (although the training time was longer).</p>

<h2 id="industrial-automation">
<a class="anchor" href="#industrial-automation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Industrial automation</h2>
<p>RL has a potential to be widely used in industrial settings for machinery and equipment tuning supplementing human operators. Bonsai is one of the startups that provides a deep reinforcement learning platform for building autonomous industrial solutions to control and optimize the work of systems.</p>

<p>For instance, customers can improve energy efficiency, reduce downtime, increase equipment longevity, and control vehicles and robots in real time. You can listen to the O’Reilly Data Show podcast in which Bonsai CEO and founder describes various possible RL use cases for companies and enterprises.</p>

<p>Google uses the power of reinforcement learning to become more environmentally friendly. The tech company’s IA research group, DeepMind, developed and deployed RL models that helped reduce energy consumption for cooling data centers by up to 40 percent and decreased total energy overhead by 15 percent.</p>

<h1 id="challenges-to-implementing-reinforcement-learning-in-business">
<a class="anchor" href="#challenges-to-implementing-reinforcement-learning-in-business" aria-hidden="true"><span class="octicon octicon-link"></span></a>Challenges to implementing reinforcement learning in business</h1>
<p>The application of RL for solving business problems may pose serious challenges. That’s because this technique is exploratory in nature. The agent collects data on the go since there is no labeled or unlabeled data to guide it with a task goal. The decisions taken influence the data received. That’s why the agent may need to try out different actions to get new data.</p>

<h2 id="environment-unpredictability">
<a class="anchor" href="#environment-unpredictability" aria-hidden="true"><span class="octicon octicon-link"></span></a>Environment unpredictability.</h2>
<p>An RL algorithm may perform exceptionally when trained in closed, synthetic environments. In video games, for example, conditions under which the agent repeats its decision process don’t change. That’s not the case for the real world. It’s for these reasons that industries like finance, insurance, or healthcare think twice before investing their money into trials of RL-based systems.</p>

<h2 id="delayed-feedback">
<a class="anchor" href="#delayed-feedback" aria-hidden="true"><span class="octicon octicon-link"></span></a>Delayed feedback.</h2>
<p>In real-life applications, it’s uncertain how much time would be required to realize the outcome of a specific decision. For example, if an AI trading system predicts that the investment in some assets (real estate) would be beneficial, we’ll need to wait a month, year, or several years until we figure out whether that was a good idea.</p>

<h2 id="infinite-time-horizons">
<a class="anchor" href="#infinite-time-horizons" aria-hidden="true"><span class="octicon octicon-link"></span></a>Infinite time horizons.</h2>
<p>In RL, an agent’s number one goal is to get the highest reward possible. As we don’t know how much time or tries it will take, we have to establish an infinite horizon objective. For instance, if we were testing a self-driving car (that uses RL) to switch lanes, we couldn’t tell how many times it will hit other vehicles on the road until it does it right.</p>

<h2 id="defining-a-precise-reward-function">
<a class="anchor" href="#defining-a-precise-reward-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Defining a precise reward function.</h2>
<p>Data scientists may struggle with expressing the definition of good or bad action mathematically, computing a reward for the action. The advice is to think about reward functions in terms of current states, allowing the agent to know whether the action it is about to take will help it get closer to a final goal. For example, if there is the need to train a self-driving car to turn right without hitting a fence, sizes of reward functions would depend on the distance between a car and a fence and the start of steering.</p>

<h2 id="data-problem-and-exploration-risks">
<a class="anchor" href="#data-problem-and-exploration-risks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data problem and exploration risks.</h2>
<p>RL requires even more data than supervised learning. “It is really difficult to get enough data for reinforcement learning algorithms. There’s more work to be done to translate this to businesses and practice,” said computer scientist and entrepreneur Andrew Ng during his speech at the Artificial Intelligence Conference in San Francisco 2017. Just imagine what chaos the self-driving vehicle’s system could cause if it was tested solely on a street: It can hit neighbor cars, pedestrians, or smash into a guardrail. So, testing devices or systems using RL in a real environment can be difficult, financially irrational, and dangerous. One of the solutions is to test it on synthetic data (3D environments) while taking into account all the possible variables that may influence the agent’s decision at each situation or time step (pedestrians, road type and quality, weather conditions, etc.)</p>

<h1 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h1>
<p>Despite training difficulties, reinforcement learning finds its way to be effectively used in real business scenarios. Generally, RL is valuable when searching for optimal solutions in a constantly changing environment is needed.</p>

<p>Reinforcement learning is used for operations automation, machinery and equipment control and maintenance, energy consumption optimization. The finance industry also acknowledged the capabilities of reinforcement learning for powering AI-based training systems. Although trial-and-error training of robots is time-consuming, it allows robots to better evaluate real-world situations, use their skills for completing tasks, or reacting to unexpected consequences appropriately. In addition, RL provides opportunities for eCommerce players in terms of revenue optimization, fraud prevention, and customer experience enhancement via personalization.</p>


  </div><a class="u-url" href="/my_blogs/reinforcement-learning/robotics/2020/08/23/reinforcement-learning.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/my_blogs/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/my_blogs/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/my_blogs/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>I Write Blogs Related to Robotics</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/tayalmanan28" target="_blank" title="tayalmanan28"><svg class="svg-icon grey"><use xlink:href="/my_blogs/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/tayalmanan28" target="_blank" title="tayalmanan28"><svg class="svg-icon grey"><use xlink:href="/my_blogs/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
